{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f59c9742-5a7f-4317-bcab-8660acc70016",
      "metadata": {
        "id": "f59c9742-5a7f-4317-bcab-8660acc70016"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This notebook trains a diffusion model on an input dataset of images. It assumes that the input data\n",
        "can be accessed at path IMG_PATH, and trains a diffusion model for that digit class. It also\n",
        "outputs a set of generated images.\n",
        "\n",
        "It also contains code for hyperparameter optimization using random search.\n",
        "\n",
        "This code was heavily adapted from the following tutorial\n",
        "https://tree.rocks/make-diffusion-model-from-scratch-easy-way-to-implement-quick-diffusion-model-e60d18fd0f2e\n",
        "\"\"\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6halb93o6REC",
      "metadata": {
        "id": "6halb93o6REC"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ggc_gLe-7knI",
      "metadata": {
        "id": "ggc_gLe-7knI"
      },
      "source": [
        "## Import Required Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8R7ZLpPI7mVS",
      "metadata": {
        "id": "8R7ZLpPI7mVS"
      },
      "source": [
        "## Define hyperparameters and configuration variables for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b-U8zBEL-H9Z",
      "metadata": {
        "id": "b-U8zBEL-H9Z"
      },
      "outputs": [],
      "source": [
        "# For data operations\n",
        "import numpy as np\n",
        "\n",
        "def noise_schedule(timesteps):\n",
        "  \"\"\"\n",
        "  Defines the noise schedule used by the CLASSICAL diffusion model.\n",
        "  In this work, we use a linear noise schedule.\n",
        "\n",
        "  Inputs:\n",
        "    timesteps, an integer representing the number of timesteps for the classical model.\n",
        "\n",
        "  Returns:\n",
        "    The noise schedule to be used by the diffusion model; it should\n",
        "    follow the interface of an array of floats.\n",
        "  \"\"\"\n",
        "  return 1 - np.linspace(0, 1.0, timesteps + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9a0d2bcf-e69d-4147-96ca-e6bed75643de",
      "metadata": {
        "id": "9a0d2bcf-e69d-4147-96ca-e6bed75643de"
      },
      "outputs": [],
      "source": [
        "# For diffusion model training\n",
        "import torch\n",
        "import random\n",
        "\n",
        "IMG_SIZE = 32     # input image size; MNIST is 28 by 28, but we pad the image to reshape it\n",
        "BATCH_SIZE = 10  # for training batch size\n",
        "timesteps = 9 - 1    # how many steps to turn a noisy image into clear\n",
        "DIGIT = 0 # The MNIST digit class to train the diffusion model on\n",
        "# The path where the training images are stored\n",
        "IMG_PATH = \"/content/drive/MyDrive/COMP693_Images/data_img\" + str(DIGIT)\n",
        "# The path to store the results of the model training (generated images and model weights)\n",
        "DEST_DIR_PATH = \"/content/drive/MyDrive/COMP693_diffusion\"\n",
        "LEARNING_RATE = 0.0008 # Step size for optimizer in model\n",
        "NUM_EPOCHS = 100 # Number of times to train on full dataset\n",
        "TRAINING_ITERS = 20 # Number of times to train NUM_EPOCHS times, with LEARNING_RATE *= 0.9 for each training iteration\n",
        "SEED = 42 # Seed for RNG's\n",
        "CLASSICAL = False # Whether or not we want to train the classical counterpart of this diffusion model\n",
        "verbose = False # Controls the verbosity when training the model\n",
        "NUM_QUBITS = 127 # The number of qubits on the target quantum machine\n",
        "\n",
        "if CLASSICAL:\n",
        "  # Define a linear space for timesteps used for noising the image classically\n",
        "  time_bar = noise_schedule(timesteps)\n",
        "  print(f'time_bar: {time_bar}')\n",
        "\n",
        "# Use the GPU on this device\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Use the same seed\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9aaa1fcf-a588-484b-99a2-a288a1a58a56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "9aaa1fcf-a588-484b-99a2-a288a1a58a56",
        "outputId": "23a037fb-ae2a-4db2-d476-f4c9e1a68139"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ef1bd4370d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot a general trend of the expected CLASSICAL clarity versus noise of the images\n",
        "noise_schedule_time = noise_schedule(timesteps)\n",
        "plt.plot(noise_schedule_time, label='Noise')\n",
        "plt.plot(1 - noise_schedule_time, label='Clarity')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MrETK9RL-R-q",
      "metadata": {
        "id": "MrETK9RL-R-q"
      },
      "source": [
        "# Process Input Images\n",
        "\n",
        "Convert grayscale images to RGB, as well as pad them with two pixels on each side to adapt to the diffusion model code, as the diffusion model code works for 32 by 32 images, and the training data is 28 by 28 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "LPUUmj9wSvJT",
      "metadata": {
        "id": "LPUUmj9wSvJT"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "# For processing training data\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def get_mnist_components(digit, load_training_data, num_qubits, verbose=False):\n",
        "  \"\"\"\n",
        "  Obtains the PCA object and the minimum/maximum values of the features for\n",
        "  the specified digit class. Uses the training data specified from load_training_data.\n",
        "\n",
        "  Inputs:\n",
        "    digit, an integer from 0 through 9 representing the digit class to obtain the PCA features and\n",
        "    maximum/minimum feature values for\n",
        "    load_training_data, a function that returns the training data in the form\n",
        "      (x_train, y_train), (x_test, y_test)\n",
        "    num_qubits, an integer representing the number of qubits on the target quantum machine\n",
        "    to perform PCA to\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Outputs:\n",
        "    a tuple containing four objects:\n",
        "      pca, an sklearn.PCA object for performing PCA on the training data\n",
        "      max_feature_vals, a numpy array of length equal to the number of features, where\n",
        "      the value at position i is the maximum value of feature i across the training data\n",
        "      min_feature_vals, a numpy array of length equal to the number of features, where\n",
        "      the value at position i is the minimum value of feature i across the training data\n",
        "      num_feat, an integer representing the number of features\n",
        "  \"\"\"\n",
        "  # Obtain the training data\n",
        "  (x_train, y_train), (x_test, y_test) = load_training_data()\n",
        "\n",
        "  if verbose:\n",
        "    print(\"loaded mnist data\")\n",
        "\n",
        "  # Normalize the training data\n",
        "  x_train_flat = x_train.reshape(-1, 28*28) / 255.0\n",
        "  x_test_flat = x_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "  # Only take digits corresponding to the specified digit\n",
        "  x_train_category = x_train_flat[y_train == digit]\n",
        "  x_test_category = x_test_flat[y_test == digit]\n",
        "\n",
        "  # In our experiments, for digits 0 and 1, we ran them with a number of features sufficient for 95% variance,\n",
        "  # but for all other digits, we used the number of features equal to the number of qubits on the quantum machine.\n",
        "  if digit < 2:\n",
        "    pca = PCA(n_components=.95)\n",
        "  else:\n",
        "    pca = PCA(n_components=num_qubits)\n",
        "\n",
        "  # Obtain the PCA-transformed data\n",
        "  x_train_pca = pca.fit_transform(x_train_category)\n",
        "  num_feat = x_train_pca.shape[1]\n",
        "\n",
        "  # Scale the features to be between 0 and pi\n",
        "  scaler = MinMaxScaler(feature_range=(0, math.pi))\n",
        "  scaler.fit(x_train_pca)\n",
        "  x_train_pca_scaled = scaler.transform(x_train_pca)\n",
        "\n",
        "  # Find the minimum and maximum values of the original data, for rescaling again\n",
        "  min_feature_vals = np.min(x_train_pca, axis=0).tolist()\n",
        "  max_feature_vals = np.max(x_train_pca, axis=0).tolist()\n",
        "\n",
        "  return pca, max_feature_vals, min_feature_vals, num_feat\n",
        "\n",
        "\n",
        "def rescale_image(image, pca, max_feature_vals, min_feature_vals, num_feat, verbose=False):\n",
        "  \"\"\"\n",
        "  Rescales an image which assumed that P(q_i = |1>) = θ_i, to correct it\n",
        "  according to the relationship P(q_i = |0>) = cos^2(θ_i / 2).\n",
        "\n",
        "  Inputs:\n",
        "    image, a 28 by 28 numpy array representing the image to correct.\n",
        "    pca, an sklearn.PCA object for performing PCA on the training data\n",
        "    max_feature_vals, a numpy array of length equal to the number of features, where\n",
        "    the value at position i is the maximum value of feature i across the training data\n",
        "    min_feature_vals, a numpy array of length equal to the number of features, where\n",
        "    the value at position i is the minimum value of feature i across the training data\n",
        "    num_feat, an integer representing the number of features\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Outputs:\n",
        "    reshaped_image, a 28 by 28 numpy array representing the corrected image.\n",
        "  \"\"\"\n",
        "  # Obtain the PCA representation of the image.\n",
        "  flattened_img = image.reshape(-1, 28 * 28)\n",
        "  orig_pca_states = pca.transform(flattened_img)[0]\n",
        "  if verbose:\n",
        "    print(f'orig_pca_states: {orig_pca_states}')\n",
        "  # Reconstruct the original expected values for the image.\n",
        "  orig_expected_vals = []\n",
        "  for i in range(len(orig_pca_states)):\n",
        "    orig_exp_val = (orig_pca_states[i] - min_feature_vals[i]) / (max_feature_vals[i] - min_feature_vals[i])\n",
        "    orig_expected_vals.append(orig_exp_val)\n",
        "  if verbose:\n",
        "    print(f'orig_expected_vals: {orig_expected_vals}')\n",
        "\n",
        "  # Add the correct angle values corresponding to the image.\n",
        "  angle_vals = []\n",
        "  for prob_1_val in orig_expected_vals:\n",
        "    prob_0_val = 1 - prob_1_val\n",
        "    if verbose:\n",
        "      print(f'prob_1_val: {prob_1_val}')\n",
        "      print(f'prob_0_val: {prob_0_val}')\n",
        "    # Unfortunately, we have cases where prob_0_val is < 0 or > 1 due to numerical imprecisions in PCA\n",
        "    # and the fact that quantum measurements are fundamentally samples. However, we make the assumption\n",
        "    # that the P(q_i = |0>) will not be significantly larger than 1 nor smaller than 0, and floor them accordingly.\n",
        "    if prob_0_val > 1:\n",
        "      if verbose:\n",
        "        print(f'prob_0_val is too large')\n",
        "      prob_0_val = 1\n",
        "    elif prob_0_val < 0:\n",
        "      if verbose:\n",
        "        print(f'prob_0_val is too small')\n",
        "      prob_0_val = 0\n",
        "    angle_vals.append(2 * math.acos(math.sqrt(prob_0_val)))\n",
        "\n",
        "  # Scale the expected values to the range of the original PCA features\n",
        "  pca_states = []\n",
        "  for i in range(0, num_feat):\n",
        "      normalized_angle = angle_vals[i] / math.pi\n",
        "      pca_states.append(normalized_angle * (max_feature_vals[i] - min_feature_vals[i]) + min_feature_vals[i])\n",
        "\n",
        "  if verbose:\n",
        "    print(f'pca_states: {pca_states}')\n",
        "\n",
        "  # Reconstruct the input image\n",
        "  data_reconstructed = pca.inverse_transform(pca_states)\n",
        "\n",
        "  reshaped_image = data_reconstructed.reshape(28, 28)\n",
        "\n",
        "  return reshaped_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a135dfe9-6a37-4a77-a7fa-ad58d7efd845",
      "metadata": {
        "id": "a135dfe9-6a37-4a77-a7fa-ad58d7efd845"
      },
      "outputs": [],
      "source": [
        "def convert_rgb_to_greyscale(input_grayscale_image, verbose=False):\n",
        "  \"\"\"\n",
        "  Converts a grayscale image into an RGB image with three channels.\n",
        "\n",
        "  Inputs:\n",
        "    input_grayscale_image, a two dimensional numpy array representing a grayscale image\n",
        "\n",
        "  Returns:\n",
        "    a three dimensional PyTorch tensor, where the first dimension contains the channels, and\n",
        "    the remaining two encode the image. The three channels all contain the same value.\n",
        "  \"\"\"\n",
        "  if verbose:\n",
        "    print(f'input_grayscale_img.shape: {input_grayscale_image.shape}')\n",
        "  # Create a tensor where the values in the input image are repeated three times.\n",
        "  tensor_img = torch.from_numpy(input_grayscale_image)\n",
        "  tensor_img = tensor_img.repeat(3, 1, 1)\n",
        "  if verbose:\n",
        "    print(f'tensor_img.shape: {tensor_img.shape}')\n",
        "    print(f'tensor_img: {tensor_img}')\n",
        "    # print(f'created tensor.shape: {torch.from_numpy(np.array([[[s, s, s] for s in r] for r in input_grayscale_image], dtype=\"float\")).shape}')\n",
        "  return tensor_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "sei94_WOJOhv",
      "metadata": {
        "id": "sei94_WOJOhv"
      },
      "outputs": [],
      "source": [
        "def standardize_image(input_image, verbose=False):\n",
        "  \"\"\"\n",
        "  Standardize the pixel values to be between 0 and 1, and also pad the image\n",
        "  with two black pixels on each edge.\n",
        "\n",
        "  Inputs:\n",
        "    input_image, a two dimensional numpy array (assumed to be 28 by 28 pixels)\n",
        "    representing an image\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Returns:\n",
        "    a new two dimensional numpy array padded with two black pixels on each edge,\n",
        "    with each input pixel value standardized.\n",
        "  \"\"\"\n",
        "  # Flatten the input image and extract the maximum and minimum feature values.\n",
        "  # there's no need to flatten\n",
        "  flattened_image = input_image.reshape(-1, 28 * 28)\n",
        "  max_feature_val = np.amax(flattened_image)\n",
        "  min_feature_val = np.amin(flattened_image)\n",
        "  if verbose:\n",
        "    # print(f'input_image: {input_image}')\n",
        "    print(f'standardize_image: max_feature_val: {max_feature_val}')\n",
        "    print(f'standardize_image: min_feature_val: {min_feature_val}')\n",
        "  # Standardize each pixel value, and produce a resulting 2D numpy array.\n",
        "  standardized_image = []\n",
        "  for pixel_val in flattened_image:\n",
        "    standardized_pixel_val = (pixel_val - min_feature_val) / (max_feature_val - min_feature_val)\n",
        "    standardized_image.append(standardized_pixel_val)\n",
        "  standardized_image_reshaped = np.array(standardized_image).reshape(28, 28)\n",
        "  flattened_image = standardized_image_reshaped\n",
        "\n",
        "  # Pad the standardized image with two black pixels on each edge.\n",
        "  flattened_image = np.pad(flattened_image, ((2, 2), (2, 2)), 'constant')\n",
        "  if verbose:\n",
        "    print(f'flattened_image.shape: {flattened_image.shape}')\n",
        "    print(f'flattened_image: {flattened_image}')\n",
        "  return flattened_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "JvpMnfl3Z2WG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvpMnfl3Z2WG",
        "outputId": "258ab20b-a212-4ca8-e2e6-5978956f5770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive at a specified filepath, for file operations.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "DXa0sJHI-191",
      "metadata": {
        "id": "DXa0sJHI-191"
      },
      "outputs": [],
      "source": [
        "def get_base_img_idxs(data_list, verbose=False):\n",
        "  \"\"\"\n",
        "  Get the random indices of the training dataset for the specified digit class\n",
        "  used for training the diffusion model.\n",
        "\n",
        "  Inputs:\n",
        "    data_list, a List[Dict[Int, Numpy.Array(28, 28)]] which represents the a list of the\n",
        "    training data for the diffusion model at each timestep. The dictionary at each index\n",
        "    maps the index of the image to the image itself (a 28 by 28 numpy array).\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Outputs:\n",
        "    base_img_idxs, a list of integers representing the indices of the images used for\n",
        "    training the diffusion model.\n",
        "  \"\"\"\n",
        "  # Find all image indices that have a complete forward diffusion process\n",
        "  # (through all noise stages)\n",
        "  base_img_idxs = sorted(list(data_list[4].keys()))\n",
        "  missing_keys = set()\n",
        "  if verbose:\n",
        "    print(f'base_img_idxs: {base_img_idxs}')\n",
        "\n",
        "  # Iterate over each number of gate pairs, and find any image index that does\n",
        "  # not exist in at least one of the datasets for the gate pair.\n",
        "  for idx in range(len(data_list)):\n",
        "    cur_keys = list(data_list[idx])\n",
        "    contains_keys = set(base_img_idxs).issubset(cur_keys)\n",
        "    cur_missing_keys = set(base_img_idxs).difference(cur_keys)\n",
        "    missing_keys = missing_keys.union(cur_missing_keys)\n",
        "    if verbose:\n",
        "      print(f'contains_keys: {contains_keys}, idx: {idx}, cur_missing_keys: {cur_missing_keys}')\n",
        "\n",
        "  if verbose:\n",
        "    print(f'missing_keys: {missing_keys}')\n",
        "\n",
        "  # Remove all images for which we do not have all noise stages for.\n",
        "  base_img_idxs = sorted(list(set(base_img_idxs).difference(missing_keys)))\n",
        "\n",
        "  if verbose:\n",
        "    print(f'len(base_img_idxs): {len(base_img_idxs)}')\n",
        "    print(f'base_img_idxs: {base_img_idxs}')\n",
        "\n",
        "  return base_img_idxs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_training_data_classical(digit, load_training_data, verbose=False):\n",
        "  \"\"\"\n",
        "  Returns the classical training data.\n",
        "\n",
        "  Inputs:\n",
        "    digit, an integer from 0 through 9 representing the digit class to obtain the training data for\n",
        "    load_training_data, a function that returns the training data in the form\n",
        "      (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "  Returns:\n",
        "    a tuple containing the training data:\n",
        "      data_list, a List[Dict[Int, Numpy.Array(28, 28)]] which represents the a list of the\n",
        "      training data for the diffusion model at each timestep. The dictionary at each index\n",
        "      maps the index of the image to the image itself (a 28 by 28 numpy array).\n",
        "      base_img_idxs, a list of integers representing the indices of the images used for\n",
        "      training the diffusion model.\n",
        "  \"\"\"\n",
        "\n",
        "  def generate_unique_random_numbers(n):\n",
        "    \"\"\"\n",
        "    Generates a sorted list of randomly sampled numbers from 0 through n - 1\n",
        "    of length 500.\n",
        "\n",
        "    Inputs:\n",
        "      n, an integer representing the maximum value to sample from\n",
        "\n",
        "    Outputs:\n",
        "      a sorted list of randomly sampled numbers from 0 through n - 1\n",
        "      of length 500.\n",
        "    \"\"\"\n",
        "    return sorted(random.sample(range(n), 500))\n",
        "\n",
        "  # Load the training data for the digit class.\n",
        "  (x_train, y_train), (x_test, y_test) = load_training_data()\n",
        "  x_train_category = x_train[y_train == digit]\n",
        "\n",
        "  # print(x_train_category.shape)\n",
        "\n",
        "  # For digits less than 3, use the specified random indices; otherwise, call the function.\n",
        "  if DIGIT <= 3:\n",
        "    rand_indices = [10, 12, 20, 36, 58, 65, 74, 90, 114, 116, 118, 120, 137, 161, 171, 189, 197, 203, 236, 240, 247, 254, 256, 280, 284, 296, 298, 314, 315, 316, 346, 347, 357, 366, 374, 394, 396, 410, 467, 469, 492, 515, 524, 533, 541, 542, 551, 555, 556, 587, 597, 598, 605, 611, 612, 616, 620, 632, 645, 656, 666, 674, 677, 707, 716, 722, 729, 742, 758, 769, 780, 796, 825, 830, 841, 844, 850, 851, 853, 867, 868, 869, 887, 896, 916, 925, 936, 955, 990, 1000, 1060, 1082, 1090, 1092, 1105, 1107, 1114, 1132, 1156, 1168, 1171, 1173, 1176, 1177, 1178, 1197, 1223, 1233, 1239, 1241, 1254, 1263, 1265, 1275, 1280, 1283, 1301, 1303, 1304, 1305, 1329, 1363, 1371, 1379, 1387, 1405, 1420, 1440, 1462, 1474, 1482, 1495, 1497, 1522, 1556, 1594, 1614, 1647, 1661, 1672, 1674, 1700, 1706, 1707, 1710, 1759, 1779, 1789, 1795, 1796, 1799, 1835, 1839, 1848, 1856, 1869, 1879, 1916, 1919, 1920, 1923, 1966, 1967, 1979, 1996, 1998, 2008, 2012, 2021, 2026, 2067, 2069, 2090, 2105, 2112, 2134, 2142, 2152, 2177, 2198, 2202, 2205, 2222, 2227, 2233, 2247, 2249, 2259, 2269, 2272, 2274, 2277, 2286, 2327, 2329, 2353, 2360, 2366, 2369, 2381, 2422, 2424, 2427, 2430, 2448, 2450, 2452, 2455, 2467, 2487, 2492, 2501, 2538, 2542, 2550, 2553, 2569, 2578, 2580, 2585, 2604, 2609, 2616, 2624, 2650, 2656, 2657, 2666, 2675, 2686, 2704, 2714, 2735, 2761, 2779, 2793, 2821, 2827, 2838, 2852, 2858, 2861, 2865, 2885, 2893, 2902, 2927, 2937, 2943, 2944, 2950, 2956, 2959, 2964, 2971, 2991, 3017, 3020, 3040, 3070, 3075, 3091, 3101, 3116, 3123, 3129, 3132, 3144, 3152, 3165, 3171, 3195, 3224, 3249, 3252, 3254, 3265, 3267, 3275, 3295, 3300, 3326, 3337, 3348, 3377, 3400, 3413, 3415, 3419, 3424, 3456, 3458, 3466, 3479, 3481, 3482, 3486, 3488, 3494, 3504, 3519, 3527, 3545, 3569, 3579, 3589, 3594, 3603, 3608, 3609, 3633, 3653, 3659, 3666, 3667, 3709, 3718, 3720, 3754, 3760, 3764, 3768, 3775, 3790, 3817, 3828, 3844, 3865, 3868, 3871, 3881, 3887, 3890, 3926, 3938, 3943, 3948, 3961, 3986, 4001, 4008, 4020, 4066, 4069, 4095, 4131, 4153, 4168, 4175, 4182, 4188, 4199, 4202, 4244, 4247, 4258, 4264, 4281, 4283, 4335, 4338, 4357, 4362, 4377, 4391, 4392, 4401, 4424, 4437, 4445, 4451, 4463, 4479, 4505, 4509, 4517, 4520, 4547, 4563, 4565, 4569, 4617, 4623, 4625, 4635, 4645, 4652, 4655, 4695, 4705, 4738, 4747, 4749, 4758, 4770, 4815, 4817, 4826, 4840, 4844, 4880, 4909, 4928, 4929, 4937, 4957, 4967, 4971, 4975, 4976, 4977, 4979, 4985, 4986, 4993, 4996, 4997, 5012, 5026, 5032, 5033, 5047, 5062, 5064, 5068, 5081, 5090, 5096, 5118, 5130, 5139, 5141, 5150, 5151, 5160, 5175, 5185, 5192, 5194, 5202, 5203, 5207, 5224, 5232, 5240, 5261, 5268, 5275, 5276, 5282, 5283, 5302, 5307, 5317, 5335, 5343, 5357, 5387, 5398, 5412, 5472, 5479, 5488, 5495, 5504, 5506, 5517, 5541, 5545, 5549, 5561, 5568, 5579, 5581, 5596, 5604, 5607, 5633, 5637, 5653, 5672, 5697, 5699, 5712, 5716, 5717, 5723, 5734, 5738, 5741, 5742, 5747, 5751, 5760, 5816, 5818, 5819, 5841, 5864, 5885]\n",
        "  else:\n",
        "    rand_indices = generate_unique_random_numbers(len(x_train_category))\n",
        "\n",
        "  if verbose:\n",
        "    print(f'get_training_data_classical, rand_indices: {rand_indices}')\n",
        "\n",
        "  base_img_idxs = rand_indices\n",
        "\n",
        "  time_bar = noise_schedule()\n",
        "\n",
        "  # Add noisy images to the data_list at each index, using a linear noise schedule.\n",
        "  data_list = []\n",
        "  for timestep in range(timesteps + 1):\n",
        "    timestep_data_dict = {}\n",
        "    data_list.append(timestep_data_dict)\n",
        "    for rand_idx in rand_indices:\n",
        "      noise_coeff = time_bar[timestep]\n",
        "      # print(f'noise_coeff: {noise_coeff}')\n",
        "      noise = np.random.normal(0, 1, size=(28, 28))\n",
        "      # print(f'noise: {noise}')\n",
        "      rand_img = (1 - noise_coeff) * (x_train_category[rand_idx] / 255) + noise_coeff * noise\n",
        "      rand_img = (rand_img - rand_img.min()) / (rand_img.max() - rand_img.min())\n",
        "      if rand_idx == 4:\n",
        "        # print(f'rand_img: {rand_img}, rand_img == x_train_category[rand_idx]: {rand_img == x_train_category[rand_idx]}')\n",
        "        plt.imshow(rand_img, cmap='gray')\n",
        "        # plt.imshow(noise, cmap='gray')\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "      timestep_data_dict[rand_idx] = convert_rgb_to_greyscale(standardize_image(rand_img, verbose=False), verbose=False)\n",
        "  # for idx in range(len(data_list)):\n",
        "  #   print(f'idx: {idx}, number of images: {len(data_list[idx])}, type(data_list[idx]:{type(data_list[idx])}')\n",
        "\n",
        "  return data_list, base_img_idxs"
      ],
      "metadata": {
        "id": "BKlxlicQSSi-"
      },
      "id": "BKlxlicQSSi-",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "63ecf33b-edf2-41cf-adcd-9475d492522e",
      "metadata": {
        "id": "63ecf33b-edf2-41cf-adcd-9475d492522e"
      },
      "outputs": [],
      "source": [
        "# For file operations\n",
        "import os\n",
        "# For obtaining MNIST training data\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_training_data(digit, training_data_path, load_training_data, classical, verbose=False):\n",
        "  \"\"\"\n",
        "  Obtains the training data for the specified digit class. If CLASSICAL = True,\n",
        "  returns the training data for the classical noise model.\n",
        "\n",
        "  Inputs:\n",
        "    digit, an integer from 0 through 9 representing the digit class to obtain the training data for\n",
        "    training_data_path, a string representing the path to the directory containing the training data\n",
        "    classical, a boolean indicating whether or not to use classical noise for the training data\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Outputs:\n",
        "    a tuple containing the training data:\n",
        "      data_list, a List[Dict[Int, Numpy.Array(28, 28)]] which represents the a list of the\n",
        "      training data for the diffusion model at each timestep. The dictionary at each index\n",
        "      maps the index of the image to the image itself (a 28 by 28 numpy array).\n",
        "      base_img_idxs, a list of integers representing the indices of the images used for\n",
        "      training the diffusion model.\n",
        "  \"\"\"\n",
        "\n",
        "  if classical:\n",
        "    return get_training_data_classical(digit, load_training_data, verbose)\n",
        "  # File path containing all training data.\n",
        "  imagedir_path = training_data_path\n",
        "\n",
        "  # Get the PCA and feature values corresponding to the training data.\n",
        "  pca, max_feature_vals, min_feature_vals, num_feat = get_mnist_components(digit, load_training_data, NUM_QUBITS)\n",
        "\n",
        "  # A dictionary mapping integers (representing the number of gate pairs used) in\n",
        "  # the input circuit to another dictionary, mapping integers representing the circuit\n",
        "  # id to a Pytorch tensor representation of that image. The dimensions of the tensor are expected\n",
        "  # to be (3, 32, 32), where the first dimension represents the color channels, and the\n",
        "  # remaining two represent the dimensions of the input image.\n",
        "  data_map = {}\n",
        "\n",
        "  # Contains all of the dictionaries of data for each number of gate pairs used\n",
        "  # as a list.\n",
        "  data_list = []\n",
        "\n",
        "  # The folders used in training the diffusion model.\n",
        "  used_folders = ['actual_original_img' + str(digit), 'original_img' + str(digit), 'num_revs_0', 'num_revs_2', 'num_revs_4', 'num_revs_8', 'num_revs_16', 'num_revs_32']\n",
        "\n",
        "  # Iterate over all files in the input directory, parsing them and adding them to\n",
        "  # data_map.\n",
        "  for fname in os.listdir(imagedir_path):\n",
        "    folder_path = os.path.join(imagedir_path, fname)\n",
        "    # Ignore unused folders.\n",
        "    if fname not in used_folders:\n",
        "      continue\n",
        "    if os.path.isdir(folder_path):\n",
        "      # These are the images that are the PCA-reduced images.\n",
        "      if fname.startswith('original'):\n",
        "        num_revs = -1\n",
        "      # These are the original training images.\n",
        "      elif fname.startswith('actual'):\n",
        "        num_revs = -2\n",
        "      # Otherwise, these are the images for which quantum noise is added,\n",
        "      # and the amount of noise added (the number of gate reversal pairs)\n",
        "      # are defined by the suffix of the folder.\n",
        "      else:\n",
        "        num_revs = int(fname.split(\"_\")[-1])\n",
        "      if verbose:\n",
        "        print(f'num_revs: {num_revs}')\n",
        "      # Store the training data in the data_map dictionary.\n",
        "      data_map[num_revs] = {}\n",
        "      for file_name in os.listdir(folder_path):\n",
        "        if verbose:\n",
        "          print(f\"file_name: {file_name}\")\n",
        "        # print(f\"file_name.split('_'): {file_name.split('_')}\")\n",
        "        # print(f\"file_name.split('_')[-1]: {file_name.split('_')[-1]}\")\n",
        "        circuit_id_str = file_name.split(\"_\")[-1][:-4]\n",
        "        # Handle duplicate files by overriding existing files.\n",
        "        if circuit_id_str.endswith(\" (1)\"):\n",
        "          circuit_id_str = circuit_id_str[:-4]\n",
        "        circuit_id = int(circuit_id_str)\n",
        "        if verbose:\n",
        "          print(f'circuit_id: {circuit_id}')\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Load the training file\n",
        "        npy_arr = np.load(file_path)\n",
        "        # If the image was generated using quantum noise, re-calculate it.\n",
        "        if not fname.startswith('original') and not fname.startswith('actual'):\n",
        "          npy_arr = rescale_image(npy_arr, pca, max_feature_vals, min_feature_vals, num_feat)\n",
        "        # Pad the image with black pixels, and add color channels that are duplicates of the image, so that it matches\n",
        "        # the requirements for the diffusion model\n",
        "        data_map[num_revs][circuit_id] = convert_rgb_to_greyscale(standardize_image(npy_arr, verbose=verbose), verbose=verbose)\n",
        "\n",
        "  # Add the dictionary corresponding to each number of gate pairs used to a list,\n",
        "  # sorted by the number of gate pairs used in the quantum circuit.\n",
        "  for num_rev in sorted(list(data_map.keys())):\n",
        "    data_list.append(data_map[num_rev])\n",
        "\n",
        "  if verbose:\n",
        "    print(data_map)\n",
        "\n",
        "    print(data_map.keys())\n",
        "\n",
        "    print(sorted(list(data_map.keys())))\n",
        "\n",
        "    for idx in range(len(data_list)):\n",
        "      print(f'idx: {idx}, number of images: {len(data_list[idx])}, type(data_list[idx]:{type(data_list[idx])}')\n",
        "\n",
        "    print(data_list[0][10].shape)\n",
        "\n",
        "    print(sorted(data_list[0].keys()))\n",
        "\n",
        "  # Add random images as the initial configuration for these noisy images, as\n",
        "  # diffusion models need to start from complete noise.\n",
        "  all_keys = sorted(data_list[0].keys())\n",
        "\n",
        "  rand_imgs = {}\n",
        "  for key in all_keys:\n",
        "    rand_img = np.random.randn(28, 28)\n",
        "    rand_imgs[key] = convert_rgb_to_greyscale(standardize_image(rand_img))\n",
        "\n",
        "  # Add these completely ranodm images to the training data.\n",
        "  data_list.append(rand_imgs)\n",
        "  data_map[100] = rand_imgs\n",
        "\n",
        "  if verbose:\n",
        "    print(f'data_map.keys(): {data_map.keys()}')\n",
        "    print(f'len(data_list): {len(data_list)}')\n",
        "\n",
        "    for idx in range(len(data_list)):\n",
        "      print(f'idx: {idx}, number of images: {len(data_list[idx])}, type(data_list[idx]:{type(data_list[idx])}')\n",
        "\n",
        "  # Reverse the training data so that it goes from noisy images to clear images.\n",
        "  data_list.reverse()\n",
        "\n",
        "  # Get the training data indices.\n",
        "  base_img_idxs = get_base_img_idxs(data_list)\n",
        "\n",
        "  if verbose:\n",
        "    print(sorted(list(data_list[0].keys())))\n",
        "    print(sorted(list(data_list[1].keys())))\n",
        "\n",
        "  # plt.imshow(data_list[8][10][0], cmap='gray')\n",
        "\n",
        "  return data_list, base_img_idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "DHzySTVNm11I",
      "metadata": {
        "id": "DHzySTVNm11I"
      },
      "outputs": [],
      "source": [
        "def cvtImg(img):\n",
        "    \"\"\"\n",
        "    Standardizes the input image tensor to contain values between 0 and 1, and\n",
        "    also moves the color channel to the end of the tensor.\n",
        "\n",
        "    Inputs:\n",
        "      img, a four dimensional image tensor, where the first dimension represents\n",
        "      the number of images, the second the color channels, and the remaining two the\n",
        "      dimensions of the input image.\n",
        "\n",
        "    Returns:\n",
        "      A standardized and permuted four dimensional image tensor, where the color channel\n",
        "      is at the end of the tensor.\n",
        "    \"\"\"\n",
        "    img = img.permute([0, 2, 3, 1])\n",
        "    img = img - img.min()\n",
        "    img = (img / img.max())\n",
        "    return img.numpy().astype(np.float32)\n",
        "\n",
        "def show_examples(x):\n",
        "    \"\"\"\n",
        "    Plots image tensors for viewing.\n",
        "\n",
        "    Inputs:\n",
        "      x, a four dimensional image tensor, where the first dimension represents\n",
        "      the number of images, the second the color channels, and the remaining two the\n",
        "      dimensions of the input image.\n",
        "\n",
        "    Returns:\n",
        "      nothing (plots the input image tensor)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    imgs = cvtImg(x)\n",
        "    for i in range(1):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.imshow(imgs[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "# x, _ = next(iter(trainloader))\n",
        "# x = data_list[0][17]\n",
        "# show_examples(x.view(1, 3, IMG_SIZE, IMG_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "697c40e0-36cd-43de-9d75-57a9fe4ef21a",
      "metadata": {
        "id": "697c40e0-36cd-43de-9d75-57a9fe4ef21a"
      },
      "outputs": [],
      "source": [
        "def test_img_plotting(data_list, verbose=False):\n",
        "  \"\"\"\n",
        "  Function for testing cvtImg given the training data.\n",
        "\n",
        "  Inputs:\n",
        "    data_list, a List[Dict[Int, Numpy.Array(28, 28)]] which represents the a list of the\n",
        "    training data for the diffusion model at each timestep. The dictionary at each index\n",
        "    maps the index of the image to the image itself (a 28 by 28 numpy array).\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "  Outputs:\n",
        "    nothing (plots an input image tensor)\n",
        "  \"\"\"\n",
        "  xs = []\n",
        "  for i in range(len(data_list)):\n",
        "    if verbose:\n",
        "      print(f'image stage: {i}')\n",
        "    xs.append(data_list[i][10])\n",
        "  # Convert the denoised images for viewing.\n",
        "  xs = torch.stack(xs, dim=0)\n",
        "  xs = torch.clip(xs, -1, 1)\n",
        "  xs = cvtImg(xs)\n",
        "\n",
        "  if verbose:\n",
        "    print(xs.shape)\n",
        "    # print(type(xs))\n",
        "  # Plot the denoised images.\n",
        "  fig, axes = plt.subplots(11, 10, figsize=(20, 22))\n",
        "  axes = axes.flatten()\n",
        "  for i, ax in enumerate(axes):\n",
        "    if i < len(xs):\n",
        "      ax.imshow(xs[i])\n",
        "      ax.axis('off')\n",
        "    else:\n",
        "      ax.axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "# test_img_plotting()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pPHV-xYBkfWh",
      "metadata": {
        "id": "pPHV-xYBkfWh"
      },
      "source": [
        "# Diffusion Model Definitions\n",
        "\n",
        "We employ a U-Net architecture with CNN's as each block, using time-embeddings to encode the timestep information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bd8c84b3-bd06-4f55-8d06-44b2e5bcf817",
      "metadata": {
        "id": "bd8c84b3-bd06-4f55-8d06-44b2e5bcf817"
      },
      "outputs": [],
      "source": [
        "def forward_noise(x_idx, t, data_list, img_size):\n",
        "    \"\"\"\n",
        "    Performs the forward noising process for given image indices, as well as\n",
        "    the timestep corresponding to those images.\n",
        "\n",
        "    Inputs:\n",
        "      x_idx, a numpy array of nonnegative integers representing the indices of the input images\n",
        "      t, a numpy array of the same length as x_idx representing the timesteps to train on\n",
        "      data_list, a List[Dict[Int, Numpy.Array(28, 28)]] which represents the a list of the\n",
        "      training data for the diffusion model at each timestep. The dictionary at each index\n",
        "      maps the index of the image to the image itself (a 28 by 28 numpy array).\n",
        "      img_size, an integer representing the length/width of the input images\n",
        "\n",
        "    Returns:\n",
        "      img_a, img_b, two images represented by four-dimensional tensors where the second\n",
        "      channel contains the color channels, where img_a is the image at timestep t, and\n",
        "      img_b is the image at timestep t + 1.\n",
        "    \"\"\"\n",
        "    # Use the data that we have already generated from the forward diffusion process.\n",
        "    img_a_list = []\n",
        "    img_b_list = []\n",
        "\n",
        "    for idx, timestep in zip(x_idx, t):\n",
        "        # print(f'forward_noise: timestep: {timestep}')\n",
        "        img_a = data_list[timestep][idx]\n",
        "        img_b = data_list[timestep + 1][idx]\n",
        "        img_a_list.append(img_a)\n",
        "        img_b_list.append(img_b)\n",
        "\n",
        "    img_a = torch.stack(img_a_list).view(len(x_idx), 3, img_size, img_size)\n",
        "    img_b = torch.stack(img_b_list).view(len(x_idx), 3, img_size, img_size)\n",
        "\n",
        "    return img_a, img_b\n",
        "\n",
        "def generate_ts(num, timesteps):\n",
        "    \"\"\"\n",
        "    Generates a sequence of random integers between 0 and timesteps - 1 (exclusive) of size num.\n",
        "\n",
        "    Inputs:\n",
        "      num, an integer representing the length of the resulting sequence to be generated\n",
        "      timesteps, an integer representing the number of timesteps used by the diffusion model\n",
        "\n",
        "    Returns:\n",
        "      a sequence of random integers between 0 and timesteps - 1 (exclusive) of length num\n",
        "    \"\"\"\n",
        "    return np.random.randint(0, timesteps, size=num)\n",
        "\n",
        "# t = np.full((25,), timesteps - 1) # if you want see clarity\n",
        "# t = np.full((25,), 0)             # if you want see noisy\n",
        "# generate_ts(25, timesteps)             # random for training data\n",
        "# x, _ = next(iter(trainloader))\n",
        "# print(x[0][2].shape)\n",
        "# a, b = forward_noise(x[:25], t)\n",
        "# show_examples(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1dd133c3-6c7b-4ca5-8f87-4daf873ed65a",
      "metadata": {
        "id": "1dd133c3-6c7b-4ca5-8f87-4daf873ed65a"
      },
      "outputs": [],
      "source": [
        "# For defining neural network blocks\n",
        "import torch.nn as nn\n",
        "# For functions (e.g., activation functions)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# This class defines a block in the neural network, which contains a 2 dimensional\n",
        "# convolutional neural network as well as a time parameter.\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines a block in the neural network, which contains a 2-dimensional\n",
        "    convolutional neural network as well as a time parameter.\n",
        "\n",
        "    Attributes:\n",
        "      conv_param (nn.Conv2d): Convolutional layer for processing the image features.\n",
        "      conv_out (nn.Conv2d): Convolutional layer for producing the output features.\n",
        "      dense_ts (nn.Linear): Linear layer for transforming the time step embedding.\n",
        "      layer_norm (nn.LayerNorm): Layer normalization for the output features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=128, size=32):\n",
        "        \"\"\"\n",
        "        Initializes the Block with specified input channels and image size.\n",
        "\n",
        "        Inputs:\n",
        "          in_channels (int), the umber of input channels for the convolutional layers. Default is 128.\n",
        "          size (int), the size of the input image (height and width). Default is 32.\n",
        "\n",
        "        Returns:\n",
        "          Block, a Block object, which contains a 2D convolutional neural network and a time parameter.\n",
        "        \"\"\"\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        self.conv_param = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv_out = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.dense_ts = nn.Linear(192, 128)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm([128, size, size])\n",
        "\n",
        "    def forward(self, x_img, x_ts):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the Block layer.\n",
        "\n",
        "        This method processes the input image tensor `x_img` by integrating it\n",
        "        with the time step embedding `x_ts`. It applies convolutional operations,\n",
        "        element-wise multiplication with the time embedding, and layer normalization\n",
        "        to produce the output tensor.\n",
        "\n",
        "        Inputs:\n",
        "          x_img (torch.Tensor), the input image tensor of shape (batch_size, channels, height, width).\n",
        "          x_ts (torch.Tensor), the time step embedding tensor of shape (batch_size, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: The output tensor after processing through the block,\n",
        "                        of shape (batch_size, 128, height, width).\n",
        "        \"\"\"\n",
        "        x_parameter = F.relu(self.conv_param(x_img))\n",
        "\n",
        "        time_parameter = F.relu(self.dense_ts(x_ts))\n",
        "        time_parameter = time_parameter.view(-1, 128, 1, 1)\n",
        "        x_parameter = x_parameter * time_parameter\n",
        "\n",
        "        x_out = self.conv_out(x_img)\n",
        "        x_out = x_out + x_parameter\n",
        "        x_out = F.relu(self.layer_norm(x_out))\n",
        "\n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6b2f4df6-b553-42f1-a478-64e30d282b27",
      "metadata": {
        "id": "6b2f4df6-b553-42f1-a478-64e30d282b27"
      },
      "outputs": [],
      "source": [
        "# This class represents the neural network model following the U-Net architecture.\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines a U-Net based neural network for diffusion models,\n",
        "    which processes images through downsampling and upsampling blocks,\n",
        "    incorporating time step embeddings.\n",
        "\n",
        "    Attributes:\n",
        "      l_ts (nn.Sequential), a sequential model to transform the timestep into a high-dimensional embedding.\n",
        "      down_x32 (Block), a downsampling block for 32x32 resolution.\n",
        "      down_x16 (Block), a downsampling block for 16x16 resolution.\n",
        "      down_x8 (Block), a downsampling block for 8x8 resolution.\n",
        "      down_x4 (Block), a downsampling block for 4x4 resolution.\n",
        "      mlp (nn.Sequential), a multi-layer perceptron for the bottleneck layer.\n",
        "      up_x4 (Block), an upsampling block for 4x4 resolution.\n",
        "      up_x8 (Block), an upsampling block for 8x8 resolution.\n",
        "      up_x16 (Block), an upsampling block for 16x16 resolution.\n",
        "      up_x32 (Block), an upsampling block for 32x32 resolution.\n",
        "      cnn_output (nn.Conv2d), a convolutional layer for producing the final output image.\n",
        "      opt (torch.optim.Adam), a optimizer for training the model.\n",
        "      data_list (list), a list of training data.\n",
        "      base_img_idxs (list), a list of base image indices.\n",
        "      timesteps (int), the number of timesteps for the diffusion process.\n",
        "      img_size (int), the size of the input images.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_list, base_img_idxs, timesteps, img_size, learning_rate):\n",
        "        \"\"\"\n",
        "        Initializes the Model with specified parameters.\n",
        "\n",
        "        Inputs:\n",
        "          data_list (list), a list of training data.\n",
        "          base_img_idxs (list), a list of base image indices.\n",
        "          timesteps (int), the number of timesteps for the diffusion process.\n",
        "          img_size (int), the size of the input images.\n",
        "\n",
        "        Returns:\n",
        "          Model, a Model object with defined architecture and optimizer.\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.l_ts = nn.Sequential(\n",
        "            nn.Linear(1, 192),\n",
        "            nn.LayerNorm([192]),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Define the downsampling process\n",
        "\n",
        "        self.down_x32 = Block(in_channels=3, size=32)\n",
        "        self.down_x16 = Block(size=16)\n",
        "        self.down_x8 = Block(size=8)\n",
        "        self.down_x4 = Block(size=4)\n",
        "\n",
        "        # Define the MLP\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2240, 128),\n",
        "            nn.LayerNorm([128]),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, 32 * 4 * 4), # make [-1, 32, 4, 4]\n",
        "            nn.LayerNorm([32 * 4 * 4]),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Define the upsampling\n",
        "\n",
        "        self.up_x4 = Block(in_channels=32 + 128, size=4)\n",
        "        self.up_x8 = Block(in_channels=256, size=8)\n",
        "        self.up_x16 = Block(in_channels=256, size=16)\n",
        "        self.up_x32 = Block(in_channels=256, size=32)\n",
        "\n",
        "        self.cnn_output = nn.Conv2d(in_channels=128, out_channels=3, kernel_size=1, padding=0)\n",
        "\n",
        "        # Make the the optimizer\n",
        "        self.opt = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Store the training data in the model itself\n",
        "        self.data_list = data_list\n",
        "\n",
        "        self.base_img_idxs = base_img_idxs\n",
        "\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def forward(self, x, x_ts):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the entire U-Net model.\n",
        "\n",
        "        This method processes the input image tensor `x` by integrating it\n",
        "        with the time step embedding `x_ts`. It passes through the downsampling\n",
        "        blocks, the bottleneck MLP, and the upsampling blocks to produce the final output image.\n",
        "\n",
        "        Inputs:\n",
        "        x (torch.Tensor), the input image tensor of shape (batch_size, channels, height, width).\n",
        "        x_ts (torch.Tensor), the time step embedding tensor of shape (batch_size, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor, the output tensor after processing through the entire U-Net,\n",
        "                      of shape (batch_size, 3, height, width).\n",
        "        \"\"\"\n",
        "        x_ts = self.l_ts(x_ts)\n",
        "\n",
        "        # ----- left ( down ) -----\n",
        "        blocks = [\n",
        "            self.down_x32,\n",
        "            self.down_x16,\n",
        "            self.down_x8,\n",
        "            self.down_x4,\n",
        "        ]\n",
        "        x_left_layers = []\n",
        "        for i, block in enumerate(blocks):\n",
        "            x = block(x, x_ts)\n",
        "            x_left_layers.append(x)\n",
        "            if i < len(blocks) - 1:\n",
        "                x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # ----- MLP -----\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = torch.cat([x, x_ts], dim=1)\n",
        "        x = self.mlp(x)\n",
        "        x = x.view(-1, 32, 4, 4)\n",
        "\n",
        "        # ----- right ( up ) -----\n",
        "        blocks = [\n",
        "            self.up_x4,\n",
        "            self.up_x8,\n",
        "            self.up_x16,\n",
        "            self.up_x32,\n",
        "        ]\n",
        "\n",
        "        for i, block in enumerate(blocks):\n",
        "            # cat left\n",
        "            x_left = x_left_layers[len(blocks) - i - 1]\n",
        "            x = torch.cat([x, x_left], dim=1)\n",
        "\n",
        "            x = block(x, x_ts)\n",
        "            if i < len(blocks) - 1:\n",
        "                x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "\n",
        "        # ----- output -----\n",
        "        x = self.cnn_output(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d0040599-3ba4-4b84-ac36-d88b7456dd81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "d0040599-3ba4-4b84-ac36-d88b7456dd81",
        "outputId": "468e9bc5-fcbb-4229-ebee-19c3d3e01f69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# For progress tracking\n",
        "from tqdm.auto import trange\n",
        "\n",
        "def predict(model, verbose=False, show_img=False):\n",
        "    \"\"\"\n",
        "    Makes a prediction from the model based on a random pixelated image.\n",
        "\n",
        "    Inputs:\n",
        "      model, a Model object to perform the prediction on\n",
        "      verbose, a boolean indicating whether debugging output should be printed\n",
        "      show_img, a boolean indicating whether the predicted image should be displayed\n",
        "\n",
        "    Returns:\n",
        "      a four dimensional PyTorch tensor representing the predicted image\n",
        "      after the reverse diffusion process.\n",
        "    \"\"\"\n",
        "    # Produce 32 images with random pixelated values.\n",
        "    rand_images = torch.stack([convert_rgb_to_greyscale(standardize_image(np.random.randn(model.img_size - 4, model.img_size - 4), verbose=verbose), verbose=verbose) for _ in range(32)]).float()\n",
        "    if verbose:\n",
        "      print(f'rand_images.shape: {rand_images.shape}')\n",
        "    x = rand_images.to(device)\n",
        "    # print(f'predict: x: {x}')\n",
        "    # Denoise the input images by following the reverse diffusion process for each timestep.\n",
        "    with torch.no_grad():\n",
        "      if verbose:\n",
        "        for i in trange(model.timesteps):\n",
        "            t = i\n",
        "            x = model(x, torch.full([32, 1], t, dtype=torch.float, device=device))\n",
        "      else:\n",
        "        for i in range(model.timesteps):\n",
        "            t = i\n",
        "            x = model(x, torch.full([32, 1], t, dtype=torch.float, device=device))\n",
        "\n",
        "    if show_img:\n",
        "      show_examples(x.cpu())\n",
        "    # Return the denoised image.\n",
        "    return x\n",
        "\n",
        "predict(Model([], [], timesteps, IMG_SIZE, LEARNING_RATE).to(device), verbose=verbose, show_img=True).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ee0efa2b-72ab-4efa-8e89-8dac57c238b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "ab0ed27997974ce8a3be168ace4156cc",
            "54923506f04a401e8496ace3fbc658bc",
            "76c6e8e008314317974df1f28ca15ca0",
            "ac7f17f89b7848459f1ec5c2462237e7",
            "6da61ec3acf442108f849a6aa024e08e",
            "1775e6c100bb4dc48e1318fdbf229af9",
            "947045cd8a6147d7a0db6246fdded50f",
            "af193cbdc3da4e54a61f8cc3324a3003",
            "db945dc4f4584e3d8400d3bd84d694ff",
            "f369e14318044219b800fbb2e6364c3a",
            "b093605751024caba616b014d812367c"
          ]
        },
        "id": "ee0efa2b-72ab-4efa-8e89-8dac57c238b8",
        "outputId": "a79bdf7d-840d-4c39-8d1f-afffbe02bfa7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab0ed27997974ce8a3be168ace4156cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x200 with 8 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABhcAAADECAYAAACcNXBiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACVy0lEQVR4nO39ebBsW17Xi47Zd9lnrn7ttdfuTn9ONaeKog6NxYUrilGFYhUoGoJiCAYWjwh8z8AAiYdRqKE+eSURVwgRNChUKC8KvCtqxaMUqIYqqjvdPvvsZu29V7+yz5yZs5/vj/sg7sjvuBF7VdQ962Tx/UScP/J3Rs4cc4zf7zd+Y869xlcry7IUhBBCCCGEEEIIIYQQQgghj4h+0R0ghBBCCCGEEEIIIYQQQshywZcLhBBCCCGEEEIIIYQQQgg5F3y5QAghhBBCCCGEEEIIIYSQc8GXC4QQQgghhBBCCCGEEEIIORd8uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXfLlACCGEEEIIIYQQQgghhJBzwZcLhBBCCCGEEEIIIYQQQgg5F3y5QAghhBBCCCGEEEIIIYSQc8GXC4QQQgghhBBCCCGEEEIIORd8ufAVJo5j8Xf+zt8Rm5ubwvM88a53vUv8t//23y66W4RcGNPpVPzET/yE+FN/6k+JVqslNE0Tv/iLv3jR3SLkQvnMZz4j/tbf+lvi6aefFkEQiJ2dHfGd3/md4tatWxfdNUIujJdffll84AMfEFevXhW+74tOpyO+8Ru/UfzGb/zGRXeNkDcNH/rQh4SmaeKZZ5656K4QciF8/OMfF5qmKf/71Kc+ddHdI+RC+dznPife9773iVarJXzfF88884z48Ic/fNHdIuRC+N7v/d7/0/VC0zRxcHBw0V38qsG86A58tfG93/u94qMf/aj44R/+YXHjxg3xi7/4i+Lbvu3bxG//9m+Lr//6r7/o7hHyhtPtdsVP/uRPip2dHfGWt7xFfPzjH7/oLhFy4fyjf/SPxO/93u+JD3zgA+K5554Tx8fH4md+5mfE29/+dvGpT32KD43IH0vu378vJpOJ+J7v+R6xubkpZrOZ+A//4T+I973vfeJnf/Znxd/4G3/jortIyIWyv78vfuqnfkoEQXDRXSHkwvmhH/oh8c53vlOyXb9+/YJ6Q8jF81//638V733ve8Xb3vY28eM//uOiUqmIO3fuiP39/YvuGiEXwvd///eLb/mWb5FsZVmKH/iBHxC7u7tia2vrgnr21YdWlmV50Z34auH3f//3xbve9S7xj//xPxZ/+2//bSGEEFEUiWeeeUasrq6KT3ziExfcQ0LeeOI4FoPBQKyvr4vPfvaz4p3vfKf4hV/4BfG93/u9F901Qi6MT3ziE+Id73iHsG37j2yvv/66ePbZZ8X73/9+8Uu/9EsX2DtC3jzkeS6ef/55EUWRuHnz5kV3h5AL5S/8hb8gzs7ORJ7notvtipdeeumiu0TIG87HP/5x8U3f9E3iV3/1V8X73//+i+4OIW8KxuOxeOyxx8QLL7wgPvrRjwpd5yElhKj43d/9XfEN3/AN4kMf+pD4u3/37150d75qYMb5CvLRj35UGIYh/cs613XF933f94lPfvKT4uHDhxfYO0IuBsdxxPr6+kV3g5A3FS+88IL0YkEIIW7cuCGefvpp8eqrr15Qrwh582EYhrh06ZIYDocX3RVCLpT/8T/+h/joRz8qfvqnf/qiu0LIm4bJZCKyLLvobhBy4fzyL/+yODk5ER/60IeErusiDENRFMVFd4uQNx2//Mu/LDRNE9/93d990V35qoIvF76CfP7znxePPfaYqNVqkv1rvuZrhBBCfOELX7iAXhFCCFkGyrIUJycnotPpXHRXCLlQwjAU3W5X3LlzR/yzf/bPxH/+z/9ZfPM3f/NFd4uQCyPPc/HBD35Q/PW//tfFs88+e9HdIeRNwV/9q39V1Go14bqu+KZv+ibx2c9+9qK7RMiF8bGPfUzUajVxcHAgHn/8cVGpVEStVhN/82/+TRFF0UV3j5A3BWmail/5lV8RL7zwgtjd3b3o7nxVQc2FryBHR0diY2MD7H9oOzw8fKO7RAghZEn4yEc+Ig4ODsRP/uRPXnRXCLlQfuRHfkT87M/+rBBCCF3XxXd8x3eIn/mZn7ngXhFycfyLf/EvxP3798XHPvaxi+4KIReObdviz//5Py++7du+TXQ6HfHKK6+If/JP/on4hm/4BvGJT3xCvO1tb7voLhLyhvP666+LLMvEt3/7t4vv+77vE//gH/wD8fGPf1z883/+z8VwOBT/9t/+24vuIiEXzn/5L/9F9Ho98Zf+0l+66K581cGXC19B5vO5cBwH7K7r/tH/J4QQQha5efOm+MEf/EHx7ne/W3zP93zPRXeHkAvlh3/4h8X73/9+cXh4KH7lV35F5HkukiS56G4RciH0ej3x9/7e3xM//uM/LlZWVi66O4RcOC+88IJ44YUX/ujz+973PvH+979fPPfcc+JHf/RHxW/91m9dYO8IuRim06mYzWbiB37gB8SHP/xhIYQQ3/Ed3yGSJBE/+7M/K37yJ39S3Lhx44J7ScjF8su//MvCsizxnd/5nRfdla86eCzSVxDP80Qcx2D/wz9D8zzvje4SIYSQNznHx8fiz/yZPyPq9fofafcQ8seZJ554QnzLt3yL+Ct/5a+I3/zN3xTT6VS8973vFWVZXnTXCHnD+bEf+zHRarXEBz/4wYvuCiFvWq5fvy6+/du/Xfz2b/+2yPP8ortDyBvOHz5r+ot/8S9K9j88V/6Tn/zkG94nQt5MTKdT8Z/+038S3/qt3yra7fZFd+erDr5c+AqysbEhjo6OwP6Hts3NzTe6S4QQQt7EjEYj8af/9J8Ww+FQ/NZv/RbXCUIUvP/97xef+cxnxK1bty66K4S8obz++uvi537u58QP/dAPicPDQ7G3tyf29vZEFEUiTVOxt7cn+v3+RXeTkDcFly5dEkmSiDAML7orhLzh/OEeYm1tTbKvrq4KIYQYDAZveJ8IeTPxH//jfxSz2YxHIv1fBF8ufAV561vfKm7duiXG47Fk//SnP/1H/58QQggR4n//q7b3vve94tatW+I3f/M3xVNPPXXRXSLkTckfHis5Go0uuCeEvLEcHByIoijED/3QD4krV6780X+f/vSnxa1bt8SVK1eo00PI/5+7d+8K13VFpVK56K4Q8obz/PPPCyH+93Xj/8gf6n7yWD3yx52PfOQjolKpiPe9730X3ZWvSvhy4SvI+9//fpHnufi5n/u5P7LFcSx+4Rd+QbzrXe8Sly5dusDeEUIIebOQ57n4ru/6LvHJT35S/Oqv/qp497vffdFdIuTCOT09BVuapuLf/Jt/IzzP4ws48seOZ555Rvzar/0a/Pf000+LnZ0d8Wu/9mvi+77v+y66m4S8oZydnYHti1/8ovj1X/918Sf/5J8Uus5HHOSPH394hvzP//zPS/Z/+S//pTBNU7znPe+5gF4R8ubg7OxMfOxjHxN/7s/9OeH7/kV356sSCjp/BXnXu94lPvCBD4gf/dEfFaenp+L69eviX//rfy329vYgyRPyx4mf+ZmfEcPh8I/+5cRv/MZviP39fSGEEB/84AdFvV6/yO4R8obzIz/yI+LXf/3XxXvf+17R7/fFL/3SL0n//y//5b98QT0j5OL4/u//fjEej8U3fuM3iq2tLXF8fCw+8pGPiJs3b4p/+k//Kf81KvljR6fTEX/2z/5ZsP/0T/+0EEIo/x8hX+1813d9l/A8T7zwwgtidXVVvPLKK+Lnfu7nhO/74h/+w3940d0j5EJ429veJv7aX/tr4l/9q38lsiwTf+JP/Anx8Y9/XPzqr/6q+NEf/VEevUr+WPPv//2/F1mW8Uik/wvRSqrjfUWJokj8+I//uPilX/olMRgMxHPPPSf+/t//++Jbv/VbL7prhFwYu7u74v79+8r/d+/ePbG7u/vGdoiQC+Y973mP+O///b//n/5/Ls3kjyP/7t/9O/HzP//z4sUXXxS9Xk9Uq1Xx/PPPiw9+8IP8E2ZC/g+85z3vEd1uV7z00ksX3RVC3nA+/OEPi4985CPi9u3bYjwei5WVFfHN3/zN4id+4ifE9evXL7p7hFwYaZqKn/qpnxK/8Au/IA4PD8Xly5fFD/7gD4of/uEfvuiuEXKhvPvd7xZ3794Vh4eHwjCMi+7OVyV8uUAIIYQQQgghhBBCCCGEkHPBAwkJIYQQQgghhBBCCCGEEHIu+HKBEEIIIYQQQgghhBBCCCHngi8XCCGEEEIIIYQQQgghhBByLvhygRBCCCGEEEIIIYQQQggh54IvFwghhBBCCCGEEEIIIYQQci74coEQQgghhBBCCCGEEEIIIeeCLxcIIYQQQgghhBBCCCGEEHIuzEdt+GN/83vANhMu2LQ0lj77Fv6EYztgs7UcbJZWgi0vZVvgW9CmMDz8nojBZhjYt0GC/ahb8juYcF5AG7PIwJalBvajRJuTR9JnT8N+TRK02R6Of5rh2GqL19fxndI0nYOtXcV2wxmOY71SA9soqcht/ATaZAaOxQ9+6O+B7c3Mh//n7wabmT8OtmpVnpeJg/N0lvlgqxnoV8LEdmUhz7GviKdpjL8ZGOjLRoxxl1o472VelT7Hc0UbD38zW/BHIYTILfxNd8E/9D76u+lOwRYlOGbzAn3Nast90yL0UVFFkx7h2M4qGCvFBL870+Ux0ts4FjUNx/HHfvbv48XexHzoT38AbBUf5yUq5AHO6wp/r+G8G14dbKWGcww5xgnxWoniHbuD19IE2lwN4yeaybaGXoE288QGm5nh9U0dfS0Uss84inUySxX+WKJfOYr7nM7kOLBivNa8xDXWTXEt9hVDW2R474NMDhbHw/v2AozPD/7zf4Y/8Cbmf1HExcoEk0yiLRiaTWjT0zF3GDHGykQRF+6CSTPw+paDuXVeYj6PFP5naugLM12+qTzCWIwt7H9mLw6GEIGuqCsTuR+mIp1HitrHUJTBlQRjJV3ofzZJ8VoG/uhQERd1A68f6zhPjbocB4mPbez2EGz/t3/6YbC9Wfl/fuDPgK09VuTp+Zr0OWqiX5gVxX6gxPm1Cmynm7L/lDpe3y/Q/+eZou5JcV1wFP+WK9Nkfw9ynN9I4ReOop6JIlw741jum6aos3LFLjARuB9oFNj/XinbfB/Hoshwf4CrhxCmiX2LNRzvsS3HnRHgDSSNU7D96D/5JcWvvnn58Lf/WbBVc1wrski+/3IN19eJwLGdzbCGmseYv/SF9SPV0bc1xb6wUFzLwW6ILMU4E5l8T5mDbQwX/TGx8N6rpeL6C35lx5jLyxI7W+oYY4bA+/RMeZ40xV5AV+ypohz3dnkwwu8q7j1YWNTndgPalOv7YPt//L9+Hjv3JuYffPf7wdZI+mDztQ7Y0oXhNRyFLxeYncwC/Uoo9mmGiTVZubAfTWd4KcxyQqSKmE1Ve4Q55r/pYjcUzxHmM8XzKYVflSZeP1M873IVcaZP5XazAm/es3B90G1cn21fkQMUz8WiVP7NTPGMUGsEYMs3Mc7+7//kX4Ptzcr/+zu+E2w1bRVsiaF4rteUxyhXPBuNMsU46oo6X1H7mIpnPlki580ixprDnuG1ihley1PUVqXqWa4rJ4A0xLzvm2jLIoyVedgDm614Floq9mS1hTXU0tC3FbcpSryUyEz0ZVVCcV15PGJTsS56GNf5xhhsP/Iv/i1+dwH+5QIhhBBCCCGEEEIIIYQQQs4FXy4QQgghhBBCCCGEEEIIIeRc8OUCIYQQQgghhBBCCCGEEELOBV8uEEIIIYQQQgghhBBCCCHkXDyyoLNddMFmVXbBNl64YqwQiAgNVJvozlC8Zj1AgY/pguDZZIZKSb41AFupEIQqQxQLsVvYj9MFFQ1/HZqIWYiCP5pCvCZUiKKlC4JZ7RSvNS1QPGh6NgRb4KLAiluVBT8qJt53rYIiJvfGCiHSXZy7YaYQektlR3gqR1EQVyHas2x0FGKNqwLHt7MgwipK9O1YIcyYCIVAWQsFxKIFcXU9aUAbv4XXGitEY9xUIa5ewYbpgubi2MJ++YZCSA5dTWgKUdByLtvSSyjyWNEVAosTHP+8huMd6XJ8pgqhdreKKbI3wN9c2RyC7fhUIaKVyuMYrSsmwFeo3i0Zs8eHYDMECsQWc3leYlch/K4QIjf1E/xRhfBldyznzUaOikhzhVab5aEwk+/jd/sKQavqgi/3HRSIy90zsEUzzKNWiXnTWxDbih300cEUx9FIUcDRtRT/vmBR0MpvQ5NZphj/Lg7kyMN1t2I1wDY/k+/BqGG/uq5CgGrJWPPWwHZZUwhmD2ThMXuG976bYp5wFXrok7SB7cyFuVr8LIQwFCKAvoO2qUIRNhJYlxnGgqBYjjWNpqhII0VtqFnYMF2oQxKF+GaeKUQBFXVIoqiH9AWxzYmOIpJGhPe0XsU6qlT49zxSiN4J+TcdG0XkzADr3WUi3sZcNTGwBrGjhXqgij52WuK41qtoU+j4Cc2S/UXLsR6YCYV4XqGoXSKs/WepQgw9k9eUiY6+EhZYu1QCzOWpiWPmF3KczCcKRcAZjn+aYEyc2QoRzEC+9/F8A9rECrFVUxHTKpFQvcQ5SJtyu7qirtUUAonLRmd1C2xXFPMSDmXxYDfGOjkOsTZS6JWLeYKCwl4uB8tEIQRZJnj9Ml3B69sYi5qBdaG1IGabo2sLf4rzfjzFWCk1tCWGXN9pirA2NawBjRRztB5jjWYk8pgVAd6332jh9xT/3jMvFULtiv3j8UweD7eBMWYoBO+XjVkF835ZQV/IBfqy1ZJjY+TheAQ25pw5pkjhmiiuHiuk6o1cjpd0qogfxTOUNFWIpBcY2+Ucf3PsyQGzk2LdOQ7Rl10MRTHJMPj2RzhGlQquScWq7KfWHMe7pmHu6Je4ftoC1y6vjYGbLYjl1hXPznKFCLuhENBeJjZau2BbtfGeFFMsri/sKUNFHdUXCjHxWOGjOdY5WYR1TZrJ3y1Ua5Qi79tWE2y+4vnReKaoJ/Zlvxoo9u/Cwms1DYyfIEDB+KqNsTif4hiZ6YJ4eNmANo7AWK+66O+xSkS+VNRXC7Vf0sY2dorrSqGoxR8F/uUCIYQQQgghhBBCCCGEEELOBV8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzgVfLhBCCCGEEEIIIYQQQggh5Fw8sqBz5CreQyiEaY41WVwu3n8S2jQS/NlmDYUwuu1dsIXV29LnsxmKXtmOQphVITySKsQspyfHYBNjWQjjcYUA1UjRf5Q2E2KY45hZiWxLGtegzX3tIdiMEAWnwgmKeyS5LID0tIuCK686KLoW76NAz7TEdt4p3umTC8LYsxqKtbSKIdiWjbSOYl7zAv0v2pF9qD9AgfSGjqJU8wYK4ZzOTsHWKa5In2sVFIw9xG6JioX+GCkE25IcxbGGGwuiS6lClEqhhFXNsCNdDcWxfF325URD38t1FJv1NzC/HAkU7vHrsmCOlqGYjR7vgu1aFef8MMXxOW4fgM3x5Zi93ziCNtuFKnMsF46uEHk9xHapIfvHXKGsZzcwJ4/GmM/rukIs0JZ9OSy2oY2p8Nt+psi30SXsm4miSLkti/l5CcbiyMS4aIYKkdcc8+bUleMnVQhhnWX4m3X/MtjETCEyGshxVp2NoM28wDlJm1fB1qih+JalEBU0FoRqkwS/59WXW7hWCCHyAnP3yMV5Cdbk+48KheiliTVHaOJ6YZUYi7kh5ytzjPM5cxSCiApxSVPH76oEVbMFW+yhQJ+lUBkdxhgXqwqRXrcmf7cYYf2SOAoBdgvz9ImJY2bXZWGz6gn2fxijSGfSxzmftXENPK7gvXsjWTQut3DtaStibJlYzzHnJ4q6xJouiAerhL4Fjs+gh9cKCqyTQ1Ouk01FzVOmKGRvYBkushn6j2Xh9XJTvnczRaXpmaKv/gRz/lGKfubO5TgJ61hjehb6bG1dUQOOcE0cDuW1wjI3oU3q4lhUNzHnuRGKlQ8PMTbTTPb31MU49wr0g2XD6uIeYbK5CrY8WcglrqIm8bDeiAyc42aGfpUksoObFq7DmYfrgqdjPR0NMG/bsyHYpnP5NxwDr2/ZWI+ZV9GHdus4ZoOxPEalQqjZVvT/JMf4rLuYy+0FcdveBOsss4LzdDzHfswMhfCmqcj5CyKeUR2/52MKWjqajkLQtYL7gZUJ1kLTRG7nZBgXmuIZTRajwHClrqiPHLyeuSBcawSK5wORYi2rYz+mXbSZKQpcH6by3ra7j7nkpoO+tqIQs61s4AKX+hjH3hx9vLuwnk3SIbQRFcUCqnBvrcDrlxnOSxrIdVp/hnNiOYp8oqj5lok4VKyfin8/Pi7xWcvUXRDeVuzRUsUaUptjPiwSRfxkihrPWXj+YmMMj3UUNW6nCoHxDH1j08L51K7LPr8xw9/UW7heDG/fB1tQVcTdHGsR4yrWIrU1uW/2sUosG9cfw8Aar6+hLzttnPd4Yf9fpor9o47zWbPwPh8F/uUCIYQQQgghhBBCCCGEEELOBV8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzsUjHzKm6XhmVsPEs8GTJ+WzukZ38Pwq/xKeSzUz8ey0s9t4pu1KZUf6PNjF86aqt9bBVtTwHLCWi2d3dap4ZuikLp+R5yjOcKtU8RzKwQTPr3KbeH51OZPP7suneBbe0zoeQvdKDd8N1TTFudcV+eyuco7nxjbDp7Cv33YLbOE9PJ91WMfxTuN70udtB12tdrDcZ9wJIYSd4llpKbq36MWyVohXXYE2RyWe/+aX6C8tDc+SFNflA+2P8Ihu0fbxjLVTD3/TmOJ5e9MKnv+WLOgpeBGeQelW8DcHJZ7r6F5Gn4/uy/FjK87yO1jBg/yNE+y/6WHfJqb8m7MT1G8wt/AcQy3BvGEHHbCVPbzPji9PzEMNncUylv+dr5njvI8DPD9RpPK6olcxt8ZzhRaBhueWGibGlGYs9CPC8yZ1D/NQKRpgm7mK89oFrj+RK+cEs8B17ChF39B9XHv6ijOQg1XZl/0Sx2fdx3NM4xneexopzliuyr4cZ6h1sq4p8pdCJ6E3Qj9oKjRdig05p81DXKOGOtqWjaqN8d4xsI4aLZzBayhyt2uhX81KzHMzDWuTopTP4PVWFDknxbXhUKGTFBgKLRyjAbaykP0jt9H3Ojnm1jMN29mZQl9l4SzWgxJ1R+wAx786UaxHBY5je+Gc+4cVxTnjbcxxSYDnwTZNvM/a23ANHHfle8gjzI8rNawzl4kMh1H0FdpeSSDXmeYMz7IdZzj+nrEGtrihOONWyH4QF6hPYI9wrZjFuMcxqvib5VShvTFY0CZTnAEer6jOTldp0uC92wvn0tdn6HeDKWqmRIaingmwb8ZleRwrOe4Pegqdlkxxxr27hvc5WMF13ivlvDedY55yHdyvLhurBWoKrM7Q18a6vHeumbjOxwWOR2+ItW2hYQ3cX9AemY8w9/oO7t/HCluU4FrU1LBd25fnuIxw/fMVtUs8wWslQ7SdjOR91WwNffvtretgO0pUNaAi/rtybrpfQ201UaC/m1u4t3MtrAsNhaZZOXlN+qw3sM4qA0WyXTIKxZnrpxHmgDBX2IbyPPQxVMRVG3OYbmMtMTtRxJmGc5oszHNWoq/pKfqVShOln2Ds6QKfEdgLZ+bPL+P1nRTjv3F8A2zZAGN2PMIHDF6GMVo+Lv+GZu5AmyuKez+N8fqmhvd+oHhumKzK63bLxfra6+6BTTjLve9uTtFv6xWsDdd03Dc467JmZ3SGNcH9Izx7fzRXPA8ssc5Zq2I9oUdyDnNLnPNWH2PYqyg0nBQ6vkWEudSO5RqpH+M9VRTaHkc+xsC1HaxNNqYKzak+1mU7C1oPr1gvQxu3g/WnaaGGsTfHPFH3cN6T5r70+UDRxi5xLSsVmlmPwnJHEyGEEEIIIYQQQgghhBBC3nD4coEQQgghhBBCCCGEEEIIIeeCLxcIIYQQQgghhBBCCCGEEHIu+HKBEEIIIYQQQgghhBBCCCHn4pGVGvQ2CncUPRTDGPyuLC5xtPsA2lzbR1GKL9RRRMN3XwLb+Euy4MSwfRPaHDuvg20lQ/GdqI+ifXMHxTFOp7JQTVpF0YvUQAHN7AjFAyf+ffxuVxYoqvoocHNVIV53J0WhkO4qCkeZx/K97yYoruJ4XbCVH0Pxp88EKGzy9P7jYIvXZMGSe2soKPa0rVBTWjJCbQi2zUIhkOfIQtuhwHF8ZwdF7vZLHDdvFUWM3AWRu2QLhb3L9ABszzjot0ch9qOmEKWtteT5O47wvlvoysJvoUjPfAXzi5HJQlinCjHCd2xj/w9tjM+48RrYkpEs+KPVUIyrsYox5qxj/pq/gmKz16/hfeaJLJgX1HEu9TF+b9nIc/Tbqo857O6ZLG5mzRSCkHWFqPYQxfxmQ4UAWkv2mXyC8+nXUHxz4mLuC05QAMnyFIJisbxebOjoowcK4dG4hyKd48dxzOyJbOvcRXHlw6dw7TnJsK9Pu4+BrVvekz4/dhfX5rsN/HcJpY/1gFegQJwdo/Cbnsi5SSuxr/Ns+UU6LR39auhjvCe6nDiba5j7qjra4jr6aNPAvGaYsq2hEK9NfBQsi3NsV09ReHSW4XqhxXK9Zevo2+UY78mcoG3lnSg8OjmV+7s9vwJtil28p/QQ78l3sW9uQ85V160GtMnHmL+ODJyTcI51YLOOfRvrchxHBZbs2SmubcuEb2F+SaeY38ORXKM6TcyhhsCCwwoV4zpDUc3jkZxf8ir6ha7451j9GK/VsnDNsnOszYtCvqCnuP5Ix3qmluNa9/JNFB01NTnXNkscnzsGjuOWcQ1sron3OX5JjhPDxj3VWEOBx5piv9dxcF9SV9TE8/2FHKoQdy8sXP+WjdzCOR5XcA0shJwfjRTF0MeuwjcsvFZzRRE/fTl+VhSitWaCYpyD6hxsQmCdHBcKcfUTuWYKS4VAZeMh2J7o4z52+zLW6+5M9qFeF2uSlolr2FqJ+za3MgSbXZPXxKequIatmQ2wHer7YBttvQq23gTvybdlf7EtrDXGAvPSstFR7EVfm2GOyTSsq/xVeZ7XNcxDxRDn+KCLz7FG2mXsXBt92bFlQWE9xeu3BK41wxBrkMhQ1M5zvIdpvpATxxifd0PM57HAfGumWHNoLfQtY4q5I70t38NwhM8IPuViLVfsYO64sok5vTXDtf2sKwtcGx3cH8UO2ioKcehlIquhgHHVwFg5K9GvDk/leZn6mEc719D3rtoo0D29hb9ZLdGHXpkfSp/1QvEMMsC1Zq6oFzUb1zxRwzWv4S48D5hjv1IfY+XxIV4rHOJPTmOM/3Ed7+vS43J95fUx/ncV6+xGgTntC/Ye2MyWQpR+4XH/Th3vc4LLrIg9xUO8R4B/uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXfLlACCGEEEIIIYQQQgghhJBzwZcLhBBCCCGEEEIIIYQQQgg5F48s6OzEKC41qKMAV+a+LH3W9SegzaebKLi83W+AbX+KAitnniz0cl3fgDavuSgsOfFGYGudKsSbSxSNKbbk/ppHKIRTOUTRzlkNhXt27qNQ2v76nvQ5eIhCWyeNLbC9p/0psO0pxD3vWrIIyE4NVTv6GgqxxajzJq5kKJIYX0YxpaMj2bWuPI33NB2iwN2ysRKgiM6RiT50vbbgyz4KfN3VUGBlvYriO+lsHWzHjiywdCl4Bdr0coyLkYW+UM9QMMfS0VbxZPGawwBjbLWFTnQ6RfGawSGK3qxM5PxSc1C053dPUKT6kos+ah4pxPEWxsOromjP5AjFbOrdPwCbaKCQz/w1FE87DGTx4L0CRek2qijGvWwYi/4uhDjpKQSzWw3p87SBosZtEwVj9xyF6LW+DaZmQxaXGjcwT8cDvH7dRl8Y+wphIwfFq+bThb4VKIRlO/hef2LjepqPMH5CQ/bltUYD2gQNjKfgPoopnvVwvWhV5etnDbxvo4J5w8gx79VVYlArGIsrXXlNHVcV43r8yOXKm5ZqhvceZyheV9ryvE8NnDv/2tNg60W4rtzQcA0JMtlHbwqsyZoz/J7mPwu2hz38zdLGe+p05fjXrmB9VIkwBg7mmEt2Qswle4nsk+kMx+wpE+uQY4U4btvEtWzuyzVSGGK/yhz71YuwzpnH+N3GKfq80ZT7Zk4wb4QWrqfLxEQh1hoFilziy8KPrRquubdPUXhzpqFPjbceB5u1KtfY7TnWS3GAPpufYLvsDGPC8NA2d2VByrLEvp4NUbTymiI2K6vox+VQ/s11F+u4T4XoP/bruB+oN3FO8lzOI5UVzNFXShQXbGuKtbqvqKFsXP/SBZHK7gTveyvAWnrZMA3MJcl8CLa5LteZbgtzl7dQdwohRHVSxd/UMX4ebsr5K1CszeEZ7omzFHNcO8N6oKmj7XTBr+wa1jMDhf5qQ8NrzUf3wXY2lfsbJFgXTlLFupajzbbwN52BXD/Opjiur47Rt++7aFut4DzpZQNsU0eOn0LgnLjVL0+c881EauHE19u4b7BnuDc/uSPPw5GGuXvtKuYh99pTYHv8FPNhL8E9avhSQ/o8Vwiu34pwXSmruNbrivqikaPv9heEx718CG1yRd02UezXG3fRd48Ujw6DEsdyfFnOw+3HHoM22y3cf5UTFJbevY3jfdzA2DYLedxCC9cyYWNsNEocj2XCwJJAdG1cCyouinG3d+U5KO+jUHM+OgTbTfOLYNNMrP21DvrLxJD79nwF98n5XTCJcI7PfAY+5gS7hrXOcSzHSl1Hf9f6GE9Tfwi2FcVzvmkNYyAY4B7bflFe37QhPkO9Y+F4v9r4WrAVazi2lQSfFyczOaaqHu5LLBfjoqwq4ucR4F8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzgVfLhBCCCGEEEIIIYQQQggh5Fzw5QIhhBBCCCGEEEIIIYQQQs7FIysk2iaKbRh9hTihIwt1ze+jsFalch1s0QYKa8wLFBrTrsriZl88RHGp1QTFYMoq9n+6hkIV109RXO4T+7IwyHqBAkveJbx+FqHYzPEMxVQaiXzv/nVUMTFeQYGVKELhkaFCzOttljwnKvHm4j72yzY3wVZbReGh0y+hEI57SbZND9FXPAfFiZaNpEAVnZqJY9QTspBWOMM5uLGBYjBaA31tXyH0pj0mi+jcDxTvDTUUuJmM7oGt1kBfXslQ6OXBaFX6XK2igHGvRFGtZvUtYDv28N5tXb6HuzHGZpmiQFljuw22T41wTlZcWTBn2sMcVL2GgvGvCvR3L8X7nG2heHulJ/vLto+5apbjPC0fOB4tgUJp/YkcB9MMc9otD8Wa3LXLYLPOcDkbDmUBV93BPoRDzH3JANeVroVCVUGKYk2+LotyjicoPj0oMD6dAYo6OWeY970dWdzwC/dxLNwYRa+0K3hPszHGz0Yix3UvxjnJQxS8LmOVuL3Cv2MUtPM8WfRT87H/TYUw7rIxFOhDhY15s92Sc0BWxTzUzIZgW7uGImDhULEWLLj8enAFmuinCpFBTK1C1zFf6cU+/mQkr1tODdeZuo4/4J1h//MYxZp3jYb0+ch6CG3MGubkQiG0mwwUwr1Cvt5ZiSKpqxqu19sFxp0wMA8Zimpcd+X1WR9hrNsOigAvE7aHecPpYb04mcn+Mwgxb+frWLPOJyjGGR2jsPdkLk/AF++jj5kWCoc6zyn6cYS1imnjvic/kWuOkY37pfEA6z1jiOuOWaDw8+vdxethLndu4PVfO0VnfLuHvhcI2beNFzH/7Ae4/t1V7McaGgokbuItCfeSPGbuEPNPMl/+GmpYYj3geiiOvZ41pM+bJg7a0RmuMQfO62B70sZcstGU14bdGMd2WEEfysUXwJY8QF+YKsQ4w6Ycn+sFrgsbXdyTRCXuuZtWA2xBJsdsXMN90Iri+UPeG4JtUMd1JlqV99ydCOu4DR9zUKDYo7lzXCuGG1gfJQsCxlaItV0xx7pt2bBSHI+z/jE2LHEtnm7L4+sMMR8GY8zBtz6HguK/O8J2oxXM+5qQ/Xu1xHrA19C/tT7uic8GWDu/7GOtEllynljX0JdDG79XneNaNvPQFsW4PosA76s1lvPJ9DX0v5ds3HdPAxzHww7WwG3F/qJmyvd6pBDL3XYxNuwOrknLhFbFHHy5qSjYp7gHObor+8I4wTF7+jL6e22Gea02wrizvJfA9ild7kdFUdO/pONatmai7+2k6Bu5Ik90dVmc3FM8f5kaitowQt/Y1NGH3Bh/czR/GWz7NTku6tex9tH1d4LtMYXm+OlY8ZsVXO8f6HJN18xWoU0ZoG+UCvHzR4F/uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXfLlACCGEEEIIIYQQQgghhJBzwZcLhBBCCCGEEEIIIYQQQgg5F48s6GxZKF5Rr6O6xMaJLARyso2iETXFryZDFJzx2yggEnZlQSg9QFEaU0PxjcoIxfc2dxTCHfYfgG37c7KQWdDCdzJ3jlDY13ZQ4KY6QdGVzoJIV898AG2airEemdtge6z1abDd25dFOuLhk9Dm6QaKZc5SHMfkBAWnnt5BYbC1pty3NYWYaHIbhbyWDVMhzKi30K8q1rr0uVvF8T4pFHOco7CRs47zoheyyM0gQhGWio7CTJ5CHGc4Q9G1SYnCV+VCSigzhbCeQH9PQxTR2eoofrMlx3bnD1DMzr6Mgj93QhzbKxsY/+FYHtuZgdeqhyg25ZTYj4GF/S+neJ/3F4Swxq+gr6w80QDbshEqRK/dOa4hsSPfv2OjUOV2BcWOsinm/SLD9eK0lHNdpakQiKqhkNStExSq0g5xPgcOfjdaEJa1c1wbhI/CZvUAc8npWQNs+lSOs5aBIpr2CQqbljaKwZUKceVsIs+BpVivzVX0W7/A/NLaxr4lJeaEDV3u7yjC+dV1hZrVkuFXMN86GubWeVdes2caClV2N3E8HIU4bk1/DGyjvc9Lnxsm+t48Q38/uH8TbNeqbwGbH2CczetyjpynCsfSUBzTX0GxxvAB3nujKucX+0Sx9ky/ALbSRDHr/TH2zVkQpPZWcS7jKQqDnimER30H15DXb43A1h/K915bwb6aOcbdMjFPUEROD3DuPE/eSzgNbHP4EHPtnuI3RwrB+1Ygi337W1izmjd9sNVexZjr6bfA9mAfc3KQX5Kv76P/G02s9+YjjC9HodUa+HL90j/BvO100VZNsR/jh7iXm/nyXmjNwznZuYwCj0GlATYtw3hq5utgO9Hk/dGaeRva5MU7wLZs1DTMQVqKsT6Yy77gtnDtbAQN/AEH1x0/RF+rtuX93SDFeqw/wn3KU5fxWuMZ7p2PJliDXF6VxWytBOvweYZ5NS1wT5kJFMYNpnI/ihoKjqYKweUyxznRp1j7W7YcB4cPhnj9EIU+72s4v5Up7tdrVcyZFV8Wb50mKD4fD/E+l42kirmvEuDewpjhWjA9kb97/zWcu+E25qv0CvptbaZYHwZ4PW1PjpcHU9wz+FPcb3S2ca0JruB9DmfoM9We7DNp+gVoE3bwPvPuFtjSVRSz9iLFXusA65daIPdtvo1++9Z1xXOy1afBttpXiFKf7INtti7XsrpCkNqcoXD90Fjuf2vtDnDtPS7x3htVxbPKlpxLN2eY4ztjrHNu9tEPygCfX4YR5vhgIUfWFaLjq1uYz/UI/cxUrYsV3NMUvrxOWQY+b3B1XMsyDfsvFFrZZog1jO0o1oex7PPRHrYZhL8Dtt9fxTXVvYT7L13bBduqkGPvdIprWWbgszPH+fLiYrmjiRBCCCGEEEIIIYQQQgghbzh8uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXj6y5oBl4ltS4wHcTpz35nEI9wHPjujGe25vM8WzNVh3PSa+t3JU+v3oTz047SRTnFtZeBdvx710HW8/Gs+9Wv0U+121wG8+qXF3Bs8Hmp3hmceXteDDq9GX5Hi7l16CN/hbUNZjs4RmunfkG2NI1+fzXWYL96kZ4Fp69uge2yu3nwDb3cY6PdxbOHj7C67f9r4J3WzqOZTjEM9vM6/IZa64idiz/cbAlKZ71VrfxzMbeWL5eXMM50Tt4Pt6lPTyP28jxTLuZibE4SuRzr0djPAuzcwPPRQ0VZ/6lGY5ZPZbPzDM2Ma6NBM+gvuLgecG3M8VZmD35/MrmE6gZcXSI5wxrJp73rXvY/84uxvHkRfk83Mom5oNONgTbsqEZGBdpFc8LzU5lXztMMS6cOcaAsYFzpSvOSl1bWGr0LvrovsL3NJXWiYtrVGngeb7GSL4Hz1DoT6SYu1PFctw08KzhycL1OiaetRl18bzJlRmuW94cvzvqy2u4dwXHdWOgOCfdwr4OujifRQ3bdQ/k/FhuKOJOR/9ZNjwD72FsYV52M3kODBNzsldirRKFeN72fvYZsBWxHJ/XbJynsYkxdqqQD4nmqPNU1NG/40I+uzer4Vm74wjHp3+Mvvx4in6bTGRdioqFZ7juv4TXSoxXwJYpzvedj+X4mZVDaDMt8azadnAZbJV17Ee7wPFoLNQXUYh56XSMvrFM6BbmZMWx1CKbyWORTxrQZryG63wjxhxtxQo/OJTnXFPUDMc6zmV+jLFjd3pgMwPcQxmlvKY0RpgLtATP2bWrGJvxFM9E1nU5d2c63lPWw7W6bWNfOx6OmVOV8/SlGR5ErN3Fc3wPXKzRorbiXHMX16xwIMdA21bkRg/P4l42cnsItoqiHjBKOX46c8xdkYtjVBToa6PoFGzagp6YLfBcdifFvloh6poYVezbbIz9EHV5/7hqYz1wMkMfLS305XyCmj2DSw3ps2djjZYaeHa4ZeDzhsEBxrpYlZ8HdDfwe6sp7tWf8jB+dBfzo66oWYUv77XulVg3X0sxRywbmUKraTTEHJ/P8F7TBb1CX8d9bTTCWElRHkNM0f3EwxKN9QX9qWwFtTAmJ7j21x82wGYp5s+yFOv/qexH5mWFhlyJe3M9fQZs0z7mUsfA+mu1ic8cmgsCq8kEfbkYYszeuXUXbL+DS4G4voa/KTryHJs5+kZaKvTbRphblwndwGeQLRfn3U3xPtNTOYeNFevFi8EQbJcruDdsKJ5tJQo90Qe23I9Sx3maKnJr28U9QtHEekhT7KeLoex/bcUzidcjHB9P4Xu5j8bhGO+9jlsyMcnlmJpdQu2Hln0JbNUI10E7w3xSlrinCery2vtAoaPaUMS1nn95WodfBU93CSGEEEIIIYQQQgghhBDyRsKXC4QQQgghhBBCCCGEEEIIORd8uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXjyzoXM5QbMOZoQBt9oSs+LeuoWjEgTUE29o0ANuD11HoJXJl0ZJkHdtsRCiedBqgYE4LddLEW45Q5OZWTxb4KOooVFUfoqhGYaHYzPAB9iPbkUU0zDPsf3OEqiD57ufANv7/vA1sq898jfT5paex/60vorjUxEXxoPEmivu0Riim0ghloZTWKgqnWMMvTyjkzYThot/OBAoP56UslOY46HyvdHGOV3YV4uRmA2yR807p87CCAuBbR3fAdqAQYRaVIZiereF9CiHHf2SjOHFdRx8azlBUsBKjMFDUkgXJdMVY7+oooNk/xbhbu/wlsA1acsxO7qJgr6cSTkzxPp0T/M1CR4Glk4X81bBQ2HCQosj2shEKFO5zBjiWrcuyL1T9IbQ5w6VHRIeoLDvdaYCtrMk5ZmopBKMVQpvVyR7YYvEs2OodFH+yx7KfNiYownSz+z/AFigEbg0dr1/05f4+zFGEcdXGdWx++ARefxeFmRvxUPoc9nGN1SsYA4pQFNXLWGI8PseG2Zoc//02CktpPYWI4ZIR6yhemVo4HlFfnpdJF33jqIp51NQwxiqeQrgvkHNff0FsWQghSoXoXUUhTh4WiusrxPHud+V8Xq1sQ5vmMQrQBZ0vgM3qN/A3HfneR0eKfzszx76aCkHYboj13Jp5Tf69M7xWHqDf2j7Ob6+Ha/GRjzmtPZfFU3VtD9qIDEVAl4lsjGuF5ivy9MJ9Fgbmbb+L4/r6FOcpTXDvEuhDuV+bWLNWFXsG/UWsoQYDrKfbG7iQjbtybh1eQiHLswiF/SoR1ipWiYLLWSpffxSgUnZHIUweB5hHXBPrmXz8qvR5amDcuAXO7/o2xpzlXANbw8e150FDzntBD0VIXUOhorhkFJlir9jD/UDpyGvxvoNr+kYLaxBHUVeldaxH84Fc2ybpH0CbrofrQhRhLixHuB+oVLFGsFI5722ia4sTHe+zaKJ/R/5VsGkDub+lUBQvY8wbQlGfRim2a8Ty3GlDzCW9uUJ8vorx/7pCTXT7cgv7NpFzZjjFeE2quOYuGzMb60BDIYTtKUTAZ91D6fPBwmchhCgnu2CrbyryWvAS2FrdBti0fdl54wj9tptjvnUyjIv5CO/d3Mb1p6rJ/qzNUDB2xcUEUOTok5ri3yC7AoWwCwPvwRvLPpjauM+32uinWz4G/EZjE2xtTIei7Mr5MPfxN0Mbc1OQYj+WCU1D35hm+Lwu7+B+sVaXaylDx2ct0wnW9NrqHtgeZphL9RD3PdqmvLYHQ4zhuYfztOvhPM18zIfRAO8zTOTvmjWsTfS5QgjaQN9rmPjdTPEMLAkUdVMm9zc/QR+dhfi9swzXt9NLGLN6gPH57MIGfaTYi5Ymzl29/PKEzvmXC4QQQgghhBBCCCGEEEIIORd8uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXfLlACCGEEEIIIYQQQgghhJBz8eiCzgrRS3OMAnTFgpDkq7dQMCeo7IKt/xYUg0kTFOSoP5AFiYI1FGo9eRFFTK7kz4PNa+G7lf+mEPy8enxd+ryyhkIe3WdQSO7OZ3fB9uwEhTbMtizEsu8oRIaOUYipcobiW/HzCnGf27IglG9/Htrc3EFBrm+cPQU2ewOF2P6/cxSS+c6pLEqd1FDEyPYe2f3etGQaip00bRTviiJZFO2sjX52SaAInRWjqFt3jt/Vvc9Inzs+Cv4d9K6ArdPE/rdjFMK6HaPAUhDdkz5/3RqK0vQj9MfD7AhsQkO/WpvLokJxiOI+o5UvgM2PngTb/BR9rbKgl7NReye0+czsv4Nttd7Da8UNsN2JMS4eu3ZbbtNFYcl196tAuHaC+cQ20D/Eqewfd3ZREGnmou81FYLRNaEQ4NuS/ap/GxU5yzGKDOaW4lrF59DmoqKYHci57r6Hgpy14y2wGQoh+CxUiPZuyDFrjHB8vFYDbGejm2CLj3bBZtdkobfmEPNNXrsOtr0Kik2tfglz4egyiuOd6XLOiY8/CW3MQCG6uGTMc4xt18EcWWvJ46avYT4fa/fBVo1wrrQSbYkmz3E1xH7NShSSrHdQYNWbYbuswHtqDBZE115DQbGZjutAmaEYYRr4YDPDBV/TME9bPooY5scdsFUUvhYviBHmCsHMioX5a1jBuF5X9F8LsL/FgiiqtXED2nSzV8C2TMwKzBHCGIIpLGQfLXB7INJtFFJ+7gqu/XsKvd/JHTnXFhOcI8tHn72PGuTCHON+SaV433GG0udU8ZudgSJOYowT38HfDMqG9LmqEHevKQTHDxUCo1aM/chSOedP6ri+dnIc/94Z9tVuY245sjF2ZgN5/QsaKAQ5KpZb5FwIITSB9a7mY11lWPIYlTruGY5nuCc+Krtg83SFfwg5lycKfccdhdL5XCEYOTdwzTIsnPcglX1mEGHenunoo8EU5z3S0NeGpTyOKznedyiwLhwpRLZnGa4DOzN5rTjQFCKkGq6RKw0UNb9axfzYMlFwdVCRx3F7huNqFPi9ZcMtcB+RDTFPFDNFjVCR1/qtK7iPyPfxGU14MsRr4VfFRCFYXL0sr1tRjtffneEcz3P02/YY4yA7wzVPhHLnEg33EV4H9w0ddHkRulivb2AaEoMh2vKFTbauEFf2C7y+3cd4/Gy+B7apj88qtAXBb6OG1zcj9BcnxnhZJmY21o+u0wBbqcg7+kjO1bmPz3a7q7fAtnINfdmOMC9PjxR71Jm8kIwc3D96q0OwHWc4n+UMnylFM3zmaBjyXmKqqHMMC2OsnOC1ThSi42GBgeEU+NytsVDXl4rniNEa+mMxwWcJ19v4bLhX4nqzWsi/cWLiPTkl5pKpQlT7UeBfLhBCCCGEEEIIIYQQQggh5Fzw5QIhhBBCCCGEEEIIIYQQQs4FXy4QQgghhBBCCCGEEEIIIeRc8OUCIYQQQgghhBBCCCGEEELOxSMr6uoxCrE8VAgxib4sDDJebUGTHYXAUtZHAYpp5zbY/Noz0uf4NRRASm0U95hXUXwjzVA0xs5QkGPiyfeuD1FExjjuY1+xmQjnKAjl6XLfVlr3oE1yE0UHN55+Gmx9DYVq9gxZzCfeQwHGzRUUeopdFPw63UfhxxeaKKKVN2RBGEshImPkX55QyJuJYq64rxbel+3L4ilGFwXW8nX8nmWj3z70UeCydTaUPgcm9qsTKmIgQfGaeIr+EWmogGgX8vVuHmP/21X0KytHJaygRAGh/QVhoMYcBeKGihiuWCiIPruHIkD1ppxzpg4qRLZSfP9qZyim2HdRGCjUUdzTPZHnzm8rRPtwKJYOF9OcsGycP5HK8/JEhmvDUYBznCjiLh1jvhoOZVG3cecY2nRmL4DN1zBWRI7zObiD/j31ZGEqd4xiamcKkaTqCYo8B01cV3RbXqP8CMesNBWi4AYKOE2HGCudTTk+TRPHYqChgONGisKgkYF5I8TwEdU1ec3TQxS4sjKMsWUjK3DcjBnWHNpEzsGGg3l6WsN4crcwz1ViDMbtmlzTjMcozDY0/wvYanOsL0rvMbB1ShRTzJuyWuCKh/0f9bEkHfk4Zo5CTXHWkP17Pt7D7+UKkd46Cq7tZ5iEO4G8Zj+cYr/OcHjElqIeKM8aYNN8rAnGjjyfRzPMey1FzbdMFCXmd9PBmKjsyLkw7aLIuT1CX5/mOJd+jjl5si6PtXb/MrTxigbYNrwvgS2d4G+GKfrsYCALea4rauKiirV/LcD+TxvoB37YkD5XHYXYYvEi2PIhxqbZ3gWb1ZFFJNd8vL45wzgZarhHq3m4YbI9vPe1XF6z8i6uT8a6QiR8ySgtzFU1DW1nQ9lnEv8htNlZxzmwJ5j3wjHWG7n+OelzpBBDrW2hEqyuEILtKwSF3Rjrl6Qm+9E8xDrCjXGOkxCvNXexFhotPLuo1fH5xvAUx3rFwL3uPMZ9w9FCfZrWsQ+hjs9G6hmO4z2FgraeK34zlMc2LzAXVhPMG8tGlGIda9fxXnUNc4ddyr7bizBPR4rnWnNDsafvPAO2pzTsRzKR4zHqYvyYE4wLW8c5jgKs0zIb9+uVLfm7DR3jQjfR1+ojzKWWif01MqzTHIXArV+R48VV5DRDwziOEpxj38cY2m5ijgktee2yFfuXeRXri/p4udeMSkUhXJ3i3GU5Cj8XTdm/8wCfMZXZdbClD3Ge9ssvgk0TOAc1uyF9dlP0jUCxDc9i3GfOBe79ZxmueUEm76Pu9zC3ZjNF7VbBuiYbY8xqKcZZ0cX6tufL62xiKR6Y2GirzxWC6F3MTXubuK8/S+X5M5tDaBOX6ENR8uXFBf9ygRBCCCGEEEIIIYQQQggh54IvFwghhBBCCCGEEEIIIYQQci74coEQQgghhBBCCCGEEEIIIeeCLxcIIYQQQgghhBBCCCGEEHIuHlnQOU9QcLmZoiBRek9ud2nlOWjjX0PRi9N7KBpRrTbAdrcji21UuihKE+R4W2cDFBBzE7z+Wy4rxF+uyiKgbdRvE/llFOO8PHodbLcuociIe29T+jxWiBUaFVTBHH4R25XvQoEVsSvPSXx/G5ps1XB+05sobKLXUVCo6qBAzLEpC6dca6NYW9ZFEZZlIytRGGg4QFt5JIsRzW0cxzhCITbrCRRYedZG4Zt9Q/b5rIbXmpYoEGPcRlGa6wEKWo1bKG54+Al5/mpr6EMHChEm/RB/81BTiJPasiiVJt4KbbSjB2DbaqH4TtnE+AkzuW+hfQvaTKcKMdSrGD9dEwV0Gg9xjk8WhbWiu9Amz3DOlw1bV4h0BWhzU1nsKBvi3HkzFF26U0dhZn1yFWz1hbVgVSE0G6boe/UJikZNvCfB1vVQMGstlOfP0TEu1hsozNrrKURwx5tgC6wvSJ+nFv4bgexEId7+BIr5NbRdsFmHclwXl4aK6+M6dj/BvLe1grZxgeNdPZPnM1rHGMgOUBhr2XAKrH2MBH0o0eXc5IQo5LWz/m6wjU5xLb4t7oMtMIfy9esoVBm9iIKIHRPnbq4QZr49xH7YphwHxwOM9ZUqjo8zwbjobWBenvTldpaNa1ahED8OLYVqXIDCpv0FkfesguvYhkK4zp1jrIxtrNOi4SWw1d8ux/atAX7P05f73whlLtaxrkK4NknlsT2eD6FNb46i2BUTfbvl4vUzTxaV75e4BvQfYj6uraLPVi6jb1dN9IO1QP7u7Az7OlEIQXfmiq1bidc/LeRaLuthHdSpoDB2s4P+X/dw/dYNeR3b66Hw/FOK+57VUYSwphAYLRTC3smCUOOsi+uaUeCau2zkMdYIE4HzV63KuXBuY30qhpiXZskdsHXcIdimmiz8HGYoNOsUuD4ZBfqy5r0CtoMEa7m1WPaPKMT9daxhXagJhXC13sC+TeW4W01wz7OveOZRU4y/VsOcXBry2lnNsa8CXVtMc5xzvXmGNg9FfI1AzmnuFJ/P6Avr/jJi6bjumjoOZhQqxKtLea7MCuahZguv1crxWqdHG2DTY6xLnFX5en4V48J7gOvKZozrelxiXp5m6JPZfEFIWUP/HoJFiMLCaxUYUqKW4Zpnugqx9olcG54IXB/MOq4/9RqueVUD56Ut8DenkTxu/QhjSpQ472HwyI9D35TExRBstol1SKDIkRVDbhfFOBbjGOduqNh7zkdYW9WbKBBdW6inj7r4YHUvxbXh6hrWIZNj3KtEueI56uZCvZVg//UYxyfPBmA7U+wlRInPTDMH50ArZb/dEJjPT8MDxW/ic6ZQw/4aFv6mX5GfR2m5oq8mPq+zFM9yHoXl3pUQQgghhBBCCCGEEEIIIeQNhy8XCCGEEEIIIYQQQgghhBByLvhygRBCCCGEEEIIIYQQQggh54IvFwghhBBCCCGEEEIIIYQQci4eWcGkXEWxreoMv969JounhAUKchztobjv6TqKRJkPUUgiWBAeKdbw+pNTFK+5VOC1Wg6KUN1/iCJOrfuyCNCDTRRqNm4+D7Zrz6KISbeL4k/GmiysYUX4zmfqD8FWt1HEpHFbIR44lAU69WdR7KPyMgqKHOzgbz5zqhDaM1BkpNyRBZDSGAVRSk0htLNkJHUUeqorBFaEf136WJnimJk1FIhyfPTH2QhFnTxTjsU1BwXc9kcowtRQCPdNUetI2FMUerrxzDX5ewoxP7H+GpjmIfbDP0RBK3NNjpVIQwG6eYSiPSf5i2BrZDfw+rWG9LnbQwGdXcU43h6h6E3DxZhNahj/iSP/RpaiSG0Z4PWXjWSM41YkmAOOF8ZyWH0XtHFdFLRbUQiFJwoRx+mCqHbLRJG7ucLhxxle3/cxbzYHKqFwOf7NfRQsG+yieHiwijkhOMH+VjNZ+NVewTzqGBhjob4DNiNAUSpnvyF9HjYw57udI7DVbVx3+z5e/2oPff7MkuPfcFAA0bDwnpaN3ME6Sq9j3i8XRJ4zgeJb2hTnxbuhEO7so38c6bLQ3uWeQkj5FOu7qINzF3ootKfPUVw9ceUaz/Iwd+tD/F5RQV8oBpiro0gWU+zbmA82YvzNeR3zy+wQ66GgIY9ZW9GvWyMcs+dMrPnONGxn1jDW0+FCbVjDGDPHy71eVErFPiJGnxoI2ffWL2O9MTrBXD6dYHzlPcXe4pJcq9y/jDF3+RSv72c4b/0DzI/FCGPn1bG8pjRvYJueh/0Q91DwXuug7akFAflUIby7aeBYjEMc/6GFuVzT5Ryk0MAW7hxzkp0rBCOHmINqOsZwthgna+j/QYq5cdnQbcV6V8X1w1kQpdQj3JMMS/Sr0wLrsfoa7l12hBw/Jyl+bzp4ADbrMtbEZR/FpoMU/SrNZV/WI2xjjTEXdgO0VUP0oUEkj22Uog/NFP/0UhMY60KRfl15qRBzxT5CC4ZgKz3MVZZCTLSj8I1+2ZM+jxX70I42BduyEY/RF0If63Cric+Z8rmc1/Qh5sNVG+v1084e2KYZjm9r+g6wtRfiMZ5ivRH2FeudjnVbvnICNtfG+9SGss83XLxPLUZfqBc4tr6GItVGhnVUqRBmHzbl7xaBos608N6NB12wjTzs77iG14sb8r3XFELQhkKsXTew5lsmHMXi67m4Dlo5JrbDgZw3+4q93IZC6NxNngBb3lfsEcQ+fnf9Semz7T0FbYoxCnYb5WWw2TbuzY/q+LxI78ljZPl4T/1xA2xpB9uFijU1szBW6mNcM8aZvBfKHRzvZB1jrB0/DrYNA3/zpQzvvRzKa1B3A/vfUuRWw8F64lHgXy4QQgghhBBCCCGEEEIIIeRc8OUCIYQQQgghhBBCCCGEEELOBV8uEEIIIYQQQgghhBBCCCHkXDyy5oIzUZzZOMUz52Yr8vlSpcA2ZYG2VcUx9eMcz9Gq2vL5oPcP8dzb9QmeL2eu4fnn+0d4jtZsAzvSTGVbs4vnRrbfgQcvzv8Azw9+4h13wXa2L58XGHXxDK1nEzw77fM5nuN4I8Tx0DryeV5re3g223gbNS9Wp6jp8IqjOPs5wvH+WiGfF5bU8T2WPVacJbtkVEL00dJFvwpS+Qy0rqs4r22O+gQTE7UIkhJ1EtxIPqM0HOB5jXYLz6UsT3BewiM89zPYxnOjj44/L32uVRViDTfxvLYyxN+0gktgs3X5nL6HHo51zcJzHkca+m1q4HeNdCh93mhh7N++i2czPv74FtjC1/Es3HAdzxnMZ/L1Rilev6mIsWVjHqCP+lU8QzAo5VzXqeAc5BaeMxiOvwS2JMJ82LJknzzro+9lLp7bGQ3xfM9JF7V2WpfQr7xM9oUhtBCiFeFZkn4FY7Yv8AzE6lyOFecEvzdVnH+aNfCeOhXMQ7NdWZ8k7qGPOvs4T3fegueM7w5x3bWexHt3TmU/SPu43mUB5tVlwzIxT5gx5siTgezLUQs1BmobWL7Z7pNg06I22JojeS3QBeb30G+ArSOw/y0Dz3rv+ng2cDaV/SOfK/Sbaui3VQPri9M+nos8LuX4XFdoGJwEuLbVYsV5xH4PbKkm11aR4vz6TtAA28TFvJcle2AbVLF204fyOcCbVYz1zHjkMv5NSaZhfvF99I0okcfCzzEfrK7iWbOjHOueyega2BbHeq3EsVad9134eIa2uYa51nfwrOCNFVk3wrbxvpsTPHM9C3CtG/Wxb6sNOU7uZqjro/uYH6IGronVBMdjaMh7iWyO51ZPFZpGlRuo89Cw0KbrWAvNFs5NryRY78UmzsnSkeAciynOcWLKvuY0MI8IW6Flc4p5O87uo82Q86ijN/D6Cu2zoweo8ZSF98A2nKJfWUJe78w61lmFtgY2LUYdFt/DWsXW5P3u2Rj39JtNHMfuQLF38XDvb1ZkX1bJZ8QpXt8Ocd08reM9XRvi2hbb8n3mit9UPXtZNhILaxXLwBzjFXiOub6wN7/dRb/1R5jD1p/FfD6YKXztLtbToiLHcbGD/t5M8ZmPmeNzmoZC30fEeO+nmuync8X4BKYiv5To35Gh0LRZQT+yUryvRiCPxzjD/KXrGNv5jqJmreJvrid4X3Epz2nWw37NNbxPra6YuyXCTHD9jErMMa5C6zQp5DnII4Wm4QznqdXCdvcVOSZVPNN02vI+cDJBnQ1xgM8lwx2Mi7yGtbgeKmJlYf8yUuTg0lc8q3QxVqozrNXGpUKbQRF7piVfz3DQ31tdrOtPTnB9uDfHvdAMl3aRrMi1VBTj+GimYi+RYr59FPiXC4QQQgghhBBCCCGEEEIIORd8uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXfLlACCGEEEIIIYQQQgghhJBz8chKcJmDQixrdRSqCENZSWLTH0KbqYPiHvPb+Jv5FRQLiQeyOJupEG/1XRTy0TRUN2q+gLb0Dgp3NO4/LX02vg77+trxQ7A95qIw0MbJKtj2z16WPj83exba6HUUD6lrKLRhGihQ1BvJYlVagQJxlaN3gC18L4rjWbcbYCusG2ALb35C+hzMUHDFLbCvy0Y8wxByFMKM5YKraS6KApudIV7LQh+da6jWsvibPbyU2JgphM1inIMwQDHL/SmKLrYfl305fagQScoU8bmKYnutAkVNX1uIlZUrKGaz/0AhNqUQpQoDTDD9BVG0tTHGWM3Dfo3vYKxPTbzPJMNxbLmyCN24pRAdOlII8i0ZjonxbiaY9/0FgbXjIxROyxSCfCcbKAo8DfE3RbEgXlXFtaHqYR7tOhh3O7MrYHOO8Z6OKvIaNcux/84E/basof81VtGH4ptyvg22UGg68DEvHYxQyEtTaJj5npz347ZCODjHdfJxC+uBcoACV8cjjJXZyYLY7xqOz04NY3HZCCfo38YqjlvHk0XLetMGtKlk6C8nGo5RUUOhtFKX845h4fcabexX7xBrh665h9+NsM6pprLYdNLCuMhS9LXRBNeexgzF6wa53N9ZA8fHSHG9Swv05byDgvRWJPc31TEHjXL8nm01wKZNMG+sGEOw6TW5TuhqKIg6dxTCjMtEhPNU1nG/kc1l3zjbxzGMdlTCr5jzLQ/HTF9YnwY2CgeubuHaXDOx/3MxBFtZYt3Q3ZMFBp0aCqaLHYw5S1GXBHO0mTV5PGoRtglTFFHfRB1lEZs4tuNyUUwY47JjYczt3R2Cza/jWndSxXZuRfYNW1uBNo613OKcQgjhKERHzRzzY1TK+0wjR3/Z9LCeuZmi3xYR/qZblddws8Q++HOM1+ruA7AdnuHaX/bRP7wFkfdxifXAho1zfHfQANtQIWq62pRrofEE/daYYyymCmFPPcJ2VimPx7TAPnRaKAjsrm9iXxV7I9/HWtfTF/ZVMxyfWMecuWzoKtFRxRo+GGG7RlMeS72F6/X8Hn7PGuAcm+sjsOVPKdYyrSF9vprhPrabYazMM/SPMxPradfD/uoLQsoVHWt/vap4plTgXkiLcGzrCiHlqSLlam05Dowq9rWI8DlZOcDxOEqHYHvVxdjo5vKaFLSwr9VEsZdQ5LBlIp5jjnQjfFaku7iOrzpy3j/OME+MEywKOhX0K7u6AbbR/S+BbTC9K302OihqXD3CZ4vxKa73TRvv6UGK8VMsbCUqqzhmfQ3Xz6TE+A8thYB2tYG/OcW+1XW5tsznOLalYg/cnF8G29oqxk+mqDUv53K7L/qYh7Ie9rVSwdrhUeBfLhBCCCGEEEIIIYQQQggh5Fzw5QIhhBBCCCGEEEIIIYQQQs4FXy4QQgghhBBCCCGEEEIIIeRc8OUCIYQQQgghhBBCCCGEEELOxSMLOhe6SkAPBSdGsSxyc6IQ/MhtFMwwt7BdMERBtfSqLIzUOURRwEkdxUgsE4V7Bh/H76YNHJLZ18oiF/mXUEzJvo4ChvNMIbh8EwV5tiqyWEi6qhBwmuNYeG0ULLndw++uO3J/7zsoGvW0jQIg1n/F+T3aehFsV4bvAdvO07KIzvY6Csv0Hu6DbdkoXRTo9GL0q63WU9Ln1MZ5mir0aBsZCr1UdRRY6Ydy/FQc9OOBid+rjlCFabuGIjpVhShS767sM+sKge5RHeO6exdFYxobeJ8rruwzt19B4dothQhu20PBn89NUCwwFXJ89us4J9khTkrtaRQsmpzi2OYTFHG7Z8jj0T9BoaC6tg62ZSNLMPcZMxzfyVS+fzdAvz1VrBdrJvpameL125n8/vzUQX/JFX3txLiG7IUoLOvb2F9Pk6+3unMT2hTjq2CzDnFtMJ4cgq3ek2MlVQi1H7ko0qkSANaTS2ArNVnQWd/A8Z8q1t00wetvrz4GtqqGQnh+W567sasQiFxuvTUhhBCliyLApYbjJhY09CpC4bcW5la3wLlKJw28/Pqh3CbHNplCqFZr4W9WMxQZKzP8dyu1LXmtedDD+sJy0TeCCYreu21FLnlN7u+lNfTtnrGNv6l9EWzaDMfRqD8hfx5jTWMZ2C9vgGvnmTgFm1CIJPoL452Mce00XZyTZcLQcczSOfrBxJLXiqaLa0A+Q78zahhz4REmE3dL/m5Vx2udvori0OMc1wVte4h928S621gQQNaPMOZcgWtRpYZ152CO1y/vyDWIv3EEbc6meH37yg7YGor8mw0W6qoHWC+l2ygIalexXgrqWPesBLhvE1057opCIUJcWXKRcyGEbuOe228oNgmxnEfnCc6nUWKMuTWsN7Qc95lFVV7rnQDHOw9wDUiyNbDVE4yfiaKuihbyqFtgv/oGzvHQwVyo9bBdsSnf024La/qDKa5PXuUMbMcVDIzdXBauDnLcq08NXD9WHmBcR9fugm0yfRJsg0jOmV6hEMpd/fLEOd9UxLhnnecK8WqFz6eR7B+pYu0pPVxjoxjH0nBxf5enWDfkswPp8+sr+L2gg/vTpvs42ESJ+XtdUU/3IzkHGGOMu7jAeqNbuwW2MwPzULvRAJur4/pjj+XnRXkdBd1DhfC4sYpxXDi4jr+9hvuQzy6IvwdTfB5gKsR+R9lyx4ZVwb1o4GGOEQmKZTu+7LeegeLK/RjHJ40xd+tX0a/0AOMnWJVrceewAW28PooO2x4+Bzpr4FpTaaC/ZIXsy4GDz22DdUVfE/THVQfzfhpjrh74uHZ1crnOKzwcMytS2Bwc75v30Ga9E/PJ3kJuSiNcy4zKc2CLEmz3KPAvFwghhBBCCCGEEEIIIYQQci74coEQQgghhBBCCCGEEEIIIeeCLxcIIYQQQgghhBBCCCGEEHIu+HKBEEIIIYQQQgghhBBCCCHn4pEFnbUTFH/KQxQfcxZEhtdGT0MbvYNCFTOFEOGoje8+9Lu70ufuUyhO0n0Jv9eIUXyjtov92ElQQCS8J4s97f4JFDbpfV4hHhSiwE3+tV2wNV+Up+EsRhGWzELRm2a4B7YzgcJU+oLoaGLi+EwFipraN/A3nxMoQHs4/B2wdR/Kv9GoocBo30MhkmWjFmEMZDkKiJ0OFkRROijqpJ+gaMx4BQWQRqFCwG7BlPjYr+oMfdtUiOGV2mtgs17BeXc8WTjpgYkiSY8dXwdbw9kC2y3vs2Brh7LAX6eKYpzGHIWgszYK8lRnKJTW92SfLFIUrqp08FqT2gHYaj4KCjmWQqCwKYvlHt7CsdiYoCDfshE30FYrUWAp1+XcVDiKew9RjCwa4Xq0pljOxguC6/0h+qil0NN1FELk1SfRP2onKHylxfJvVqbYr5da6I9X99GHsj6uBYdbr0qfwxDFFK9v4vfSKeZz5wznxNXkNaRnoSD11hqupz0Tr3U0VYg/FphLZp5879YIhTwPbcVELRmVAuc9R1cW2VxeQ/o25rm2oxDfzFCMsGEqhDt9eY4NXSFipmFfw5socqm1cF2JCxQBm5fPSJ9XWlgfdY8UdaCDgoWPOyiIfrApx+ft6SvQxt5EIeXxMfqypqNwqteQx/agxFwVzdBv9RQFqTv2NfzNGoqwjeuyYKGrYz1gjjE+l4nIRv+0FGNbq8t193qMYo4HBua4ssR2ukLvt6fL+WuS4FpxaQvnsqyh+LFpYC5sa4q627svfT6LcI0xDhX5cgvr9aMdrLGrxdukz09fewra1Pr3wRbjNkUkGc6T78i25jred83BMctLjJP4AOdp0MC91syQ42RdxzyoFXitZUNTiEjOJzgxY022NS1cA0KFGGoUYKxUdfQ/vSn72jxHEczVdA9sRn4ItkkF+5bjIwKxYr5V+txXiCu3fNy/i0IhMHqIcXd8sCd9Hnroj3dTFIJ9qtkAW1Bi35Ku7LezPs6lewnXOquO67BIsO5pdLBvtalcI/T7uJ6saDZef8kwFMK1psK/LYF5p8jlXKfNMU/EHo6Rm6DfFgXuSwwf4zOry3XOKpb5wq/jPtm9i9d6IDCmcg39w2/KNdmh4nlAPcW82dzG9UHro8hz+QBjymtgjTpZCI16hrVc4WMtp59h/eiXuBbcamMOG16R52WgWOtXNcwdto3XWibSsyHYhi76d9bA2tZ3FuZzgs8zbRNt0yb6QS1SbGgi3I8edPekz+kc65zoHVjnXLF3wVY6GD+/08VYuWHI+fWoizEsDMzn6To+Zz5LcN/tuXiffl8hdD6RfU01tif6EGy1Gua0Sz72tzfCdiumvG4Hin19I8cYO/S+vH03/3KBEEIIIYQQQgghhBBCCCHngi8XCCGEEEIIIYQQQgghhBByLvhygRBCCCGEEEIIIYQQQggh54IvFwghhBBCCCGEEEIIIYQQci4eWdA5baMAyopA8aR5TxaJ2buEohqXT1Bs4uEmXmuYo+jt1QUxH2uGokjuZezr9X1UjbrTReGe0xwFPvKqLGx49L9im9efxX48NkcRp8fvohDTzbdfkT5XezgWtRdR6OnWNooT1nMUNqp3ZaGUh1dQJKXo4ViMH6JYaaihAK3YRrGT40C+J6f6MrSxYxyfZWOA+idiq4ICkZYpj+UkxfHevYQihnsKkTtvFf2jcyoLVWVTnLvhCAU6iwIFlu6aKPiz0nwL2NIDWejpPS0U5HnJRX93Zkdg27BQJO4kk4WltQT7Vd1AkeeJg+JbWRXvvTGQ2+VrONajQxTVWsPLi/uvY04wIxzbvi2L70wUAqMtD0X1lg0nwXxVztG/B6eysNG4gjlhuI4CUVmEAki9BsbdiimreTUVwvXT3uNg82conJrMUbh2qhA/T7vy9W5to+B9Jb0ENncX773rorOtFXLS0c9wfM4GuLQ3qzg+5Rrmod5UHjN7D5NcOFJc6+tQQNAIURS11UKh83YuXy938DfjEYplLRujAkXjfBxK4ddl1b+mjbWQ6+EcF4q1ZxzjvyExa3I+TB/g987mmKdrG5ibthQ5uJui2NlsTY5Zbx+V9jqXFQKuIxScvTPFuPDXZWFdJ8S1Z1fDWHnYRsE17wh9+a4jC5utbmP/bwr07XrQAFtviOKHQuBaoE/l601HeC1noihClginwLmcFZinJ3tyXX9g4VyGFVwXTA3HumZjLZE4coxZIc5lpY0+lU7vgO0ox348OEbh542n5do5fZti3fy8Il82FMKBDgpvpgti6K+9iGuYewP3Sw9jzFPrBsa0K+T+OymKC54coSCg/RTmke021l+Ri7G5OpCFqxuHKEw6aS1/DRXrOJaWh3V9Y2HPmlvoe7kiVryOQtQ0xzXFiuQaITNwv+duowBr3sI9Q/sOXv++QkB3x5D3OPsKhfHZUPF8QFHDv55gzK7XZdtmjjF2ILAGzLqYo1ev4zp5Ot6VPpcm3mPaxRw0amK91NMUwr4DbPf6guDtpQjHOhGKYmPJKIbotxPFs6hS8SzHseX7twycl1jH7yW6oh6dY15rG4o1YybXEp/dx9zaHOMcN1bQptkY25XBNbAFC5vUNMV5738R9+tBiLXWcAd/c76FY9Rpoz+35g3pc+8Uc1rdxlwd1/Ce1upYEzzmY22rWXJdpit0mkcC88SGqRAiXiKMCu6vWx3FwwodxeszSx4kPcVx1RVa8JaFOUbv43OPaB19uVbdlT4HVaxpyn18nmnUsO5O9o/BFhpY93W2ZV87nqHvmW20Pa4/A7Z+F/0xOsbnTGMLY+r1hfVnp1DUSAJtJxNcZ/sR1j+Oj3N87MtrxvgM188owGCJGl/ec1r+5QIhhBBCCCGEEEIIIYQQQs4FXy4QQgghhBBCCCGEEEIIIeRc8OUCIYQQQgghhBBCCCGEEELOxSNrLpgzPIvpVHEIV3/hiCxTcUbk3cfwPCjndTyPtPXkHv7msazh4HwRz1PMN/B8ubtNPAfsKMczvowIzygzF86E7Xw93vdb+/ibBw080+rBCZ4XuP1p+bx5652/D21+t/0k2B57/V1gCy/hmZBdWz7zz+/jWWSF4lzK2vN4Ll3jIZ6Z597Ac8ayE/mc8eZDPDs1Kpb/3ValiueiRXMctx1Dbje38YzIh/t4npr9LF7fyXAOeoV87tqNhuKcYRdjLMEjeEVsKc5dm9wFW7Mqz98fhJ+HNkUdfTQaYvw4IcbP1YZ8/nCkOMf/U+FnwbZh4piVJZ6fOrXlc/quWXgG5T0Tz952H+D4rG7hOX2HDp5BWe/KejNagOckz06X/7zgtMQzFse+4gzbjpw7qgqdmhXFcBzbeBbjYIBrjSHkczRbJeb3g/IxsGljhY7B5S+C7QkXfSZ6RvaP3ehPQpvT/A+wr2O8dzfGJfqlFTm/XC9wLKr6BtisOq4r4T7miaaQz+68Y6AeyqqHNltxtv/cXwPb0S3UXJq48rnl5gauzW0bz3peNmoGrg2ag/k8ncu2VBxAm6JsgK13hHl0OlXYWrJ/r1QwZ56G6Bs7AvN5XuD5u/P1fbAdTuX5W7+MZ9VrN58Am1fH2iEo8ZzRMB9Kn0cZ3vfDAtexsoY5YTrDs5OrC2cxPzjF+7ZbOGaHM4zhYYzz+Yy4AraHmpz8ous4ruYBzt1SYeA4ZjbWyd5C2mi5DWjTN9Av8hnmR+FhfrcWcu00xBx31sM1pvX1/xv+Zg/n8rEaapOkXTnn77hYb9x2MGcc7WHNLa7j+tG6Id+7PsF8XA3we1uK/Yw5x/1MtSavFY0a5rKtCp5NfNBDPy51rIk9XMaEFst7lZM1XCtygfXHsuHpCh/VcV6inhwraQuL+iBR6BXpWFgFGc7VPJbnOCtxL12kV8HW8TEvvdTDvFdfQb3CclPOydsVrKUPjtEfSxfjv9rE/caXBvK53fMU1wW9jnnp1EGH3FXoqwhLjrtxHfflvqs4472B56avVxW1lkKnxq/J85me4H1PZhhjy4ZRxxjwPcVzm0yxFgg5f1sCvxdpeP1hinNln+K6Mu9jnHW25Ou1DNTLEDN8/rJVos8cCvSFqUIL5/A1OQ83bqB/54q86Zl4TysN3Le5x1jrj3p47v1BIMdGaKN/u4qnkJap0H4ZYW46DDGH9Uv53vVVXJPsuUKIwcOz/JeJZIj5atZDX05reO96In+3cNEPNA91R7IErz+K8fluxcSxzYScn/IR1nxjgWvNeoBrwUTxHC5QlEiNluzLThN9I76N8TlsoG/fUehL6VsYK9d1fG61vqBjVD3CtWzo4jgWiv20rnjeNdNxj91ZqIM7DRxHLcE5dp0v73nU8j/dJYQQQgghhBBCCCGEEELIGwpfLhBCCCGEEEIIIYQQQggh5Fzw5QIhhBBCCCGEEEIIIYQQQs4FXy4QQgghhBBCCCGEEEIIIeRcPLKgc2Gh2EYwRXGWuCULd7QnKHA3Pb0Otux5FBW78wDF07KhLOoyX0cxiy0NRUBOawrRMgfFYM2DrwXb5Q1ZtLP/v6Hoxd63oLBJrYuCGaebqDIyDGVBq2ufQQG3dqIQ2d1B8ZB68xBsgzuyWFB6CQWo9BMUChqNUBhsqqHtqmI8al8vC0dV2jg+k2Ock2VDL1GoJrFQeKhv3Zc+FyH6Rumh2JaT4bzHEQpVjfSh9Pkow/eGT2koPntTR380NZyX/RbekxvKgjaXjbdCm34FxWBsgTnB03Ac55YscvPQwH41BQphHWQovuMOG2BrX5X7f6YQm+soRN2yFRRr9CwUpfPdLbCNYlmwsZigUFiYowDQsmHkuDZUFeJbib/Qbt6ANgp9KDH0ULAxKdCvxjPZ5yuXcZ7WTfwBs3oJbBsjFH6eGui39pmcD7PqQ2hjZRj/Yw9jzFeIKa1MZP8IGijcJ0LM8aaGa2WYvwy2clW+zysHKBgX+Q2w1feHYHN2UEQ+3UQh3ycGcqwMLcyF9TkKaC0b4wz9z0oVAvSF7N9RhqVaTfFvQ87qKPjVyfE3ZzU570RVzEP1l1FIea/6e2BLFbFyOsJ1q7UtizVHXZxPy8Ic3xvgvecNFIQdn8rt2g1cJxMdYzhVxFgxQ1s5O5I+P3yAY1YKFMK7sY61T5zgPb36Ot774IocK2cl1oaus9z/RkiPcK3wLRz/8UzOc2GI4xUGmDemEc5TY45illFzIU/v4hpT3EehvEsPFPnRbqAtQ5Hw4FAWOb8T4VyuplijOQ28J22E+yV3wV2KEfrnw+gMbO0m/qZWw+9uOXLerp7hWuTZWI+1mjj+utUA27aHAteL4o3GCdZtkcD96rIRm+jfjo73ZS2IaFc7WOd7FYyxYoA20xhiP3J5vP0Ac1BvinvuVQ/XgBx1K0WxicYslvejUw39MS6xTn5pgGtdS8NaqFzIyVqEce0ZONYtVb1+guvYNL0pfe7VFKK4zi7YzhRC7dNdtJ0FKDra7cu2SqMBbcoEx2fZSGKFCKupED+Pcf4KXc6vswzbRCHu+ayW4jcVgtGpQnC9nct1faWGvznpYz3gZOgzKrF2U+AepD6XfaZ1ptg71zE+jQfYj5E9BFt3HfO8NsP1YX1VFkC/3EMBbXGIuePMQzFrbw3H44Ziq5zY8t7KFriv6uqKZ08l5s1lwl7BOan6GBdxBfOtm8t71sJEH9U93KeIEuOiUOxt8wn+Zqcj216cNqBNf4Jz19nCZz6HMdaLmmLN6CdyrFgC/ex4BeN/zUPf3jXxPpMz/G4coa2rvyh9dnefgDZ++DjYxBjX3sP0HtgqKcbx3VL2haM5xqImFALgky9vzVjuXQkhhBBCCCGEEEIIIYQQQt5w+HKBEEIIIYQQQgghhBBCCCHngi8XCCGEEEIIIYQQQgghhBByLvhygRBCCCGEEEIIIYQQQggh5+KRBZ1TDYUezASFfLNEFkU5K4fQptZBIYk4QmENx0Exr92tBTHVGEUHT7cegG3jv6G4an4dhcYihWjs3d6CoMXbUTRus4+CIuZdFAqdP4HCyU8k8tg6218Hbd62dRNs918agk2PUJTmxltlERPnFgq/3buGc/mWEAW/phEKIp7VULB0pXtV+vx6E0VBqsFyC+gIIUQ0RMGZZgd9SJvItrM2tlmr43hkM5yXEwMFygpvKH0+nKLvjQIUHmxk6Mur7i7YtAc4x2bthvT5zhiFazZtFBhPGygM9GqM/rFmyYI5JwLjOrrzPNjedl0huK4QOxpP5PS33bwGbe7vvQi2lRYK6OzFKFLf0I/ANrZkQZ50hkJyT5soVrps+IVCeFTDfKLlcm7SHBTRPLXwe+0CxdQOcxRdapZyu8EJXr/ZQoFhcRuFcY0MxauyOSqKtXbk662d4VgctDFWkhHek5mh8OUNXxbuiyqYD7QDjLt4jGtPO8ZYHOzLsV5xcb32HuK15lcVwqBHn8O+XcE8l06G0udajiJviYvr1rKRl5iHHANtQSnnpjBCPxh10Yf0APP+UT4E29ZIFlNLtG1oUyjEynML14H5BIXNujHm4OaxfE/TAv1Wn2Md1Y1wbXB8HLOHC2O7MsR/O2PqmG+PG7fBNl9rgM1K5PUz8BTCjAKFNocFCsu1cMhEr4nf9UbyPXQCjNd5hNdfJjQbtyGFjXM3M+X4z4cKEc8qDmxQxzHrjPA31wI5xr44RR927+E+6DTcBZto3gWTb+A6s2nKdX2QYq51bbyn9bgBtt6aIk5COY8mNfSVdYWgfFBgfG2OUEz09YqcI2Yx1lB+ifVMfR1zeTzCfCPuYY7TrstzZwrcJwqFYPqyUcQKwfiZQiQ5keueNFKIWU+wzh8r0sbYxPy7spDnokkD2gwSzF1bhwph6Sbm33COgtxnplwL3Rmib5cZ7r0GOvZ/NkC/ChYErpMRxvrAxX6FHazRtpMrYLNSueabRlg7aoniOYhirciPsd3KOsZstS3nTEcheK9neJ/LRqWFdX6QYx2ezDGHpbqcFxTas6KiEO2uuor5cxQ1/CnWNNGpPOaljn41L/Fadwa41jRqKIybjzD/VTU5XgzFPr8S74Eta+CArEbrYHMCxfOuDH33dE/ux5GL199t4veqJfppOMfYm2hjsE035DnQQ6yJM4F5dKjj9ZeJaR9zZKEQkq+7DfxyU87xcYH+WNExLsaKdHI2xrycORifYSD3Q+8oRI0P8Bnt7Aj9Janhel/r4757MJTXgsxGP3AU+sUHD/E3ex4+Sziq4m9+zQra6sGu/PkMxzaZKZLTAPvr6vh8pL6BvlzP5XZVDdeQeoFrZT/E+XwU+JcLhBBCCCGEEEIIIYQQQgg5F3y5QAghhBBCCCGEEEIIIYSQc8GXC4QQQgghhBBCCCGEEEIIORd8uUAIIYQQQgghhBBCCCGEkHPxyKpXpUKIcKQPwebsLQj+XUVBkXkbxRoreyguEbsoCFUasvBF4KDoTXQT+9r3UXzDK1FARHNQMGfDlcWqXjlA0YsnfBTf2HgGhZgGIb7P6dZkoTT7FMUyt0scs7srKKL1pz+NQqRHu3J/1298HtpsvvgnwLa6gyIjE4UQjrOKIs9Z/PvS5/YcBSnLGEWGlg4PhZ7KAufdXJcFxeMhivudChTRaa6hL1cnCjGlVBZwmR2hoMtasAG2kcC46x69BLZiiHMclHKsK7TERb+HPnqso/Dr2uwS2A42ZV9u9NH3tnZR3OueQmDx2SYK3H5pJl//RCEAVltBAbr0COd8pBAYneY4T/5C346ufhbanB2g/ywbhkABJCPBudJjOTcZNgo9tqsouDpNcelq9hVC3rmsztTI0B8HJgp+3SgwLsYrKB62OUbRvGB/U/rcrqPgUq4QqU0LbDezUFDcnMl9m7ZwPapUTsGmh5tgCzZRrM0cy+Je2QzX2PaOIlZSzPF5gmNbSTDvjxYuF9gKMT4D18Blw/DQb2MLE6ehL4xlE4XN0BuFyE30x5rAGix25fXIV4jq1q/h+p89wHxYHmP/p3cw/l/1ZIH7QCgEkSPMEYmG/T87RYE4sSACnORYR41yjBXvAGu+kULEPJnLvuzO8VrjDMVr9yMU/Ow46N/eBMUgD+pyPqx20Q9GDq7ry4SpEG8fhphzguaCx5cKYXhFEWIW6NsnBa7hxVxeG/wEc2i8iYLFMwN9/fIKzmVwC/cgbVPu27qJ+wMjx3VNa+D1yy7e50yT70kvUJTZa2Cc5Atii0IIMbTxuzueHDtl+3+CNvZtrGtbvXtgSxTCntM+5oOVUPaDhxXMBQ0dr79sWGuYaxt1nKthXxaITXXMq6GBNqNo4I9i+SVGFXkvYVexNtJPMIceH+Fv2jbmx4chCl5GoRx7D8/wvtMU91AtD3/zxMDvrmRD6bOu8Pdmooi7EdZoUbQLNncix8pD4wjaDK5j3n5+hv04ijHWxye4zrtV2XaiENj18uXfW4z76Gv9FtbT3gruKXVNrmn8EmMsEThGuYnr9TDDfWwp0Ce7jhw/jS1co/xTtK3FeP1WhLHnWhhTI03+zeoA19jSxvXozMGqcq7hmnQ4w/HQ2ihUvWPJNZJpPgdtrtwdgm2k/Q7Y7pm49hYhruOXCrlvXQvzyzzC+qKykBOWjc6z+OyvaaK/TMoh2MK5nGNSH/PcRCF4/VwVc+T61afAlt3Dfffs8L70eTr5HLQ5zjBv3rYaYOsm6AcPRyhEHCZy/Vy4GP+ug9+rV/FZjpthHtpJca9i9tD/tIc3pc+fKTFvVJ7/OrB5ryqep7/+CtjK6Q7YxoHs89EI9+ZhFeO60HGteRT4lwuEEEIIIYQQQgghhBBCCDkXfLlACCGEEEIIIYQQQgghhJBzwZcLhBBCCCGEEEIIIYQQQgg5F3y5QAghhBBCCCGEEEIIIYSQc/HIgs6mQnzPaiqECDVZwGVVuwFt0hEK4b28/gBs1w5QlCJ9TBbuifd7+L3Hsa/VMYoT7mkoBPbMKYqP3W3JYhvfaO1CG+MeCorsFSiWWd14CLaHLfkdT2sTRXX0DEWGOiUKj7z6dXj9t3/xGenzcRPvu3f5M2Br3USBmPLdL4Ot/yUUzHr8CVlMaqOCgtcPJyjQu2wYCq2Th2MU4BIDWUimnqI/li282GyAcxys4zvB0cdl/9h9HmNgdIjiO6P5HtiGAc7Lsy30yYeuLBDbOcHYcRR5Y91C4dfqCMXNyqksotPZwO+dOZg3KiaK9PxuhgJOTzbl+xxHKETUG+E87TZxHI0UhQcnMYoADdfkcXQfothPqGE8LRuWQuDar2AOq7kLPq8QfY4OcYyO60OwZdvYj+sn8m9qnS1oU+8+CTa/h/03tzAWNwbYj1NHFtF6+R6KzW2t4PcSH0Uo/Tm281oN6fOah+upv4XiZ9kBrpWOiXMyL+XxXglRWC6J8Z5OLuP4rIaoEBmWKPzW8uSYHSn+3YOlDcG2bKQJ5tF8jOvxw74sgGbYClFUHWurEwfnuAxxPUofDqXPr7tYNyQPcI4n0QrYRIrxmQZvA9vqXVnEbLaOeXrLw2t1B+ij7RL9I89lEVBTQwHaIwe/F3RxXVn1cbwP6rJA5FqMa0OaXgabVuA6oKcomOfPca1crcr3FE5xHbMVIsbLRB5gbXitg/N0OpTv8yTBMSwsnDfhYDsQhxZCGBP5u56PvigUtdE7GyicenaMovX6HIU3jxY0Enf8Q2gzEJjf5+gGoraDexCnkGu0JxVjYSnESh+mGBOVBBfY8aF8T4l3H9pYCg3ZOaYWUa6ggLbl4Y3Oc3leKgXmPM1CscVlo3uAgouJam/hybFiu7jmBk2FCLOlmKtSkV8W8uikGEKbYQO7tVPDxwv357in91Nsly+Isq67mBtb08fBluWKejpFZ4v6ck6+n2CbVht9KFIIY1d3MOfUjxvS58f76KPTe5i3rckQbBsazt0oxnbGWPYXbWF/JoQQIsX+LxuXr2DdkHbR1jvANST1F2yKuTM6uA/XPZyDHVzWhbuDPrk2l+v68T281nGMsTgNcb3QVtCPzAjFcvuuvCb1LUVtnuM+8/Ia5pdqgTHb0RR+1MffiBbWvJ6FAr23Ypwnw8X1WVvcKwoh9jP8rnMsT0xxCcc7zvbQFi232PntvRfBVrvaANuaj6LAm9tyDr41xLWnreOcxAOck9EI97FnVcWzoUAe7yTA8bfWFM9Qp+i3dQvFyd/SvAI2dyznxJFCMHoWY19f7WNtMungGLkW1n3W5rNguxbJ+/PHp7iX0F7Ceq58iOtzjM2EmGDN2GjKsV1LMNYNxV7Fsr68NYN/uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXfLlACCGEEEIIIYQQQgghhJBz8ciaCzkeWyYa9/Cs4E9Z8pmHyRzPRA90PJduo8TzB8dXb4HN/fRj0ufyG/GsxOrreK7ow+t4rmurxHOGb5/geWRaVz5HN6vgWVW9p/CMq8fneI5WNMezwW7U5YNXoyGeA6Z9Ac/t2vomHJ/xq3gm8pc2RrLhHp6d2trFs/yO345nFm+/juedjZt4Rlkxksd7sIbn0qYGjtmyMXbxwLNNG8/9dOJd6fNhhueOP2aiX902FOcY3sIz0NauyXExyPAMx/XH8Rzdwecw7q6mV8EWK/Q91jx53scjPK/tQYZn3G9r6C/2tQbY0oF8zt3ZBMe66eA5gIcRaoWsjfBc4eOqrPNQ0fDsSi/CuawOboJtWHsMbMEUz9+Lm3K+6ml4zmBTW/7zgvM+5uBTxVmGk1iev3GJY6bVcd4bKfrjYRdzmOHLPl+UGANzH89TVJ2xevJAce7vZdTf0Ptyvt1+DOfTShV5Q3E49STAfNuZyb58MMe1LVnDdXE2RO2EI8UZv2+7Kn/3uIJxvaYoCLbqmEu6Gs5n0EV9lfi6/Bva2eegzaCFehzLhl7DcfOaOB5bgeynIw/Xi33F+fKmQn8jC9DmLuiCGCbGayxQg+aG4nj/dKrQV4lRm6koZc2C8hTPr4+qWCcUJebg/f4x2IIFTZ5xgbHZbGHcPWu/ALYH+V2wXW7Ivhzfx1pR1zDGsghtMwvPww7qWI+GY3k8LAu1VLToC2BbJrw5zvnLx5jLZ6ac59wrmBvNsgE2d4Z5O07xjO5qJq8pZwr5Bre4A7bPxFjTX62jzbUuga3jyu10HX1qPcAaRxSKs/dHuA5EtuxTvRi3fHtdPKs52cHzvtPqdbAFC1ucYAVrqK0+3tNrZ/jv2tz7+Jt5C/czaSjHcG7gfsnyln+taD2B43a5ivE/WKhHJyWeBd2dYYyZ+QhsoaeoJRb2Jc5aA9o8McdY9GeY360I40Kb4/qRjBb00AysI9IcY9jMMdcOGuhrdUdeKzYcrL0em+M+yHAx/vfP0P/CAzm/2zmOxaiDZ5OnDu4jPIWw36hQxGckB6Ou4z2ZIe4Ll43TW/hc5e421kf+47tgq5pyLZRN0Pd6Cl2N+qlCX8mvg+1sgv2wS7mWGCuepdUVdU+whn7bSvE3yzbeQyuR48WY49pjCryn0TGuNd34VbAdXMI6qq7YJ6w25bpyu46LanUP48zdw31a6L8OtsT9GrDZhbxmVIe3oU2q2OtrAsdxmbj8DD4j9ErMAbMU89XBTK4L0hT3DInqWZSPc24o/sn6zgSfA7Uz2Ze9Aa5HyQiD5SzG9e2gjuuDPUAfjQvZ/3pVrLk3OrjJWdnGPdrbFdqYeYH1Snig0OlZCPduT6HxdhmfnW8+9XawbdzCOuHOMd6XcVn+DaeGc5JnOGbFCOf9UeBfLhBCCCGEEEIIIYQQQggh5Fzw5QIhhBBCCCGEEEIIIYQQQs4FXy4QQgghhBBCCCGEEEIIIeRc8OUCIYQQQgghhBBCCCGEEELOxSMLOmsqYZD2Hti+3pQFlV43UHSp9FCMb2cPBVa+6LexI39KFoRqvr4DTc6uouhS83UUtDHe8dtgm9VRBPDrS1kc7GSI4ofek/i9Tx6icMp6jEJja6cN6XPko5jg4Cra2l/AfvjXUTDriVCeg5vXULTLfqAQRDlAIdK0RBHvFbEOtjyQRVLHqM8ntK8CQWdHR7EtSyFs5NtyXJgKkcf7JzgHG87TYBubKLByMJHFjrZshbjqayjgdF1D0W63fBJskwxFjJ1jOSdUNBQt2/VR9MYKUJRqaKCDXPXl+yxdFMs5wO6LnU3MJbdPsR9NZyh9nrgZtKlYKCjUazwHNm9wCLbjAmNF7MmxqLfwe3qGfrBsaItKj0KI5gbmvkYqC3dpEbYZ57hMveKibXeKa0GmyTlmoBAFq5QobDTfwhiu91DsaNhFMbLU/rz02RF4T2c9FFx2FOLwdgW/e8uXha+0IQrermboe/k7MFiqt1D8XEvk32wUKHg9PMF/lzDLMRfWSxxHbQ3XSntBbHps4fzaily7bGhjzMthjuKMvzuR1/HqKgr+XbIxp40KrAlCDWsCy5Bz3VahEK+8hPVL7w7WZKbAvLmxhuJ4VtCQPq8muI5NBpjjN13sv95Bwb9pIfuaX2L9Ype4ztwbYL1oiCH+Zkv220brBrTxLIyVaoiCbq9VMRYrZyhopzdlf0kUhZTjoW8sE/5ltD2jED+en8l5LpoqRI1xqMXZBP2sZmIuad2Q193kTFHbadgvz0YxvjzC/Fj4uK4PLHmdLIc4lzMTr++VuC4YVawpE13OoysNrL38S1gvPVVgu+oU15nThf73H+L6ejvBXJ44uC7U5grBSIG2dC7nA01gLsibmJOWjbNjzHsPRpjT2nU519Z8HO+ixL2WluIcpBZeXyRyXV+Ot/F7Cea9o1UU3py0cV6qFoplFp48p7vhENqUCh+KMvSFWo77XTOW9y4HMebjwwjXZdtEQeS4hmPb3r4ifd6Y4FodjNG3i0ghgq0QGG55mEuiSN5bmBmuFUWx/DXU9euYh/Ix1vBhD+fqQSnnK62lEL02sD51HZwXPcf5s2oKEWZNfj6yZWA+z2roo12FUPihIj7jEPtbX2gXaegvmkIovLOC9WNFw5j9OgvnINUeB1u/+xm5H3soNNsvsf9Vxb7by3Hc/CraZsfyHLsaxoFnYX2qV/Bay8Ttg5fApl/CebrmYt2qW3LdoWUogm2WOD6jIe5n0j7O3WSItdrcuCV9PsqwzaUVnKdYx73KzhxrmGIDvztN5FzajjBvOH1cQ+5HQ7C9buDYas49sJ1UMXe8syU/q7ixjc8u1ro4HvpdtI0G+KylUcWY7YzkdSqq4vo5mg3BVuRf3nNa/uUCIYQQQgghhBBCCCGEEELOBV8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzgVfLhBCCCGEEEIIIYQQQggh5Fw8uqDzCAUiQoVQ0s3X5PcVcQdFNewtFCP7d+sojnMlQ5GOF2/L4k+VFgrNegMU5GhZKI7V/QwKj1y/hCJUnziQRUAaPRQd3BqgaIfWQXGPyEIxkv7tBZHEzS9CmyRUCKe8DcXf0gMUx/lSUxZ6s7r4TmmaolBIq4PzZEUo0F2voTjpyUwWJ3ymhqLPZzHOybJhWzhGsUChpCSWxVO2W5vQJncUAjRt9MdNUyGSbHSkz+4ZzslgYwg2Zx+FZOJLKB7u3cF5D4dy3zyFUPOGoRAem+GY9awHYJtZckwZ4yvQZhKgCG5njHnj+a95CmyvLuSq52p4jw9TjDGzj7YzF8VxNqeYH+/bsmBWQxxAm1mGsb5s5CXOS3g8Bpthy7n1wQzbbKxgvqoqxMjmIc5fWT4hffb6KOAWtdBfVoRCANFEHxoqxEK1WBZJHlUwhts7uPSOD3FdqcxRwC26IQtJpRFeq1T4aFrDtSFNMU/0TmQx6HEH/XitiSqsQYoCeqtVFMwan6CoW+nKfjCvYo4zbLz+shHVFb7WwjHyhDzHeaJYZwq8VnZPMcehQqRzQbxucmUITfQx5mStg74cH6F/uGUHbMG63I9wijmiXuD1TwQKlm35KC6X2HKN2jMwLr62htc6eBrHduVAEVOpXNNkJgowhxPFuuvhb6Ym1tNzHcWyC3MofZ6kmB+1NuagZcJ5iP5zt4HixKNMriGbdczlFcWcZz0UdDRTXCuKuSzgeqmJ+bjv18C2OsE4SWuKfgj0DceX6wa7imtdLUb/LA0U3jQdFJueTeQayhxirnEVu8CbQ/TPRo6xqXly/4NV7NelGHPSWagQvFYIn8dzXCsKS44Tw0I/qDsohrpsGGsoGHlpdg3bRbIwpjvBfdW+g35bzDHudBvjzps0pM+xcwxtMkU+awvFuqYrrq8QjMzncszGKfpQrhAstkzF3rbEPYhryU6/3sDYLDLMEVsZPjNYmeDzhr2B7MuTM5yTqI1rWOpiXeU7OE+eiWvK3kz2edNRCI7Wl39vcbKH43ZyBfdRVe85sNU1uWZKzhS10QjFbOsu+vfYQ1/LFOLKsZB9fmZhbT5M0Ec3V9Fn4gzXAjfE7xau7JNtgbUW7liFcCL05WGKef+WQAF3IW6BZejJsb2pWGuuN7BWbBzib97vYryHCY7l1JNj1Fc8swo0rKPmCjH1ZWJt7Rmw1TUc8LB8HW0DuZ1fwfXCmihqjhh9z5hh3vF0HO+m35A+VxSCzkGMvzlT7Bsmilxtxfg8bV7INXuUYs1R1Q/Bpq/g8wZXYKxcruD6cKOPOaG2IIQdhRhP9xwcM6HYK+7UsO6zFM/Ok65smyqeneUG5jQdp+WR4F8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzgVfLhBCCCGEEEIIIYQQQggh5Fzw5QIhhBBCCCGEEEIIIYQQQs7FIws6pw0UENo+VYhSXpIFie5unUCbs9solHbjGYVomYWiXF+XymKZnkLoaXQJhSqGx9iP53QUoB10UYxk8qQs5pNUUWjn4U3s/2Ub+1HbRBHASk0WHpn0UbRjsoXCNdYXUDzkdPdVsD352vPS5+J/ehna2F9ogG3eV4jSOShsUr40BJv/tCwXlCkEkVQiXctGT0N/0T0UmMtj+f7P4iG0ya6juNROguN2W0NhI7uU56W5gSosk9voj6Mqisa0FQJoXQdFI72aLFSVmXjfZx28Vj1HoZ3iAP3bHcmxMqyiKLhA3RqRtTFX7d9GESN/Wx7HvkIILzQxBlIL/bYywd8sKzge5VjOaZGO4+o4mEuWjWaJ91A4OJaWJ4/lEzMUa5vEGAMbHgo4DRTDNslloaS1FEVT03sNsPkFrhejzhrY6uUQbLmQhcebCjHecQXf67ttjM97Ryjw1+jLfdsOMMYeHKFI2s51FKpKpyho1WjJ66LXw/GvzlBAT0vQlydVXMM9haixEcm/uWKiaORh/mUqS72JiHOMgd4Yc3BmyfkkLlH0sjbBnBlZ6GvzEV7/OJV9LazehTajGPOtE2OOLHUUXctCrHMcW+6v46BfTSu4nhoTRfw00ZcngVxXVnL0szse9tWZ43p6x0VxPzOX17uypqgzDVwbdhyMsaxEX+7rQ+zHXB7HwFeI5UXLLehsr6IfbBm4djqxLLpqHWBu0Ro4b6mJdb6bNsDWGclrQ1ZirDoh1s66Qjh120KBWMPAGDZz2TfSGfb/KEfpTWuMdbi9jSKM80KOV6+PuaC1i2tRa4bj3yzxPmuOvM70++iL4RTFCk9d7P+lKo53WyhyhJDHLFaInBe6Sq50uSimmN/vJZirVqpyzqmqhHx7OEajY6wbZhtDsDkdOe+NpigKXJlhjB2k98A2nitEQReE1IUQom7JceCY6NvaTFFzo/ax8AT6ZKnL12sr/CWKMYbvKer8gY9z0q3K+b1l4zOPTZwmYU9xno4E2uIC1w+9kO/TzXGsc4WY8LLR2sX19HIXfTLLh2AblPI86zle66qD67qjEI1fTzFXRwmu/+XCfrrj4HOb3MO9f5DhfiMsMEd2XbRpkRwIkY19nc+GYKtbWLeNVnF9qA/ReVeriryTyf0oppjP5w9xL3FUYMyamxjcaybOX2UhltMQY7tMcP3JxXLvu3vT+2B7YKHo8ONtfP7SDWX/y8YKofME4+KmqRAnT3EPLzK0dRemOEow38ZT9G2jphBSTzGmVmzFc5pgIQ5i9Nmqi2vZWor3vt/FHNybY5xNDRyjJJNrwcu7OCfPehj/cQ/bZUOMn1yxPvQXnjMbBd57YGNcZHOcl0eBf7lACCGEEEIIIYQQQgghhJBzwZcLhBBCCCGEEEIIIYQQQgg5F3y5QAghhBBCCCGEEEIIIYSQc8GXC4QQQgghhBBCCCGEEEIIORePLOhcZij0MK6iUFejKos/VGIUU8m/9g7Ynr29AbZ7PopZ3qnLoiX+AxTfSG9hX4+fROGXWYGCf/EUBT+rQ1mQ5+X6A2jTDFD8cLt8HmzCQUGOl7wF8dBAIXxaQVGNfA2FpDZyFHAxHpffIem/g+JYk3V8z7TroK2coqhOtIViR62hPO+TKQphiSoK3C0brolCUtoWiqmEoSzqEglFm/HvgW2wjaIuZfoY2BJf9pmHZ2fQZmMTRV6LMcbinkBRmriN7dbmcqyMslegzeZZH2xd1BkT1Rz7dlyRc0JPfwnamHX097P4JtgmGuYSc0FIKg9egDZ3zzDHrSlEe07GDbDpxm2whVVZ0CpLUTjJr+D1lw0jU4iMpSjSlQh5/mpNzNNFdB1sY+2T2G6OAk6NwVD6fNJEQbFdC22ZeBZsnQkKiA4DXH+cBXGsdIr5MfBQpG+ljmuDoxDCCsIFnw/w+psN9KvaIY6jpWE771i22Q4KM/rBDtiyVfTbdR3zUKrjWpBZq9LnPMK1wW3g2rxseKZCiLVAHyqmssjlZI6CYqEiT1gWrkdzDdfsaiz72mAPxb3iDMX9zgYoTlbVV8FWsbHuGyzceqOGYxEIvP6hjcKjzX0UcAsaD6XPL+voL1uLKnJCiOHmLbDNUhzHxsK65U1x3no5rlHj8SWwHW8PwWZrmPuqiTzHuoXrZB4rxPeWCLuv+DdOLvq7Wy5MgI9529cxr+o21s5lH8Ub01T2Pd3E+NrSME5SDX29OsTfDCP09/mCcHhcw3VzxcLvWTH6wSzC/l5ZEJKtFIrcG+FvdgzcI4RDjB09X6hnLIXwfA3jcFuRu9ohrmOjGNuVCwLORQP7n2YYS8tG0cS4aM+w3h0v7C3OTnG8BxnuFUWA9ZKVbIItOZL925ooarS5IobninrGwf77Cv8LCvk3tBx9b5yjv6yOMBfOBeaS+7kcB76J1zoxcd1JAuxHxbiGtlJeP5IZ1nFTxfoahRg/ww2M/3qAAtHNBXHico59FckjP/J505J1FaLJNZyrfIJ1T2tBXHl4hvvr8RifC1UqXWyHriwchchzpZR9axThGmWWuF50S1xXCg39NMoVoreavFcxNaxV3BqugUGG13+LwHg/maP/uSMco/7CPiS08TcVOtDiioXP04oM511PMTaOxLH0ObFxohoGroO+gfOyTPjVLbA5A8wnJwXuycTCc8PYH+K1DIyVmkA/8HLMa7liXUkNee7cTHEtD+uQWorPZAoD9wOxqi6byfPenWMN+aCHebPhYYztG4r9l+LZ1jXnBthsTfZla4TPyY5OcU09MI/BZjQxpra9XbBdWqj9hgmOY+TimJkNnJdHgX+5QAghhBBCCCGEEEIIIYSQc8GXC4QQQgghhBBCCCGEEEIIORd8uUAIIYQQQgghhBBCCCGEkHPBlwuEEEIIIYQQQgghhBBCCDkXj6zuY49RGMR2UKRosi8LLjcmKMjRjlDg4o6GIqxthSDfu1NZbCfcRfEN30OBi8v7eKv3XBSqcd/1MbA5n5IFC5+vo0BUUqCQn1ui6GVzioJT15rymJ1lKITVvIOCP3tzFL156yUUqnl1YcyubeJYW6ffArZadhds3ewAbHoF+1FNDqXPeXMbfzNGAaBlYywwLrwuCqWEE1kwZ+qh70X528B29zM4n0YTBXM0Ifu82UWhowfVz4Kt2sG4iHoo6rKSoAD1bUu+z8hEUfa7xyiMqWvoy7VCIXboyH47zVD5qRahyNO4gsJDTQOv3zvelT5vOyfQpqu/Bjb34RNgM6o4T12FGG85l8exMFBor+ug+NaykSQolLSqoS0cyL6gRSjMplUw53T0IdjauiI3CdmXr8QoTlQfob/MslfB1i/R17Y1FKBt2HJsn2R4/foexpjdRoGobI45sq7LsW6Y6Ht5ispSQYnjLxRiiqW/J33emmDcuQoh6Gn0AGxZhkJb+iaunyNPHsd6hG2aMQpqLxvdHOelbaFIVxHKcbDSQsEvc4i+PDBw7dGOcN57C4KWaQ1jZ2ihb68foThx1sI6IRyhMGBSyGvloEThxFAhal6UuFaejlEgzg/lMdINhSBnFcWnrTt4/VVTURqPFuLTvgNNGiHWo5ZK6FxRN+Qxruvpwrqb1LBNo/o02JaJeQVjfcXGNdCO5VxyPMTcWFH4bN1pgM1cxxqkuiCIGkwx7yUF1jNphL/pJCgm6gfoB0kk+3s1R78wNYyJkYb92A3x34oNPfkekiGKOdZ0zC39HPN2qRCz9S3ZP7cy3AtoqUJYcXoItq6Dv5k5OB7Oghiim2NtNxwoRCuXjKFi3g0Hc23LkNtli8LnQohqrhAYD7H2DIcNsBW+PMejGa4ntRhjuMQlQJj2EGxxifM+yWRf1mz00Ugh5pqVGLN1B2uVVmWhc+4utNmdYr3RnaM/WhquHxNNXk81F/tg+zhAW218drFeYtyFmHLEsS6v6bpCfN5RCPYuHXWc99U5jls32QdbasljtFpFX644uPa3PJx3VyF0bNUUMRvL+S+OcQ7yAteLLQ3zeVDDWJk7+JtRKI+RrxCaPVbsp8sQRY0txX49a+DznKliD9+05dzRTvF5oK7h92KBuWlQwf7aJt77miW3iyc4d2Wh+HfVBq5Ty0Tfxrlz1rFO8BN8Ljmcvi5fK8R8ngZDsM0V+7RhjsnJsHA9rmhynBUe+naUYL27H+P1Uw3XRV3xHDWoyM/hKoq1wTHQz1arLbBVFOLt2ani2ZmJv3Fsy7ZZgc/XnBrmiW0L6+J2hOuDqWNMjXT5N3MLn2d4dgNshovPFx4F/uUCIYQQQgghhBBCCCGEEELOBV8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzsUjay7EOp4vtWrhmbZBUz7/aeOteO64HeMZbrHAc6MmKZ7BVTmTz6EqanhWVXiC516lFp5H9q4Mz+2d3LwPtuiSfJ7X2x9ehTa9bTyvfbTym2DbuI9nYSXlQ+nz1QTP90rXcXyu23iWV1TgObT/81S+d32O5wyWTdRh6OzjPa08hmc6u0PFGWg1+TdCH8+vHIyWX3PBNvEM58TGOY4S2W/PhngenDdFv40fw7MkvQmegVZk8qGKSXoZ2rgjPNdxdh9jrGmgXw0j9PlpXZ7TVorX7z58DmyejefZtz08/21kXJc+Zyme0f1AcYa5e4hn5j3MMX+5qRxTr1ZxTqonOI5pFc8VPpphLkxjnKfY7kifKxH6T7muOBt/yagaeDbgdIx+FSwM0VxxBqKpOAPVGmI7TXHW8DiV51Sb4BmUcw3PdewLXKN0vQE2b4p+W43k36gEn8d+ZXj2qBNi/HcMzK1xKPtHYHSgjV3DszBtD/PSVHGGcLsvn0uZKfRKQgdjfdrHc1L9NYwVI1XoyJjyv3PolzgW5gTPuF02/BW8r70e5hh74Yzmygj1LJIWanloAcZd6uN5p/aZPJa+gWtxzcRrBVX8zcEAz093CozPeCH0hvnboc3mTHGOqYk+GgnU42qM5HOANUUMd8f4vbKFNjvC804jXY67jq5Ys6Z4nm0cKepMC9fAkYfrVmVNnqfG6ha0MWcYd8uEq7jvO0c4/sHC2fJGdh3auAeYI8YprqeFpjjfd+F8/1N0RTGJUR9Fj7ChbaAfVE1F/lrYvowPcD+TBViXJAnm96MM/dFZqCV0Rb3qzHGsTUuhxRU1wJYurKeDXJG3I4zfzEO9GL+Cml2JUOimZPL5+6ml0KOxcA+1bHiKM/kngybYerq8f8xnOHeu9xjYfMVeMZ3h2mxqcq2ynaM/ujruU0SBe8XxEa5FfQvv0zAb0udOugttbB1jxY4bYKsluG8YDOTaf9/EutCrKvJ7rtCLstDnM1Ouj4JMsaef4fdChbblcYD50ergmlJvyvPiJjiXSbQHtmUjUmhJ9DP0v80WzqlzItfTfYVmXz/G2lxT6HtMUlx3JzHWPYYt+0zLxRi2BebDQGDt3x3iWnZqY7wvLg+24mz80FZos2hY1xddtOk25up8gnVUKuQcMNAxDmY6+qln4ByYJX7XK3Hez5J7ch98nDvbwDkILFwbl4lKgD7U7WGutlSpOpV1cNuFQq+lj/nKa2Dubk3RXyoTfA6s5bKtr+Far1obajr+m3hb8cikVOjLrEWyz+c21m6nI+xH18U8Ma/gj/anmL+7iucStiPHStLH8fFt7H9YYt9OFPFjNRVaVe1npc81B/NXOsf7zBTaeI8C/3KBEEIIIYQQQgghhBBCCCHngi8XCCGEEEIIIYQQQgghhBByLvhygRBCCCGEEEIIIYQQQggh54IvFwghhBBCCCGEEEIIIYQQci4eWdC5gToPYnCKohGmJgu9lDEKg5xmKF6RKwSdNRe/Oy4X3of0USTNwK8J0UBhk/0+2moWitL4gSwkc7yquG+B4kG+QgBlHKIgR5nK4kyJhWJziYGio1aM/S8KnNJuIAs6O3UUQCs8/M1ZOgSbrRDzmq+isElal8WIQl0h+qwQzFo2il0UcKq5KLroVmX/biqEYEWKoku+iz6UB+hDribPQaKhIFI5RyUfM0B/KUsU7bQVoohVXfbb6gDbbDyJfZ2neE9NHftRDeTxCCOFGLqG70etFMWayhb2w1kQ/YtQH07kT2AyqQq8z0BDMaWyQBH5IpDzRBahWI4fPnJaftPi3FCItSeYO4y5LHZk6kNoU1ZQEKnwUATXVIh27ywIlE1L7ENZYO7bSVFMSfMUuW+Avly4cq42anh9J8V7chTCY6sFCiLP+7KAk2Vi3pi6KmFc7Kue4vqZLijvDhVCW50K+nalizFQlLjWRxn2Y1qT2wU55q9a791gWzb0qzje7U2F+FYk+1qgX4I2SYnzPnNxnbU7CkHYhVxaznG8swD9cT7HfOXlGCu2i/dUseTrxQohMl0RT7aH92QpRGLzBfH2rSoKACYerg36XCFiKDBmjYXxnpsYwy0f7ykqsK9N7wbYOh7WwGEirzXxCNvoXVXBuzxMr6CfdRQixuVCDbKi8J+ajkLKaybmoJlCjDtd2KvkBrZZzbC2KwzMtblirUtzjP1soR++op7xa4q+Zii2LhLF3sKQ83Qa4vfmUxwfI8CY1jWFb5dyvJoKZcUkVuS3Eu9p0MZauihxbA1dXp8qIV6rrRBqXTama7jGagohe8OXbZmBuatM8VpxSyEOv6gwLoQoTdkWKuJuMFN8z8C86ijWhbqDtjSX/bQIca3zNIXouCLXjgXm/EZTXhvixecKQgg/wfXD1BXi8InCvxfWujTGtSK1FQLvPt7TmkLYs1TsoUYTeV5U97TT38XfXDKSdfSFjom+NslxXuyrCzWCQow7KdFWxugLDR9/0/LQP7RsodbScb1O8fGOKEr0Bb2Gcbbt4TzHCzWZq1obLMyR4RD7oWfYzqlih/OpIh6FPN61Go6PX2AcmIViXnL8zYmrev4i9yMpsb5QlASifYh+tUwM2ijQ27Bw35AbOLaVhanLwm1oEznoQ5FCEHnQUNT1KfpofUGo3qign1WEQgxZx9pKU+xjowj7cbwQi36gEK52MK5zRe1TUwipW4q8bObobOXCc4lijuOo2t/ZDeyva2Bu0nX8zXkp1wBZiHWC0LAfu6dYczwK/MsFQgghhBBCCCGEEEIIIYScC75cIIQQQgghhBBCCCGEEELIueDLBUIIIYQQQgghhBBCCCGEnAu+XCCEEEIIIYQQQgghhBBCyLnQyrJUqAoRQgghhBBCCCGEEEIIIYSo4V8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzgVfLhBCCCGEEEIIIYQQQggh5Fzw5QIhhBBCCCGEEEIIIYQQQs4FXy4QQgghhBBCCCGEEEIIIeRc8OUCIYQQQgghhBBCCCGEEELOBV8uEEIIIYQQQgghhBBCCCHkXPDlAiGEEEIIIYQQQgghhBBCzgVfLhBCCCGEEEIIIYQQQggh5Fz8/wCRBp3Cni2mpQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def predict_step(model, verbose=False):\n",
        "    \"\"\"\n",
        "    Performs predictions while also storing and displaying the denoised image\n",
        "    at the end of each timestep.\n",
        "\n",
        "    Inputs:\n",
        "      model, a Model object to perform the prediction on\n",
        "      verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "    Returns:\n",
        "      nothing (plots the progression of the denoised image at each\n",
        "      timestep through the reverse diffusion process)\n",
        "    \"\"\"\n",
        "    # Generate 8 random images, as well as a list of images representing the\n",
        "    # denoised image after each timestep.\n",
        "    xs = []\n",
        "    rand_images = torch.stack([convert_rgb_to_greyscale(standardize_image(np.random.randn(model.img_size - 4, model.img_size - 4), verbose=verbose), verbose=verbose) for _ in range(8)]).float()\n",
        "    x = rand_images.to(device)\n",
        "\n",
        "    # Denoise the image after each timestep, adding the denoised image to a list.\n",
        "    with torch.no_grad():\n",
        "        for i in trange(model.timesteps):\n",
        "            t = i\n",
        "            x = model(x, torch.full([8, 1], t, dtype=torch.float, device=device))\n",
        "            xs.append(x[0].cpu())\n",
        "    # Convert the denoised images for viewing.\n",
        "    xs = torch.stack(xs, dim=0)\n",
        "    xs = torch.clip(xs, -1, 1)\n",
        "    xs = cvtImg(xs)\n",
        "\n",
        "    # Plot the denoised images.\n",
        "    plt.figure(figsize=(20, 2))\n",
        "    for i in range(len(xs)):\n",
        "        plt.subplot(1, len(xs), i+1)\n",
        "        plt.imshow(xs[i])\n",
        "        plt.title(f'{i}')\n",
        "        plt.axis('off')\n",
        "\n",
        "predict_step(Model([], [], timesteps, IMG_SIZE, LEARNING_RATE).to(device), verbose=verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ede500db-da1d-4b88-ab1d-07131e2a1823",
      "metadata": {
        "id": "ede500db-da1d-4b88-ab1d-07131e2a1823"
      },
      "outputs": [],
      "source": [
        "def train_one(model, x_img, opt=None, lr_scheduler=None):\n",
        "    \"\"\"\n",
        "    Trains the model on one input image.\n",
        "\n",
        "    Inputs:\n",
        "      model, a Model object to train and update the parameters for\n",
        "      x_img, a four dimensional tensor representing one image input to the model\n",
        "      opt, an optional Optimizer argument to use for training the model (otherwise, assumed\n",
        "      to be a part of the model)\n",
        "      lr_scheduler, an optional LearningRateScheduler argument to use for training the model\n",
        "      (otherwise, assumed to be a part of the model)\n",
        "\n",
        "    Returns:\n",
        "      a float representing the loss after training the model on the input image\n",
        "    \"\"\"\n",
        "    # Generate a random timestep, as well as the image and the noisy image at the next timestep.\n",
        "    x_ts = generate_ts(len(x_img), model.timesteps)\n",
        "    x_a, x_b = forward_noise(x_img, x_ts, model.data_list, model.img_size)\n",
        "\n",
        "    # Fit these inputs to the device.\n",
        "    x_ts = torch.from_numpy(x_ts).view(-1, 1).float().to(device)\n",
        "    x_a = x_a.float().to(device)\n",
        "    x_b = x_b.float().to(device)\n",
        "\n",
        "    # Predict the resulting image, and backpropogate to minimize the loss at this timestep.\n",
        "    y_p = model(x_a, x_ts)\n",
        "    loss = torch.mean(torch.abs(y_p - x_b))\n",
        "    if opt == None:\n",
        "      model.opt.zero_grad()\n",
        "    else:\n",
        "      opt.zero_grad()\n",
        "    loss.backward()\n",
        "    if opt == None:\n",
        "      model.opt.step()\n",
        "    else:\n",
        "      opt.step()\n",
        "    if lr_scheduler != None:\n",
        "      lr_scheduler.step()\n",
        "\n",
        "    # Return the loss.\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "rSVXEWjoPTq9",
      "metadata": {
        "id": "rSVXEWjoPTq9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(loss_vals, loss_label='Loss'):\n",
        "  \"\"\"\n",
        "  Plots the loss values as a function of the iteration number.\n",
        "\n",
        "  Inputs:\n",
        "    loss_vals, a list of loss values to plot.\n",
        "    loss_label, a string labelling the type of loss to plot.\n",
        "\n",
        "  Outputs:\n",
        "    Nothing (plots the loss values by iteration number).\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(loss_vals, label=loss_label)\n",
        "  plt.xlabel('Iterations')\n",
        "  plt.ylabel(loss_label)\n",
        "  plt.title(f'{loss_label} vs. Iterations')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e9ffea54-b7cb-43c7-8f6b-54ceccfbbe99",
      "metadata": {
        "id": "e9ffea54-b7cb-43c7-8f6b-54ceccfbbe99"
      },
      "outputs": [],
      "source": [
        "def train(model, batch_size, R=50):\n",
        "    \"\"\"\n",
        "    Train the diffusion model using R epochs.\n",
        "\n",
        "    Inputs:\n",
        "      model, a Model object to train and update the parameters for\n",
        "      R, an integer representing the number of epochs used to train the model.\n",
        "\n",
        "    Returns:\n",
        "      nothing (optimizes the parameters in the model during training)\n",
        "    \"\"\"\n",
        "    bar = trange(R)\n",
        "    total = len(model.base_img_idxs)\n",
        "    loss_vals = []\n",
        "    # For each epoch, train on all available images.\n",
        "    for i in bar:\n",
        "      for idx in range(0, len(model.base_img_idxs), batch_size):\n",
        "        # print(f'idx: {idx}')\n",
        "        batch_idxs = []\n",
        "        for actual_base_idx in range(idx, idx + batch_size):\n",
        "          # print(f'actual_base_idx: {actual_base_idx}')\n",
        "          batch_idxs.append(model.base_img_idxs[actual_base_idx])\n",
        "        loss = train_one(model, batch_idxs)\n",
        "        loss_vals.append(loss)\n",
        "        pg = (idx / total) * 100\n",
        "        if idx % 5 == 0:\n",
        "          bar.set_description(f'loss: {loss:.5f}, p: {pg:.2f}%')\n",
        "    plot_loss(loss_vals, 'Training Loss')\n",
        "    return loss_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zc39DEDN6AE8",
      "metadata": {
        "id": "zc39DEDN6AE8"
      },
      "source": [
        "# Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "Sw9RZFGr6DEW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw9RZFGr6DEW",
        "outputId": "589ea236-bb66-4965-939d-89118caf67f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting scipy==1.11.1\n",
            "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy==1.11.1) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (9.4.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.1->pytorch-fid)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Installing collected packages: scipy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-fid\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch-fid-0.3.0 scipy-1.11.1\n",
            "Collecting ray[tune]\n",
            "  Downloading ray-2.24.0-cp310-cp310-manylinux2014_x86_64.whl (65.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.14.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (24.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.0.3)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]) (1.16.0)\n",
            "Installing collected packages: tensorboardX, ray\n",
            "Successfully installed ray-2.24.0 tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "# Install packages for FID calculation for validation loss (hyperparameter tuning)\n",
        "%pip install pytorch-fid scipy==1.11.1\n",
        "\n",
        "# For concurrent hyperparameter optimization\n",
        "%pip install ray[tune]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "K0aYJ5Z46TSf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0aYJ5Z46TSf",
        "outputId": "71bca93c-4bbf-4b16-aea3-1877dfc84582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPUs: 8\n",
            "Number of GPUs: 1\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# See the number of resources available on the system\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "num_cpus = multiprocessing.cpu_count()\n",
        "print(f\"Number of CPUs: {num_cpus}\")\n",
        "\n",
        "import torch\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Number of GPUs: {num_gpus}\")\n",
        "if num_gpus > 0:\n",
        "    for i in range(num_gpus):\n",
        "        print(torch.cuda.get_device_name(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "F10S3B5kH5d5",
      "metadata": {
        "id": "F10S3B5kH5d5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def save_images(images, directory):\n",
        "    \"\"\"\n",
        "    Saves an array of images to the specified directory.\n",
        "\n",
        "    Inputs:\n",
        "      images, a numpy array of grayscale images to save\n",
        "      directory, a string representing the directory to savee the images\n",
        "\n",
        "    Returns:\n",
        "      Nothing (saves images at directory)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    for i, img in enumerate(images):\n",
        "        img = Image.fromarray(np.uint8(img * 255), 'L')  # Convert to PIL Image\n",
        "        img = img.resize((299, 299))  # Resize to fit InceptionV3 size requirements\n",
        "        img = img.convert('RGB')  # Convert to RGB\n",
        "        img.save(os.path.join(directory, f'image_{i}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "vOd7HiYlGzw1",
      "metadata": {
        "id": "vOd7HiYlGzw1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def setup_testing_folder(digit, img_path, num_qubits, load_training_data, num_images_fid, classical, verbose=False):\n",
        "  \"\"\"\n",
        "  Saves the testing images for the specified digit to the specified file path.\n",
        "\n",
        "  Inputs:\n",
        "    digit, an integer representing the testing digit class\n",
        "    img_path, a string representing the file path to save the images to\n",
        "    num_qubits, an integer representing the number of qubits on the quantum hardware (for PCA reduction)\n",
        "    load_training_data, a function that returns the training data in the form\n",
        "      (x_train, y_train), (x_test, y_test)\n",
        "    num_images_fid, an integer representing the number of\n",
        "      testing images to save in the final directory\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Returns:\n",
        "    Nothing (saves the testing images to img_path)\n",
        "  \"\"\"\n",
        "  # Get the indices of the images used for the training data\n",
        "  _, base_img_idxs = get_training_data(digit, img_path, load_training_data, classical, verbose)\n",
        "\n",
        "  rand_indices = base_img_idxs\n",
        "  if verbose:\n",
        "    print(f'rand_indices: {rand_indices}')\n",
        "  if len(rand_indices) != 500:\n",
        "    print(f'len(rand_indices) is not 500, but should be: {len(rand_indices)}')\n",
        "\n",
        "  num_pca_components = num_qubits\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = load_training_data()\n",
        "\n",
        "  # print(\"loaded mnist data\")\n",
        "\n",
        "  x_train_flat = x_train.reshape(-1, 28*28) / 255.0\n",
        "  x_test_flat = x_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "  # print(f'one x_train_flat data, scaled and normalized: {x_train_flat[0]}')\n",
        "\n",
        "  x_train_category = x_train_flat[y_train == digit]\n",
        "  x_test_category = x_test_flat[y_test == digit]\n",
        "\n",
        "  one_x_categ = x_train_category[10] * 255\n",
        "  one_x_categ = one_x_categ.reshape(28, 28)\n",
        "\n",
        "  # print(f'one x_train_category data: {one_x_categ}')\n",
        "  # plt.imshow(one_x_categ, cmap=\"gray\")\n",
        "  # plt.show()\n",
        "\n",
        "  # Reshape dimension for each image, assuming the images are 28x28\n",
        "  image_shape = (28, 28)\n",
        "\n",
        "  # Store the testing images in a folder\n",
        "  testing_list = []\n",
        "\n",
        "  # Add each testing image image\n",
        "  for i in range(len(x_test_category)):\n",
        "      # Reshape the flattened image back to its original 28x28 dimensions\n",
        "      image_reshaped = x_test_category[i].reshape(image_shape)\n",
        "      testing_list.append(image_reshaped)\n",
        "\n",
        "  # Add all images in the training dataset that were not used for training the diffusion model\n",
        "  for i in range(len(x_train_category)):\n",
        "    if i not in rand_indices:\n",
        "      train_image_reshaped = x_train_category[i].reshape(image_shape)\n",
        "\n",
        "      testing_list.append(train_image_reshaped)\n",
        "\n",
        "  # Save num_images_fid images from the testing dataset\n",
        "  testing_list = testing_list[len(testing_list) - num_images_fid:]\n",
        "  save_images(testing_list, 'test_img' + str(digit))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "qdGf4-kRIOfU",
      "metadata": {
        "id": "qdGf4-kRIOfU"
      },
      "outputs": [],
      "source": [
        "def get_generated_images(model, img_size, num_image_batches=158, verbose=False):\n",
        "  \"\"\"\n",
        "  Generate num_image_batches * 32 number of images from the specified model.\n",
        "\n",
        "  Inputs:\n",
        "    model, a Model object to obtain generated images from\n",
        "    img_size, an integer representing the length and width of the image\n",
        "    num_image_batches, an integer representing the number of batches to generate\n",
        "    verbose, a boolean indicating whether or not to print debugging output\n",
        "\n",
        "  Outputs:\n",
        "    numpy_gen_imgs, a numpy array of shape (num_image_batches * 32, img_size, img_size)\n",
        "    containing the generated images.\n",
        "  \"\"\"\n",
        "  # Generate num_image_batches batches of images (where each batch contains 32 images).\n",
        "  generated_images = []\n",
        "  for _ in range(num_image_batches):\n",
        "    generated_images.append(predict(model=model, verbose=verbose, show_img=False))\n",
        "  # Randomly shuffle the generated images.\n",
        "  np.random.shuffle(generated_images)\n",
        "  # print('loaded', len(generated_images))\n",
        "\n",
        "  # Convert the generated images to a numpy array, and save them at a specified directory.\n",
        "  # print(generated_images[0].shape)\n",
        "  generated_images_tensor = torch.cat(generated_images, axis=0)\n",
        "  # print(generated_images_tensor.shape)\n",
        "  permuted_gen_tensor = torch.permute(generated_images_tensor, (0, 2, 3, 1))\n",
        "  # print(permuted_gen_tensor.shape)\n",
        "  numpy_gen_imgs = permuted_gen_tensor.cpu().numpy()\n",
        "  # print(numpy_gen_imgs.shape)\n",
        "  # print(type(numpy_gen_imgs))\n",
        "\n",
        "  return numpy_gen_imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "MNW-Lfe4GxA2",
      "metadata": {
        "id": "MNW-Lfe4GxA2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def calc_validation_loss(model, digit, img_size, img_path, num_qubits, load_training_data, validation_loss_func, num_images_fid, classical, verbose=False):\n",
        "  \"\"\"\n",
        "  Calculates the validation loss for the input model.\n",
        "\n",
        "  Inputs:\n",
        "    model, the model for which to calculate the validation loss for\n",
        "    digit, an integer representing the testing digit class\n",
        "    img_size, an integer representing the length and with of the input images\n",
        "    img_path, a string representing the file path to save the images to\n",
        "    num_qubits, an integer representing the number of qubits on the quantum hardware (for PCA reduction)\n",
        "    load_training_data, a function that returns the training data in the form\n",
        "      (x_train, y_train), (x_test, y_test)\n",
        "    validation_loss_func, the function to use to calculate the validation loss\n",
        "    num_images_fid, an integer representing the number of\n",
        "      testing images to save in the final directory\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Outputs:\n",
        "    a float representing the validation loss value (the FID score) for the model.\n",
        "  \"\"\"\n",
        "  # Set up the folder containing the testing data.\n",
        "  setup_testing_folder(digit, img_path, num_qubits, load_training_data, num_images_fid, classical, verbose=verbose)\n",
        "\n",
        "  # The number of images needed for FID calculuation for the specified size.\n",
        "  num_image_batches = int(math.ceil(num_images_fid / 32))\n",
        "\n",
        "  # Save the generated images for the model.\n",
        "  actual_npy_arr = get_generated_images(model, img_size, num_image_batches=num_image_batches, verbose=verbose)\n",
        "  actual_imgs_list = [actual_npy_arr[i] for i in range(actual_npy_arr.shape[0])]\n",
        "  reshaped_actual_imgs = []\n",
        "  for actual_img in actual_imgs_list:\n",
        "    grayscale_img = actual_img[2:-2, 2:-2, 0]\n",
        "\n",
        "    # print(grayscale_img.shape)\n",
        "    reshaped_actual_imgs.append(grayscale_img)\n",
        "  reshaped_actual_imgs = reshaped_actual_imgs[len(reshaped_actual_imgs) - num_images_fid:]\n",
        "  save_images(reshaped_actual_imgs, 'gen_img' + str(digit))\n",
        "\n",
        "  # Paths to the directories containing the two image sets\n",
        "  path1 = 'gen_img' + str(digit)\n",
        "  path2 = 'test_img' + str(digit)\n",
        "\n",
        "  # Calculate FID score\n",
        "  fid_value = validation_loss_func([path1, path2], batch_size=50, device='cuda', dims=2048)\n",
        "  if verbose:\n",
        "    print(f'Digit {digit} FID score, Quantum:', fid_value)\n",
        "\n",
        "  return fid_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "NFG6oPtFXPth",
      "metadata": {
        "id": "NFG6oPtFXPth"
      },
      "outputs": [],
      "source": [
        "def get_global_vars():\n",
        "  \"\"\"\n",
        "  Get the configuration variables for the model.\n",
        "\n",
        "  Inputs:\n",
        "    None\n",
        "\n",
        "  Outputs:\n",
        "    The configuration variables for the model.\n",
        "  \"\"\"\n",
        "  return timesteps, IMG_SIZE, DIGIT, IMG_PATH, NUM_QUBITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "UlgdGMvLIWwy",
      "metadata": {
        "id": "UlgdGMvLIWwy"
      },
      "outputs": [],
      "source": [
        "from ray import tune, train\n",
        "\n",
        "def train_diffusion_model(config, load_training_data, validation_loss_func, num_images_fid, get_global_vars, classical, checkpoint_dir=None, val_interval=500, verbose=False):\n",
        "    \"\"\"\n",
        "    Train a diffusion model interfacing with ray tune.\n",
        "\n",
        "    Inputs:\n",
        "      config, a dictionary providing the configuration variables for the run for this training\n",
        "      load_training_data, a function that returns the training data in the form\n",
        "        (x_train, y_train), (x_test, y_test)\n",
        "      validation_loss_func, the function to use to calculate the validation loss\n",
        "      checkpoint_dir, an optional string to indicate the directory to store the checkpoint results\n",
        "      val_interval, an integer indicating the interval of epochs for which to calculate validation loss\n",
        "      verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "    Outputs:\n",
        "      Nothing (updates ray tune with the validation loss for this run)\n",
        "    \"\"\"\n",
        "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # TODO: add timesteps\n",
        "\n",
        "    timesteps, img_size, digit, img_path, num_qubits = get_global_vars()\n",
        "\n",
        "    data_list, base_img_idxs = get_training_data(digit, img_path, load_training_data, classical, verbose)\n",
        "\n",
        "    model = Model(data_list, base_img_idxs, timesteps, img_size, config[\"lr\"]).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss()  # Update based on your loss function\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config[\"lr_step_size\"], gamma=config[\"lr_gamma\"])\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "        checkpoint = torch.load(path)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "        lr_scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
        "\n",
        "\n",
        "    bar = range(int(config['num_epochs']))\n",
        "    total = len(base_img_idxs)\n",
        "    # loss_vals = []\n",
        "    # For each epoch, train on all available images.\n",
        "    cur_batch_size = int(config['batch_size'])\n",
        "    # cur_timesteps = int(config['timesteps'])\n",
        "    for i in bar:\n",
        "      for idx in range(0, len(base_img_idxs), cur_batch_size):\n",
        "        # print(f'idx: {idx}')\n",
        "        batch_idxs = []\n",
        "        for actual_base_idx in range(idx, min(len(base_img_idxs), idx + cur_batch_size)):\n",
        "          # print(f'actual_base_idx: {actual_base_idx}')\n",
        "          batch_idxs.append(base_img_idxs[actual_base_idx])\n",
        "        loss = train_one(model, batch_idxs, opt=optimizer, lr_scheduler=lr_scheduler)\n",
        "        # loss_vals.append(loss)\n",
        "        # pg = (idx / total) * 100\n",
        "        # if idx % 5 == 0:\n",
        "        #   bar.set_description(f'loss: {loss:.5f}, p: {pg:.2f}%')\n",
        "      if (i + 1) % val_interval == 0:\n",
        "        validation_loss = calc_validation_loss(model, digit, img_size, img_path, num_qubits, load_training_data, validation_loss_func, num_images_fid, classical, verbose)\n",
        "        train.report(dict(loss=validation_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "imPyOPQIJ7nu",
      "metadata": {
        "id": "imPyOPQIJ7nu"
      },
      "outputs": [],
      "source": [
        "def get_save_filepath():\n",
        "  \"\"\"\n",
        "  Obtains the filepath to store the resulting data.\n",
        "\n",
        "  Inputs:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    a string containing the path to the directory to save the results\n",
        "  \"\"\"\n",
        "  return DEST_DIR_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "oKbxfz-prdP7",
      "metadata": {
        "id": "oKbxfz-prdP7"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune import CLIReporter\n",
        "from functools import partial\n",
        "import json\n",
        "\n",
        "def hyperparameter_tuning(config, load_training_data, validation_loss_func, num_images_fid, get_global_vars, get_save_filepath, classical, num_samples=10, max_num_epochs=2000, num_parallel_trials=4, verbose=True):\n",
        "    \"\"\"\n",
        "    Performs hyperparameter tuning via random search for the diffusion model.\n",
        "\n",
        "    Inputs:\n",
        "      config, a configuration dictionary indicating the search space for the hyperparameters.\n",
        "      load_training_data, a function that returns the training data in the form\n",
        "        (x_train, y_train), (x_test, y_test)\n",
        "      validation_loss_func, the function to use to calculate the validation loss\n",
        "      get_global_vars, a function returning the variables used for configuring the model\n",
        "      num_samples, an integer representing the number of runs to do for hyperparameter tuning\n",
        "      max_num_epochs, an integer representing the maximum number of epochs for any one run\n",
        "      num_parallel_trials, an integer representing the number of parallel trials to run\n",
        "      verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "    Outputs:\n",
        "      None (saves the results of hyperparameter tuning to a file)\n",
        "    \"\"\"\n",
        "    # Define a scheduler to decide how to schedule each trial on the hardware\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=500,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    # Displays the results of hyperparameter tuning.\n",
        "    reporter = CLIReporter(metric_columns=[\"loss\", \"training_iteration\"])\n",
        "\n",
        "    # Define the function for which the trial of training the diffusion model should be in.\n",
        "    train_diffusion_model_args = partial(train_diffusion_model, load_training_data=load_training_data, validation_loss_func=validation_loss_func, num_images_fid=num_images_fid, get_global_vars=get_global_vars, classical=classical)\n",
        "\n",
        "    result = tune.run(\n",
        "        train_diffusion_model_args,\n",
        "        resources_per_trial={\"cpu\": (multiprocessing.cpu_count() / num_parallel_trials), \"gpu\": (torch.cuda.device_count() / num_parallel_trials)},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter\n",
        "    )\n",
        "\n",
        "    # Save the best trial\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    if verbose:\n",
        "      print(f\"Best trial config: {best_trial.config}\")\n",
        "      print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "    # Assuming best_trial is obtained as shown before\n",
        "    best_config = best_trial.config\n",
        "    best_loss = best_trial.last_result['loss']\n",
        "\n",
        "    # Prepare a dictionary to save\n",
        "    save_dict = {\n",
        "        \"best_config\": best_config,\n",
        "        \"best_loss\": best_loss\n",
        "    }\n",
        "\n",
        "    save_filepath = get_save_filepath()\n",
        "\n",
        "    # Choose a file path\n",
        "    file_path = f\"{save_filepath}/best_trial_config.json\"\n",
        "\n",
        "    # Write to a JSON file\n",
        "    with open(file_path, \"w\") as f:\n",
        "        json.dump(save_dict, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "FIFmMZFMkX4i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIFmMZFMkX4i",
        "outputId": "51813629-d076-42b8-8f3d-5cc69e2be9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "print(os.cpu_count())\n",
        "print(multiprocessing.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Exnju6M83XB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Exnju6M83XB",
        "outputId": "90980ff3-cf15-4452-8bd3-6ed6326d5d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "2024-06-10 00:19:11,091\tINFO worker.py:1753 -- Started a local Ray instance.\n",
            "2024-06-10 00:19:11,930\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
            "2024-06-10 00:19:12,097\tWARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_diffusion_model_2024-06-10_00-19-11   |\n",
            "+------------------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator                       |\n",
            "| Scheduler                        AsyncHyperBandScheduler                     |\n",
            "| Number of trials                 20                                          |\n",
            "+------------------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_diffusion_model_2024-06-10_00-19-11\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-06-10_00-19-08_924375_1449/artifacts/2024-06-10_00-19-11/train_diffusion_model_2024-06-10_00-19-11/driver_artifacts`\n",
            "\n",
            "Trial status: 20 PENDING\n",
            "Current time: 2024-06-10 00:19:12. Total running time: 0s\n",
            "Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                          status       batch_size            lr     lr_step_size     lr_gamma     num_epochs |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_diffusion_model_0ff20_00000   PENDING               2   0.00286089               100          0.9            500 |\n",
            "| train_diffusion_model_0ff20_00001   PENDING               4   0.0703962                200          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00002   PENDING              32   1.47975e-05               50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00003   PENDING              32   0.00209113                50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00004   PENDING               4   0.0043282                200          0.1           1500 |\n",
            "| train_diffusion_model_0ff20_00005   PENDING               8   0.018564                  50          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00006   PENDING               8   1.47636e-05              100          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00007   PENDING              16   0.0924523                 50          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00008   PENDING              16   9.30428e-05              100          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00009   PENDING               2   0.00205629               200          0.9           2000 |\n",
            "| train_diffusion_model_0ff20_00010   PENDING               2   1.25802e-05               50          0.1           2000 |\n",
            "| train_diffusion_model_0ff20_00011   PENDING               4   3.027e-05                100          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00012   PENDING              32   5.13397e-05              200          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00013   PENDING               8   0.000349705              200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00014   PENDING               8   0.00413991               200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00015   PENDING               8   1.33576e-05              200          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00016   PENDING              32   0.00839812               200          0.9           1500 |\n",
            "| train_diffusion_model_0ff20_00017   PENDING              16   0.00142173                50          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00018   PENDING               8   3.64553e-05               50          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00019   PENDING              32   9.17256e-05              100          0.5           1500 |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=3233)\u001b[0m 2024-06-10 00:19:17.343814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=3233)\u001b[0m 2024-06-10 00:19:17.343873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=3233)\u001b[0m 2024-06-10 00:19:17.345543: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=3236)\u001b[0m 2024-06-10 00:19:18.890852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_diffusion_model_0ff20_00003 started with configuration:\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_diffusion_model_0ff20_00003 config             |\n",
            "+------------------------------------------------------------+\n",
            "| batch_size                                              32 |\n",
            "| lr                                                 0.00209 |\n",
            "| lr_gamma                                               0.5 |\n",
            "| lr_step_size                                            50 |\n",
            "| num_epochs                                            1500 |\n",
            "+------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3236)\u001b[0m Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[36m(func pid=3236)\u001b[0m \r    8192/11490434 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r11490434/11490434 [==============================] - 0s 0us/step\n",
            "\n",
            "Trial train_diffusion_model_0ff20_00002 started with configuration:\n",
            "+----------------------------------------------------------+\n",
            "| Trial train_diffusion_model_0ff20_00002 config           |\n",
            "+----------------------------------------------------------+\n",
            "| batch_size                                            32 |\n",
            "| lr                                                 1e-05 |\n",
            "| lr_gamma                                             0.5 |\n",
            "| lr_step_size                                          50 |\n",
            "| num_epochs                                          1500 |\n",
            "+----------------------------------------------------------+\n",
            "\n",
            "Trial train_diffusion_model_0ff20_00000 started with configuration:\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_diffusion_model_0ff20_00000 config             |\n",
            "+------------------------------------------------------------+\n",
            "| batch_size                                               2 |\n",
            "| lr                                                 0.00286 |\n",
            "| lr_gamma                                               0.9 |\n",
            "| lr_step_size                                           100 |\n",
            "| num_epochs                                             500 |\n",
            "+------------------------------------------------------------+\n",
            "\n",
            "Trial train_diffusion_model_0ff20_00001 started with configuration:\n",
            "+-----------------------------------------------------------+\n",
            "| Trial train_diffusion_model_0ff20_00001 config            |\n",
            "+-----------------------------------------------------------+\n",
            "| batch_size                                              4 |\n",
            "| lr                                                 0.0704 |\n",
            "| lr_gamma                                              0.1 |\n",
            "| lr_step_size                                          200 |\n",
            "| num_epochs                                            500 |\n",
            "+-----------------------------------------------------------+\n",
            "\n",
            "Trial status: 4 RUNNING | 16 PENDING\n",
            "Current time: 2024-06-10 00:19:42. Total running time: 30s\n",
            "Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                          status       batch_size            lr     lr_step_size     lr_gamma     num_epochs |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_diffusion_model_0ff20_00000   RUNNING               2   0.00286089               100          0.9            500 |\n",
            "| train_diffusion_model_0ff20_00001   RUNNING               4   0.0703962                200          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00002   RUNNING              32   1.47975e-05               50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00003   RUNNING              32   0.00209113                50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00004   PENDING               4   0.0043282                200          0.1           1500 |\n",
            "| train_diffusion_model_0ff20_00005   PENDING               8   0.018564                  50          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00006   PENDING               8   1.47636e-05              100          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00007   PENDING              16   0.0924523                 50          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00008   PENDING              16   9.30428e-05              100          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00009   PENDING               2   0.00205629               200          0.9           2000 |\n",
            "| train_diffusion_model_0ff20_00010   PENDING               2   1.25802e-05               50          0.1           2000 |\n",
            "| train_diffusion_model_0ff20_00011   PENDING               4   3.027e-05                100          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00012   PENDING              32   5.13397e-05              200          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00013   PENDING               8   0.000349705              200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00014   PENDING               8   0.00413991               200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00015   PENDING               8   1.33576e-05              200          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00016   PENDING              32   0.00839812               200          0.9           1500 |\n",
            "| train_diffusion_model_0ff20_00017   PENDING              16   0.00142173                50          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00018   PENDING               8   3.64553e-05               50          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00019   PENDING              32   9.17256e-05              100          0.5           1500 |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "Trial status: 4 RUNNING | 16 PENDING\n",
            "Current time: 2024-06-10 00:20:12. Total running time: 1min 0s\n",
            "Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                          status       batch_size            lr     lr_step_size     lr_gamma     num_epochs |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_diffusion_model_0ff20_00000   RUNNING               2   0.00286089               100          0.9            500 |\n",
            "| train_diffusion_model_0ff20_00001   RUNNING               4   0.0703962                200          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00002   RUNNING              32   1.47975e-05               50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00003   RUNNING              32   0.00209113                50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00004   PENDING               4   0.0043282                200          0.1           1500 |\n",
            "| train_diffusion_model_0ff20_00005   PENDING               8   0.018564                  50          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00006   PENDING               8   1.47636e-05              100          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00007   PENDING              16   0.0924523                 50          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00008   PENDING              16   9.30428e-05              100          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00009   PENDING               2   0.00205629               200          0.9           2000 |\n",
            "| train_diffusion_model_0ff20_00010   PENDING               2   1.25802e-05               50          0.1           2000 |\n",
            "| train_diffusion_model_0ff20_00011   PENDING               4   3.027e-05                100          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00012   PENDING              32   5.13397e-05              200          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00013   PENDING               8   0.000349705              200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00014   PENDING               8   0.00413991               200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00015   PENDING               8   1.33576e-05              200          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00016   PENDING              32   0.00839812               200          0.9           1500 |\n",
            "| train_diffusion_model_0ff20_00017   PENDING              16   0.00142173                50          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00018   PENDING               8   3.64553e-05               50          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00019   PENDING              32   9.17256e-05              100          0.5           1500 |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "Trial status: 4 RUNNING | 16 PENDING\n",
            "Current time: 2024-06-10 00:20:42. Total running time: 1min 30s\n",
            "Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                          status       batch_size            lr     lr_step_size     lr_gamma     num_epochs |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_diffusion_model_0ff20_00000   RUNNING               2   0.00286089               100          0.9            500 |\n",
            "| train_diffusion_model_0ff20_00001   RUNNING               4   0.0703962                200          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00002   RUNNING              32   1.47975e-05               50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00003   RUNNING              32   0.00209113                50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00004   PENDING               4   0.0043282                200          0.1           1500 |\n",
            "| train_diffusion_model_0ff20_00005   PENDING               8   0.018564                  50          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00006   PENDING               8   1.47636e-05              100          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00007   PENDING              16   0.0924523                 50          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00008   PENDING              16   9.30428e-05              100          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00009   PENDING               2   0.00205629               200          0.9           2000 |\n",
            "| train_diffusion_model_0ff20_00010   PENDING               2   1.25802e-05               50          0.1           2000 |\n",
            "| train_diffusion_model_0ff20_00011   PENDING               4   3.027e-05                100          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00012   PENDING              32   5.13397e-05              200          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00013   PENDING               8   0.000349705              200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00014   PENDING               8   0.00413991               200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00015   PENDING               8   1.33576e-05              200          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00016   PENDING              32   0.00839812               200          0.9           1500 |\n",
            "| train_diffusion_model_0ff20_00017   PENDING              16   0.00142173                50          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00018   PENDING               8   3.64553e-05               50          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00019   PENDING              32   9.17256e-05              100          0.5           1500 |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "Trial status: 4 RUNNING | 16 PENDING\n",
            "Current time: 2024-06-10 00:21:12. Total running time: 2min 0s\n",
            "Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                          status       batch_size            lr     lr_step_size     lr_gamma     num_epochs |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_diffusion_model_0ff20_00000   RUNNING               2   0.00286089               100          0.9            500 |\n",
            "| train_diffusion_model_0ff20_00001   RUNNING               4   0.0703962                200          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00002   RUNNING              32   1.47975e-05               50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00003   RUNNING              32   0.00209113                50          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00004   PENDING               4   0.0043282                200          0.1           1500 |\n",
            "| train_diffusion_model_0ff20_00005   PENDING               8   0.018564                  50          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00006   PENDING               8   1.47636e-05              100          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00007   PENDING              16   0.0924523                 50          0.1            500 |\n",
            "| train_diffusion_model_0ff20_00008   PENDING              16   9.30428e-05              100          0.5           1500 |\n",
            "| train_diffusion_model_0ff20_00009   PENDING               2   0.00205629               200          0.9           2000 |\n",
            "| train_diffusion_model_0ff20_00010   PENDING               2   1.25802e-05               50          0.1           2000 |\n",
            "| train_diffusion_model_0ff20_00011   PENDING               4   3.027e-05                100          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00012   PENDING              32   5.13397e-05              200          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00013   PENDING               8   0.000349705              200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00014   PENDING               8   0.00413991               200          0.5           2000 |\n",
            "| train_diffusion_model_0ff20_00015   PENDING               8   1.33576e-05              200          0.5           1000 |\n",
            "| train_diffusion_model_0ff20_00016   PENDING              32   0.00839812               200          0.9           1500 |\n",
            "| train_diffusion_model_0ff20_00017   PENDING              16   0.00142173                50          0.1           1000 |\n",
            "| train_diffusion_model_0ff20_00018   PENDING               8   3.64553e-05               50          0.5            500 |\n",
            "| train_diffusion_model_0ff20_00019   PENDING              32   9.17256e-05              100          0.5           1500 |\n",
            "+------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
        "\n",
        "# Define the search space for the hyperparameters.\n",
        "config = {\n",
        "    \"batch_size\": tune.choice([2, 4, 8, 16, 32]),\n",
        "    # \"timesteps\": tune.choice([2, 4, 6, 8]),\n",
        "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
        "    \"lr_step_size\": tune.choice([50, 100, 200]),\n",
        "    \"lr_gamma\": tune.choice([0.1, 0.5, 0.9]),\n",
        "    \"num_epochs\": tune.choice([500, 1000, 1500, 2000])\n",
        "}\n",
        "\n",
        "# Define the number of images to use for FID calculation.\n",
        "num_images_fid = 5000\n",
        "\n",
        "hyperparameter_tuning(config, tf.keras.datasets.mnist.load_data, calculate_fid_given_paths, num_images_fid, get_global_vars, get_save_filepath, CLASSICAL, num_samples=20, max_num_epochs=2000, num_parallel_trials=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PUWCth7l7SBe",
      "metadata": {
        "id": "PUWCth7l7SBe"
      },
      "source": [
        "# Model Training (Given Hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classical():\n",
        "  \"\"\"\n",
        "  Returns whether or not to train a diffusion model with classical noise.\n",
        "\n",
        "  Inputs:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    a boolean indicating whether or not to train with classical noise.\n",
        "  \"\"\"\n",
        "  return CLASSICAL"
      ],
      "metadata": {
        "id": "kdYyCURRcd6-"
      },
      "id": "kdYyCURRcd6-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xp57ynIH7l91",
      "metadata": {
        "id": "xp57ynIH7l91"
      },
      "outputs": [],
      "source": [
        "def get_hyperparameters():\n",
        "  \"\"\"\n",
        "  Obtains the hyperparameters for training one specific instance of model.\n",
        "\n",
        "  Inputs:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    The hyperparameters for training one instance of model.\n",
        "  \"\"\"\n",
        "  return TRAINING_ITERS, NUM_EPOCHS, BATCH_SIZE, LEARNING_RATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XkJoa-bzAjkr",
      "metadata": {
        "id": "XkJoa-bzAjkr"
      },
      "outputs": [],
      "source": [
        "def construct_model(get_global_vars, get_hyperparameters, get_training_data, load_training_data, classical, verbose=False):\n",
        "    \"\"\"\n",
        "    Constructs a model object for training.\n",
        "\n",
        "    Inputs:\n",
        "      get_global_vars, a function that returns configuration variables for the model\n",
        "      get_training_data, a function that returns the training data for the model\n",
        "\n",
        "    Outputs:\n",
        "      a Model object with configuration specified by get_global_vars and training data from get_training_data\n",
        "    \"\"\"\n",
        "    timesteps, img_size, digit, img_path, num_qubits = get_global_vars()\n",
        "\n",
        "    training_iters, num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
        "\n",
        "    data_list, base_img_idxs = get_training_data(digit, img_path, load_training_data, classical, verbose)\n",
        "\n",
        "    model = Model(data_list, base_img_idxs, timesteps, img_size, learning_rate).to(device)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SMFOltsZ7aQK",
      "metadata": {
        "id": "SMFOltsZ7aQK"
      },
      "outputs": [],
      "source": [
        "def train_model_hyperparams(model, get_global_vars, get_hyperparameters, load_training_data, validation_loss_func, num_images_fid, classical, verbose=False):\n",
        "  \"\"\"\n",
        "  Trains a model for a specific set of hyperparameters (decoupled from Ray Tune reporting).\n",
        "\n",
        "  Inputs:\n",
        "    model, the model for which to calculate the validation loss for\n",
        "    get_global_vars, a function returning the variables used for configuring the model\n",
        "    get_hyperparameters, a function returning the hyperparameters for this particular run\n",
        "    load_training_data, a function that returns the training data in the form\n",
        "      (x_train, y_train), (x_test, y_test)\n",
        "    validation_loss_func, the function to use to calculate the validation loss\n",
        "    num_images_fid, an integer representing the number of\n",
        "      testing images to save in the final directory\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Outputs:\n",
        "    The same model object, with weights updated\n",
        "  \"\"\"\n",
        "  timesteps, img_size, digit, img_path, num_qubits = get_global_vars()\n",
        "\n",
        "  training_iters, num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
        "\n",
        "  global_loss_vals = []\n",
        "  global_validation_loss_vals = []\n",
        "  setup_testing_folder(digit, img_path, num_qubits, load_training_data, num_images_fid, verbose=False)\n",
        "\n",
        "  # Perform 10 training iterations, reducing the learning rate for every subsequent training.\n",
        "  for _ in range(training_iters):\n",
        "      loss_vals = train(model, batch_size, num_epochs)\n",
        "      global_loss_vals.extend(loss_vals)\n",
        "      validation_loss = calc_validation_loss(model, digit, img_path, num_qubits, load_training_data, validation_loss_func, num_images_fid, classical, verbose)\n",
        "      global_validation_loss_vals.append(validation_loss)\n",
        "      plot_loss(global_loss_vals, 'Training Loss')\n",
        "      plot_loss(global_validation_loss_vals, 'Validation Loss')\n",
        "      # Reduce learning rate for next training\n",
        "      for pg in model.opt.param_groups:\n",
        "          pg['lr'] = max(0.000001, pg['lr'] * 0.9)\n",
        "\n",
        "      # Show result after each training\n",
        "      predict(model, verbose)\n",
        "      predict_step(model, verbose)\n",
        "      plt.show()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P4WUf9iiAiLT",
      "metadata": {
        "id": "P4WUf9iiAiLT"
      },
      "outputs": [],
      "source": [
        "def main(save=False, verbose=False):\n",
        "  \"\"\"\n",
        "  Trains a diffusion model given a specified set of hyperparameters.\n",
        "\n",
        "  Inputs:\n",
        "    save, a boolean indicating whether or not to save the results of this run\n",
        "    verbose, a boolean indicating whether debugging output should be printed\n",
        "\n",
        "  Outputs:\n",
        "    None (prints results to the console with verbosity indicated by verbose,\n",
        "    and saves the results if save is True)\n",
        "  \"\"\"\n",
        "  timesteps, img_size, digit, img_path, num_qubits = get_global_vars()\n",
        "\n",
        "  num_images_fid = 5000\n",
        "  classical = get_classical()\n",
        "  model = construct_model(get_global_vars, get_hyperparameters, get_training_data, tf.keras.datasets.mnist.load_data, classical, verbose)\n",
        "  train_model_hyperparams(model, get_global_vars, get_hyperparameters, tf.keras.datasets.mnist.load_data, calculate_fid_given_paths, num_images_fid, classical, verbose)\n",
        "\n",
        "  num_image_batches = 158\n",
        "\n",
        "  generated_imgs = get_generated_images(model, img_size, num_image_batches, verbose)\n",
        "\n",
        "  save_filepath = get_save_filepath()\n",
        "\n",
        "  if save:\n",
        "    training_iters, num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
        "    np.save(save_filepath + \"/generated_imgs_\" + str(digit) + \"_batch_size_\" + str(batch_size) + \"_learning_rate_\" + str(learning_rate) + \"_timesteps_\" + str(timesteps) + \"_normnoise_\" + \"training_iters_\" + str(training_iters) + \"_num_epochs_\" + str(num_epochs) + \"_num_images_\" + str(num_image_batches * 32) + \".npy\", generated_imgs)\n",
        "    torch.save(model.state_dict(), save_filepath + \"/generated_imgs_\" + str(digit) + \"_batch_size_\" + str(batch_size) + \"_learning_rate_\" + str(learning_rate) + \"_timesteps_\" + str(timesteps) + \"_normnoise_\" + \"training_iters_\" + str(training_iters) + \"_num_epochs_\" + str(num_epochs) + \".pth\")\n",
        "    print(f\"Saved generated images and model weights.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lxvqYEHSN4yK",
      "metadata": {
        "id": "lxvqYEHSN4yK"
      },
      "outputs": [],
      "source": [
        "# Train the diffusion model, given a specific set of hyperparameters.\n",
        "main(save=False, verbose=verbose)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab0ed27997974ce8a3be168ace4156cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54923506f04a401e8496ace3fbc658bc",
              "IPY_MODEL_76c6e8e008314317974df1f28ca15ca0",
              "IPY_MODEL_ac7f17f89b7848459f1ec5c2462237e7"
            ],
            "layout": "IPY_MODEL_6da61ec3acf442108f849a6aa024e08e"
          }
        },
        "54923506f04a401e8496ace3fbc658bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1775e6c100bb4dc48e1318fdbf229af9",
            "placeholder": "​",
            "style": "IPY_MODEL_947045cd8a6147d7a0db6246fdded50f",
            "value": "100%"
          }
        },
        "76c6e8e008314317974df1f28ca15ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af193cbdc3da4e54a61f8cc3324a3003",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db945dc4f4584e3d8400d3bd84d694ff",
            "value": 8
          }
        },
        "ac7f17f89b7848459f1ec5c2462237e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f369e14318044219b800fbb2e6364c3a",
            "placeholder": "​",
            "style": "IPY_MODEL_b093605751024caba616b014d812367c",
            "value": " 8/8 [00:00&lt;00:00, 95.30it/s]"
          }
        },
        "6da61ec3acf442108f849a6aa024e08e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1775e6c100bb4dc48e1318fdbf229af9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "947045cd8a6147d7a0db6246fdded50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af193cbdc3da4e54a61f8cc3324a3003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db945dc4f4584e3d8400d3bd84d694ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f369e14318044219b800fbb2e6364c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b093605751024caba616b014d812367c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}